{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyM7c71l3+aPt2xqceZZ+ezm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonnross88/WhoLetThePuppiesIn/blob/main/puppies_comb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brought Over\n"
      ],
      "metadata": {
        "id": "OP85daCPwDGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "\n",
        "!pip install cartopy\n",
        "!pip install --upgrade hvplot\n",
        "!pip install --upgrade panel\n",
        "!pip install --upgrade param\n",
        "!pip install --upgrade holoviews\n",
        "!pip install --upgrade umap-learn\n",
        "!pip install --upgrade geoviews\n",
        "!pip install --upgrade bokeh\n",
        "!pip install --upgrade jupyter_bokeh\n",
        "!pip install pysal\n",
        "!pip install spatialpandas\n",
        "!pip install thefuzz\n",
        "!pip install pmdarima\n",
        "!pip install dask[dataframe]\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "g3fZdMSGi5lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import auth, userdata\n",
        "\n",
        "# PROJECT_ID = userdata.get('MrPrime')\n",
        "# auth.authenticate_user(project_id=PROJECT_ID)\n"
      ],
      "metadata": {
        "id": "R8Ex8EqliqGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def download_file(file_path, file_url):\n",
        "    file_path = Path(file_path)\n",
        "    if not file_path.exists():\n",
        "        print(f\"File not found at {file_path}. Downloading now...\")\n",
        "        subprocess.run([\"wget\", file_url, \"-O\", str(file_path)])\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(f\"File already exists at {file_path}\")\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NrflvdpB65n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_url = 'https://raw.githubusercontent.com/jonnross88/WhoLetThePuppiesIn/main/notebooks/helper_functions.py'\n",
        "hf_path = Path('/content/helper_functions.py')\n",
        "\n",
        "\n",
        "download_file(hf_path, hf_url)"
      ],
      "metadata": {
        "id": "EaFC9-rK3zVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "UpbiBlw8iG7z"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "from functools import partial\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from PIL import ImageDraw, Image  # For image processing\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Related third party imports\n",
        "from bokeh.models import FixedTicker, NumeralTickFormatter\n",
        "import cartopy.crs as ccrs  # For cartographic projections and geographic plots\n",
        "import colorcet as cc  # Additional color palettes\n",
        "from esda.moran import Moran, Moran_Local  # Spatial autocorrelation statistics\n",
        "from fiona.io import ZipMemoryFile\n",
        "import geopandas as gpd\n",
        "import geoviews as gv\n",
        "import geoviews.tile_sources as gts\n",
        "import spatialpandas as spd\n",
        "import holoviews as hv\n",
        "\n",
        "from holoviews import opts\n",
        "import hvplot.pandas  # noqa\n",
        "from matplotlib import pyplot as plt\n",
        "import libpysal as lps  # Spatial analysis library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import panel as pn\n",
        "import panel.widgets as pnw\n",
        "from pmdarima import auto_arima  # For determining ARIMA orders\n",
        "import seaborn as sns\n",
        "from splot.esda import plot_local_autocorrelation\n",
        "from sklearn import metrics  # For evaluating model performance\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import umap\n",
        "from thefuzz import fuzz  # For string matching\n",
        "from joblib import Memory\n",
        "from tqdm import tqdm\n",
        "import pysal as ps\n",
        "from libpysal.weights import DistanceBand, KNN, Kernel, Queen, Rook\n",
        "import esda\n",
        "from splot.esda import (\n",
        "    lisa_cluster,\n",
        "    moran_scatterplot,\n",
        "    plot_local_autocorrelation,\n",
        "    plot_moran,\n",
        ")\n",
        "from splot.libpysal import plot_spatial_weights\n",
        "import libpysal as lps\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Local application/library specific imports\n",
        "import helper_functions as hf  # Custom helper functions for this project\n",
        "# from translate_app import translate_list_to_dict\n",
        "\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures as cf\n",
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "import re\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import lxml\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tiUNJtJlwC4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXy-ssluiG7x"
      },
      "source": [
        "\n",
        "## Dog Puppulation in Zürich: A Geospatial Neighborhood Analysis\n",
        "\n",
        "### Introduction\n",
        "\n",
        "#### Problem Statement:\n",
        "Can we, by the end of January 2024, develop a *pawsome*, data-driven model  that forcasts the dog *puppulation* density across Zürich’s 34 neighborhoods, identifies areas as high density clusters if their dog density is above the 75th percentile and as low density clusters if below the 25th percentile, and achieves this with a Mean Absolute Percentage Error of less than 10%, using time series cross validation, to support urban planning, pet-related businesses, and community welfare?\n",
        "\n",
        "\n",
        "#### Context:\n",
        "Following the City Council Resolution to override the Law on the Keeping of Dogs, the City of Zürich has embarked on a comprehensive exploration of dog *puppulation* dynamics in its neighborhoods. This initiative, prompted by that regulatory shift, aims to sniff out patterns in dog *puppulation* density that impact urban planning, business opportunities, and the overall welfare of our furry companions and their owners. The study leverages data from **2015** to **2020** to improve urban planning, boost pet-related business ventures, and foster community welfare through a better understanding of dog *puppulation* density patterns. This study is vital in this new era for Zürich, providing practical recommendations for the near future. The aim is to develop a data-driven model that reliably predicts the dog *puppulation* density across Zürich’s 34 neighborhoods in the near future.\n",
        "\n",
        "\n",
        "#### Criteria for Success:\n",
        "Our goal is to *unleash* the power of predictive modeling to forecast the dog *puppulation* density patterns in Zürich, aiming to achieve a Mean Absolute Percentage Error of less than 10% with our model, which we will use to make informed predictions for 2024. Achieving this would be a *pawsitive* step towards informed future urban strategies.\n",
        "\n",
        "\n",
        "#### Constraints within Solution Space:\n",
        "- **Temporal Scope**: This study utilizes data from 2015 to 2020 across all datasets. The taxable income datasets, which is only available up to year t - 3 was incorporated into our analysis. To align with other datasets that extend to 2021 and 2022, we employed an auto arima model to predict taxable income for these years.\n",
        "- **Spatial Resolution**: The study focuses on dog *puppulation* density at the neighborhood level. This may not capture variations within neighborhoods or between smaller areas.\n",
        "- **Generalizability**: The findings of this study are specific to Zürich and may not be applicable to other cities or regions with different demographic, economic, and cultural contexts.\n",
        "\n",
        "\n",
        "#### Stakeholders:\n",
        "- **City Planners and Local Authorities:** Empower data-driven decision-making to enhance urban living conditions.\n",
        "- **Business Enterprises:** Guide service offerings and marketing strategies.\n",
        "- **Dog Owners:** Offer insights into community resources and pet care options.\n",
        "\n",
        "\n",
        "#### Key Data Sources:\n",
        "- **Geospatial Boundaries:** [Zürich Statistical Quarters](https://data.stadt-zuerich.ch/dataset/geo_statistische_quartiere)\n",
        "- **Dog Ownership Records:** [Dog Owners Dataset](https://data.stadt-zuerich.ch/dataset/sid_stapo_hundebestand_od1001/download/KUL100OD1001.csv)\n",
        "- **Demographic Statistics:** [Population Dataset](https://data.stadt-zuerich.ch/dataset/bev_bestand_jahr_quartier_alter_herkunft_geschlecht_od3903/download/BEV390OD3903.csv)\n",
        "- **Economic Indicators:** [Income Dataset](https://data.stadt-zuerich.ch/dataset/fd_median_einkommen_quartier_od1003/download/WIR100OD1003.csv)\n",
        "- **Household Dynamics:** [Household Size Dataset](https://data.stadt-zuerich.ch/dataset/bev_hh_haushaltsgroesse_quartier_seit2013_od3806/download/BEV380OD3806.csv)\n",
        "\n",
        "#### Analytical Objectives:\n",
        "- **Understand the Relationship**: Dig into the relationship between demographic factors and dog *puppulation* density across Zürich’s neighborhoods.\n",
        "- **Identify Trends and Clusters**: Track and map out the spatial and temporal trends of dog *puppulation* density. Identify spatial clusters of high and low dog *puppulation* density.\n",
        "- **Predict Future Trends**: Predict the near-future trends of dog *puppulation* density using historical data, aiming for a Mean Absolute Percentage Error of less than 10%. This includes forecasting where Zürich’s dog *puppulation* will be booming across its 34 neighborhoods in the immediate future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6wy-ycIiG7z"
      },
      "source": [
        "### Imports & Configurations\n",
        "\n",
        "This section includes the necessary imports for libraries, configuration settings for dataframes and visualizations. These components establish the foundational setup for subsequent data analysis and exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "nxPthiUXiG70"
      },
      "outputs": [],
      "source": [
        "# Additional configurations for visualization libraries\n",
        "gv.extension(\"bokeh\")\n",
        "hv.extension(\"bokeh\")\n",
        "hvplot.extension(\"bokeh\")\n",
        "# pn.extension(template=\"fast\", nthreads=4, sizing_mode=\"stretch_width\")\n",
        "pn.extension()\n",
        "# memory cache\n",
        "# Set the cache directory\n",
        "CACHE_DIR = \"./zurich_cache_directory\"\n",
        "memory = Memory(CACHE_DIR, verbose=0)\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "bbsBOXR7iG70"
      },
      "outputs": [],
      "source": [
        "# Pandas display options\n",
        "# Disable warnings for chained assignments\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.options.display.max_columns = 50\n",
        "pd.options.display.max_rows = 100\n",
        "hv.streams.PlotSize.scale = 2  # Sharper plots\n",
        "\n",
        "# Seaborn style setting\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Panel configuration for improved interactivity performance\n",
        "pn.config.throttled = True\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Clear any output created by the extensions and settings\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv1aoHgRiG70"
      },
      "source": [
        "### Data Description\n",
        "\n",
        "This project utilizes various datasets to reveal the relationship between dog owner geodemographic factors and dog population density in Zurich.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th>Dataset</th>\n",
        "            <th>Source URL</th>\n",
        "            <th>Original Source</th>\n",
        "            <th>Description</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td><a href=\"#Zurich-Statistical-Districts-Geospatial-Data\">Zurich Districts Data</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/geo_statistische_quartiere\">Link</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/geo_statistische_quartiere\">Stadt Zürich</a></td>\n",
        "            <td>Statistical Quarters</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><a href=\"#Zurich-Dogs-Dataset\">Zurich Dogs Data</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/sid_stapo_hundebestand_od1001/download/KUL100OD1001.csv\">Link</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/sid_stapo_hundebestand_od1001\">Stadt Zürich</a></td>\n",
        "            <td>Dog populations of the City of Zurich since 2015.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><a href=\"#Zurich-Population-Dataset\">Zurich Population Data</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/bev_bestand_jahr_quartier_alter_herkunft_geschlecht_od3903/download/BEV390OD3903.csv\">Link</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/bev_bestand_jahr_quartier_alter_herkunft_geschlecht_od3903\">Stadt Zürich</a></td>\n",
        "            <td>Population by neighbourhood, origin, sex and age, since 1993.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><a href=\"#Zurich-Income-Data\">Zurich Income Data</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/fd_median_einkommen_quartier_od1003/download/WIR100OD1003.csv\">Link</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/fd_median_einkommen_quartier_od1003\">Stadt Zürich</a></td>\n",
        "            <td>Median income of taxable individuals by year, tax rate and urban district, since 1999</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><a href=\"#Zurich-Household-Dataset\">Zurich Household Data</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/bev_hh_haushaltsgroesse_quartier_seit2013_od3806/download/BEV380OD3806.csv\">Link</a></td>\n",
        "            <td><a href=\"https://data.stadt-zuerich.ch/dataset/bev_hh_haushaltsgroesse_quartier_seit2013_od3806\">Stadt Zürich</a></td>\n",
        "            <td>Private households by household size and urban district, since 2013.</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\n",
        "<p>These datasets collectively enable a comprehensive analysis of dog ownership trends in Zurich.</p>\n",
        "\n",
        "\n",
        "### Data Loading\n",
        "First, we load in all of the datasets.\n",
        "\n",
        "To enhance readability and ensure consistency across datasets, original column names were translated from German to English and standardized to snake case using our `sanitize_df_column_names` helper function. This transformation facilitates a cleaner, more uniform `pd.DataFrame` structure for analysis.\n",
        "\n",
        "We then inspect the columns and select the ones we would like to keep for our analysis. We also rename the columns to make them more readable and consistent across datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnTeeBziG71"
      },
      "source": [
        "#### Zurich Statistical Districts Geospatial Data\n",
        "\n",
        "This first geodataset comes as a compressed file containing 3 geojson files.\n",
        "\n",
        "1. `z_gdf_0`: point geometry data at the ideal position for placing a number label on the polygon map.\n",
        "\n",
        "2. `z_gdf_1`: polygon geometry data specifically for visual representation in cartography i.e.maps.\n",
        "\n",
        "3. `z_gdf_2`: polygon geometry data recommended for use for accurate geometry calculations, like spatial joins or area calculations.\n",
        "\n",
        "Together these three files provide excellent geodedic information on the geographical region of Zürich for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jla5idPniG71"
      },
      "outputs": [],
      "source": [
        "# Define the URL for the Zurich Statistical Quarters geospatial data ZIP file.\n",
        "zip_gdf_url = \"https://storage.googleapis.com/mrprime_dataset/zurich/zurich_statistical_quarters.zip\"\n",
        "\n",
        "# Load the geospatial data into Zurich Geo DataFrames.Would you prefer if we do\n",
        "zurich_geo_dicts = hf.get_gdf_from_zip_url(zip_gdf_url)\n",
        "\n",
        "# Rename keys in the Zurich Geo DataFrames with a prefix.\n",
        "z_gdf = hf.rename_keys(zurich_geo_dicts, prefix=\"z_gdf_\")\n",
        "\n",
        "# Display the information and a sample of data from each GeoDataFrame in the z_gdf dictionary\n",
        "for key in z_gdf.keys():\n",
        "    print(f\"\\nInformation for {key}:\")\n",
        "    z_gdf[key].info()\n",
        "    print(f\"Sample data from {key}:\")\n",
        "    display(z_gdf[key].sample(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w97fDKn1iG72"
      },
      "source": [
        "#### Zurich Dogs Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poyiot08iG72"
      },
      "outputs": [],
      "source": [
        "# zurich_dog_data_link = \"https://data.stadt-zuerich.ch/dataset/sid_stapo_hundebestand_od1001/download/KUL100OD1001.csv\"\n",
        "zurich_dog_data_link = (\n",
        "    \"https://storage.googleapis.com/mrprime_dataset/zurich/zurich_dogs.csv\"\n",
        ")\n",
        "# InfoDataFrame is a custom class that inherits from pandas.DataFrame and our InfoMixin\n",
        "zurich_dog_data = hf.InfoDataFrame(pd.read_csv(zurich_dog_data_link))\n",
        "zurich_dog_data.limit_info()\n",
        "\n",
        "# zurich_dog_data = hf.sanitize_df_column_names(zurich_dog_data)\n",
        "# zurich_dog_data.limit_info()\n",
        "zurich_dog_data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJoUL4ldiG72"
      },
      "source": [
        "#### Zurich Population Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zguR6Xo8iG72"
      },
      "outputs": [],
      "source": [
        "# zurich_pop_link = \"https://data.stadt-zuerich.ch/dataset/bev_bestand_jahr_quartier_alter_herkunft_geschlecht_od3903/download/BEV390OD3903.csv\"\n",
        "zurich_pop_link = \"https://storage.googleapis.com/mrprime_dataset/zurich/zurich_pop.csv\"\n",
        "zurich_pop_data = hf.InfoDataFrame(pd.read_csv(zurich_pop_link))\n",
        "zurich_pop_data.limit_info()\n",
        "# zurich_pop_data = hf.sanitize_df_column_names(zurich_pop_data)\n",
        "# zurich_pop_data.limit_info()\n",
        "print(\"Showing a full row of the Zurich population DataFrame:\")\n",
        "zurich_pop_data.sample().T"
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "Nhj-JNsSHyUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCGOCPniG72"
      },
      "source": [
        "#### Zurich Income Dataset\n",
        "These data contain quantile values of the taxable income of natural persons who are primarily taxable in the city of Zurich. Tax income are in thousand francs (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQpo9JSdiG72"
      },
      "outputs": [],
      "source": [
        "# zurich_income_link = \"https://data.stadt-zuerich.ch/dataset/fd_median_einkommen_quartier_od1003/download/WIR100OD1003.csv\"\n",
        "zurich_income_link = (\n",
        "    \"https://storage.googleapis.com/mrprime_dataset/zurich/zurich_income.csv\"\n",
        ")\n",
        "zurich_income_data = hf.InfoDataFrame(pd.read_csv(zurich_income_link))\n",
        "zurich_income_data.info()\n",
        "\n",
        "# Clean column names, display info and sample\n",
        "# zurich_income_data = hf.sanitize_df_column_names(zurich_income_data)\n",
        "# zurich_income_data.info()\n",
        "\n",
        "print(\"\\nShowing a full row of the Zurich income DataFrame:\")\n",
        "zurich_income_data.sample().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJF9j6KhiG73"
      },
      "source": [
        "#### Zurich Household Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoGI2AvTiG73"
      },
      "outputs": [],
      "source": [
        "# zurich_household_data_link = \"https://data.stadt-zuerich.ch/dataset/bev_hh_haushaltsgroesse_quartier_seit2013_od3806/download/BEV380OD3806.csv\"\n",
        "zurich_household_data_link = (\n",
        "    \"https://storage.googleapis.com/mrprime_dataset/zurich/zurich_household.csv\"\n",
        ")\n",
        "zurich_household_data = hf.InfoDataFrame(pd.read_csv(zurich_household_data_link))\n",
        "zurich_household_data.limit_info()\n",
        "print(zurich_household_data.columns)\n",
        "# zurich_household_data = hf.sanitize_df_column_names(zurich_household_data)\n",
        "# zurich_household_data.limit_info()\n",
        "print(\"\\nShowing a full row of the Zurich household DataFrame:\")\n",
        "\n",
        "zurich_household_data.sample().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcpTwKjUiG73"
      },
      "source": [
        "### Dataset Wrangling\n",
        "\n",
        "Before diving into Exploratory Data Analysis (EDA), we need to prepare our datasets. This involves:\n",
        "- Removing unnecessary columns\n",
        "- Renaming columns for consistency\n",
        "- Adding new columns\n",
        "- Cleaning data (handling missing values, correcting datatypes, and standardizing data)\n",
        "\n",
        "These steps will ensure our data is clean and well-structured, setting the stage for effective and accurate analysis in the EDA phase. We'll apply these steps to each dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqj28Pb5iG73"
      },
      "source": [
        "#### Zurich Statistical Districts Geospatial Data\n",
        "\n",
        "Additional steps for this dataset not yet mentioned:\n",
        "\n",
        "- area calculations\n",
        "- spatial join with the geospatial data so that we can consider the districts if we wanted to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O53JAStviG73"
      },
      "outputs": [],
      "source": [
        "zurich_map_gdf = z_gdf[\"z_gdf_1\"]\n",
        "\n",
        "zurich_map_gdf.rename(\n",
        "    columns={\"qname\": \"neighborhood\", \"qnr\": \"subdistrict\", \"knr\": \"district\"},\n",
        "    inplace=True,\n",
        ")\n",
        "# Format the subdistrict column to have 3 digits\n",
        "zurich_map_gdf[\"subdistrict\"] = zurich_map_gdf[\"subdistrict\"].astype(str).str.zfill(3)\n",
        "\n",
        "# Create the refined geodataframe\n",
        "subdistrict_gdf = zurich_map_gdf[\n",
        "    [\"neighborhood\", \"subdistrict\", \"district\", \"geometry\"]\n",
        "].copy()\n",
        "\n",
        "# Display geodataframe information and CRS\n",
        "subdistrict_gdf.info()\n",
        "display(subdistrict_gdf.crs)\n",
        "\n",
        "# Display a sample entry from the transformed geodataframe\n",
        "subdistrict_gdf.sample().T\n",
        "# Load the geospatial data for calculation\n",
        "zurich_calc_gdf = z_gdf[\"z_gdf_2\"]\n",
        "\n",
        "# Calculate area in square meters and add as a new column\n",
        "zurich_calc_gdf[\"subd_area_km2\"] = (\n",
        "    zurich_calc_gdf.to_crs(ccrs.GOOGLE_MERCATOR).area / 1e6\n",
        ")\n",
        "\n",
        "# Rename the column for consistency with the main geodataframe\n",
        "zurich_calc_gdf = zurich_calc_gdf.rename(columns={\"qname\": \"neighborhood\"})\n",
        "\n",
        "# Merge calculated features with the main geodataframe (subdistrict_gdf)\n",
        "area_gdf = subdistrict_gdf.merge(\n",
        "    zurich_calc_gdf[[\"neighborhood\", \"subd_area_km2\"]], on=\"neighborhood\"\n",
        ")\n",
        "\n",
        "# Display a snapshot of the merged geodataframe\n",
        "display(area_gdf.sample().T)\n",
        "\n",
        "\n",
        "districts_gdf = (\n",
        "    subdistrict_gdf.drop(columns=[\"neighborhood\", \"subdistrict\"])\n",
        "    .dissolve(by=\"district\")\n",
        "    .reset_index()\n",
        ")\n",
        "districts_gdf = districts_gdf.dissolve(by=\"district\").reset_index()\n",
        "districts_gdf[\"d_area_km2\"] = districts_gdf.to_crs(ccrs.GOOGLE_MERCATOR).area / 1e6\n",
        "\n",
        "display(districts_gdf.sample().T)\n",
        "districts_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Z9IPhsiG73"
      },
      "outputs": [],
      "source": [
        "# Save the geodataframe to disk in the data folder\n",
        "# area_gdf.to_file(\"../data/zurich_neighborhoods.geojson\")\n",
        "# districts_gdf.to_file(\"../data/zurich_districts.geojson\")\n",
        "\n",
        "# Save the geodataframe to disk in the data folder\n",
        "hf.save_to_data(area_gdf, \"zurich_neighborhoods.geojson\")\n",
        "hf.save_to_data(districts_gdf, \"zurich_districts.geojson\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj28IJdLiG74"
      },
      "source": [
        "#### Zurich Dogs Dataset\n",
        "\n",
        "The original dataset had 31 columns, many redundant. We've picked 18 for our analysis:\n",
        "\n",
        "- deadline_date_year\n",
        "- holder_id\n",
        "- age_v_10_cd\n",
        "- sex_cd\n",
        "- circle_cd\n",
        "- quar_cd\n",
        "- quar_lang\n",
        "- race_1_text\n",
        "- race_2_text\n",
        "- breed_mixed__breed_cd\n",
        "- breed_mongrel_long\n",
        "- breed_mixed__breed_sort\n",
        "- breed_type_cd\n",
        "- birth_dog_year\n",
        "- age_v_dog_cd\n",
        "- sex_dog_cd\n",
        "- dog_color_text\n",
        "- number_of_dogs\n",
        "\n",
        "From these columns we create a new dataset, `dog_data` and we and we transform these column in preparation for the EDA phase. These transformations included:\n",
        "- Converting the columns which only contain two different values two binary columns\n",
        "- translating some values from German to English\n",
        "- dealing with missing values\n",
        "- standardizing some of the values for easier grouping."
      ]
    },
    {
      "source": [
        "zurich_dog_data = hf.InfoDataFrame(pd.read_csv(zurich_dog_data_link))\n",
        "\n",
        "zurich_dog_data_column_name_translations = {\n",
        "    'StichtagDatJahr': 'reporting_year',\n",
        "    'HalterId': 'owner_id',\n",
        "    'AlterV10Cd': 'age_group_10',\n",
        "    'SexCd': 'owner_gender',\n",
        "    'KreisCd': 'district',\n",
        "    'QuarCd': 'subdistrict',\n",
        "    'Rasse1Text': 'breed_1',\n",
        "    'Rasse2Text': 'breed_2',\n",
        "    'RasseMischlingCd': 'mixed_breed_code',\n",
        "    'RasseMischlingLang': 'mixed_type',\n",
        "    'DatenstandCd': 'data_status_code',\n",
        "    'AlterV10Lang': 'age_group_10_long',\n",
        "    'AlterV10Sort': 'age_group_10_sort',\n",
        "    'RassentypCd': 'dog_size',\n",
        "    'GebDatHundJahr': 'dog_birth_year',\n",
        "    'AlterVHundCd': 'dog_age',\n",
        "    'SexHundCd': 'dog_gender',\n",
        "    'HundefarbeText': 'dog_color',\n",
        "    'AnzHunde': 'number_of_dogs',\n",
        "    # 'SexLang': 'sex_long',\n",
        "    # 'SexSort': 'sex_sort',\n",
        "    # 'KreisLang': 'district_long',\n",
        "    # 'KreisSort': 'district_sort',\n",
        "    # 'QuarLang': 'subdistrict_long',\n",
        "    # 'QuarSort': 'subdistrict_sort',\n",
        "    # 'RasseMischlingSort': 'mixed_breed_sort',\n",
        "    # 'RassentypLang': 'breed_type_long',\n",
        "    # 'RassentypSort': 'breed_type_sort',\n",
        "    # 'AlterVHundLang': 'dog_age_long',\n",
        "    # 'AlterVHundSort': 'dog_age_sort',\n",
        "    # 'SexHundLang': 'dog_sex_long',\n",
        "    # 'SexHundSort': 'dog_sex_sort',\n",
        "}\n",
        "\n",
        "print(f\"Dataset now has {zurich_dog_data.shape[0]} rows and {zurich_dog_data.shape[1]} columns\\n\")\n",
        "zurich_dog_data.rename(columns=zurich_dog_data_column_name_translations).sample()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NKBbUJny94cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import translate\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    \"\"\"Function to remove accents from a string\"\"\"\n",
        "    import unicodedata\n",
        "    nfkd_form = (\n",
        "        unicodedata.normalize(\"NFKD\",input_str).encode(\"ASCII\", \"ignore\").decode())\n",
        "    return nfkd_form\n",
        "\n",
        "\n",
        "\n",
        "# def convert_to_snake_case(name):\n",
        "#     \"\"\"Convert a camel case string to a snake case string\"\"\"\n",
        "#     name = re.sub(r'[\\s,-]+', '_', name)\n",
        "#     name = re.sub(r\"([A-Z])([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
        "#     name = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name)\n",
        "#     name = re.sub(r'[()]', '', name)\n",
        "#     return name.lower()\n",
        "\n",
        "# @memory.cache\n",
        "# def translate_list_to_dict(\n",
        "#     list_of_strings,\n",
        "#     project_id: str = \"mrprime-349614\",\n",
        "#     source_lang: str = \"de\",\n",
        "#     target_lang: str = \"en-US\",\n",
        "# ) -> dict[str, str]:\n",
        "#     \"\"\"Translates a list, or another interable, of strings using Cloud Translation API.\n",
        "#     Returns a TranslateTextResponse object.\"\"\"\n",
        "#     client = translate.TranslationServiceClient()\n",
        "#     location = \"us-central1\"\n",
        "#     parent = f\"projects/{project_id}/locations/{location}\"\n",
        "#     response = client.translate_text(\n",
        "#         request={\n",
        "#             \"parent\": parent,\n",
        "#             \"contents\": list_of_strings,\n",
        "#             \"mime_type\": \"text/plain\",  # mime types: text/plain, text/html\n",
        "#             \"source_language_code\": source_lang,\n",
        "#             \"target_language_code\": target_lang,\n",
        "#         }\n",
        "#     )\n",
        "#     trans_dict = {\n",
        "#         text: translation.translated_text\n",
        "#         for (text, translation) in zip(list_of_strings, response.translations)\n",
        "#     }\n",
        "#     return trans_dict\n",
        "\n",
        "\n",
        "# def sanitize_df_column_names(df):\n",
        "#     \"\"\"Function to danitize column names by translating and conveting to snake case\"\"\"\n",
        "#     column_list = df.columns.tolist()\n",
        "#     # translate the column names\n",
        "#     translated_dict = translate_list_to_dict(column_list)\n",
        "#     # map the translated column names to the column names\n",
        "#     df.rename(columns=translated_dict, inplace=True)\n",
        "#     # convert the column names to snake case\n",
        "#     df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
        "#     return df"
      ],
      "metadata": {
        "id": "pFLfe7yk0LZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N7Mjg18iG74"
      },
      "outputs": [],
      "source": [
        "\n",
        "zurich_dog_data = zurich_dog_data.rename(columns=zurich_dog_data_column_name_translations)\n",
        "\n",
        "# After renaming, you may still need to adjust the data types for certain columns\n",
        "zurich_dog_data[\"owner_id\"] = zurich_dog_data[\"owner_id\"].astype(\"string\").str.zfill(6)\n",
        "zurich_dog_data[\"dog_age\"] = zurich_dog_data[\"dog_age\"].astype(int)\n",
        "zurich_dog_data[\"district\"] = zurich_dog_data[\"district\"].astype(int)\n",
        "zurich_dog_data[\"subdistrict\"] = (\n",
        "    zurich_dog_data[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")\n",
        "print(f\"Dataset now has {zurich_dog_data.shape[0]} rows and {zurich_dog_data.shape[1]} columns\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LGkvizuiG76"
      },
      "source": [
        "\n",
        "The number of dogs for each row is given in the `number_of_dogs` column. These are 'brothers and sisters' that also have the same owner and same characteristics.\n",
        "\n",
        "E.g.\n",
        "- `standard` or breed\n",
        "- `dog_color_en` or dog color, etc.\n",
        "\n",
        "\n",
        "We expand the dataset to have one dog for each row, by repeating the rows by the number in the `number_of_dogs` column. We reset the index after so that we have a unique index for each row.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Repeat each row based on the number of dogs in the row represents\n",
        "zurich_dog_data = zurich_dog_data.loc[\n",
        "    zurich_dog_data.index.repeat(zurich_dog_data[\"number_of_dogs\"])\n",
        "]\n",
        "# drop the number of dogs column\n",
        "zurich_dog_data.drop(\"number_of_dogs\", axis=1, inplace=True)\n",
        "# reset the index\n",
        "zurich_dog_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\n",
        "    f\"Dataset now has {zurich_dog_data.shape[0]} rows and {zurich_dog_data.shape[1]} columns\"\n",
        ")\n",
        "\n",
        "zurich_dog_data.sample(3)\n",
        "\n",
        "dog_columns = [\n",
        "    'reporting_year',\n",
        "    'owner_id',\n",
        "    'age_group_10',\n",
        "    'owner_gender',\n",
        "    'dog_size',\n",
        "    'dog_age',\n",
        "    'mixed_type',\n",
        "    'dog_gender',\n",
        "    'dog_color',\n",
        "    'breed_1',\n",
        "    'breed_2',\n",
        "    'district',\n",
        "    'subdistrict',\n",
        "]\n",
        "dog_data = zurich_dog_data[dog_columns].copy()\n",
        "\n",
        "print(f\"Dataset now has {dog_data.shape[0]} rows and {dog_data.shape[1]} columns\")\n",
        "\n",
        "\n",
        "# dog_data = zurich_dog_data[list(new_column_names.values())].copy()\n",
        "display(\n",
        "    dog_data.describe(include=\"all\")\n",
        "    .T.sort_values(by=\"unique\")\n",
        "    .infer_objects(copy=False)\n",
        "    .fillna(\"\")\n",
        ")"
      ],
      "metadata": {
        "id": "1FKF7PMHBiAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrNAq2bhiG74"
      },
      "source": [
        "First look at thhe `dog_density` data which will be our target variable for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4EskzBxiG74"
      },
      "outputs": [],
      "source": [
        "# Extract the 'subdistrict' and 'subd_area_km2' columns from the area_gdf dataframe\n",
        "area_df = area_gdf[['subdistrict', 'subd_area_km2']]\n",
        "\n",
        "\n",
        "# create a function to create the dog density dataframe\n",
        "def create_dog_density_df(dog_data, area_df):\n",
        "  \"\"\"Creates a dataframe with dog density per subdistrict and reporting_year.\"\"\"\n",
        "  # get the unique subdistricts\n",
        "  subdistricts_list = area_df.subdistrict.unique()\n",
        "  # filter out subdistricts not in the area_df\n",
        "  dog_data.loc[~dog_data.subdistrict.isin(subdistricts_list),\n",
        "               \"subdistrict\"] = None\n",
        "  dog_data.dropna(subset=[\"subdistrict\"], inplace=True)\n",
        "  # get the count of dogs per reporting_year\n",
        "  dog_data_counts_df = dog_data.groupby(['reporting_year', 'subdistrict'\n",
        "                                         ]).size().reset_index(name='count')\n",
        "  # merge the dog counts with the area dataframe's subdistrict and subd_area_km2 columns'\n",
        "  dog_density_df = dog_data_counts_df.merge(area_df)\n",
        "  # calculate the dog density\n",
        "  dog_density_df['dog_density'] = (dog_density_df['count'] /\n",
        "                                   dog_density_df['subd_area_km2']).round(2)\n",
        "  # pivot the dataframe\n",
        "  dog_density_pivot = dog_density_df.pivot(index='subdistrict',\n",
        "                                           columns='reporting_year',\n",
        "                                           values='dog_density')\n",
        "  return dog_density_pivot\n",
        "\n",
        "\n",
        "create_dog_density_df(dog_data,area_df).hvplot.heatmap(\n",
        "                          cmap='greens',\n",
        "                          height=600,\n",
        "                          width=600,\n",
        "                          title='Dog Density per Sub-District').opts(\n",
        "                              active_tools=['box_zoom'],\n",
        "                              color_levels=7,\n",
        "                              line_width=2,\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q1vwAufiG74"
      },
      "outputs": [],
      "source": [
        "dog_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whk5vXZjiG74"
      },
      "outputs": [],
      "source": [
        "# get subdistricts_list from  area_df as these subdistricts are used in predicting the target\n",
        "subdistricts_list = area_df.subdistrict.unique()\n",
        "dog_data.loc[~dog_data.subdistrict.isin(subdistricts_list),\n",
        "               \"subdistrict\"] = None\n",
        "# drop rows with missing subdistricts\n",
        "dog_data = dog_data.dropna(subset='subdistrict')\n",
        "print(f\"Dataset now has {dog_data.shape[0]} rows and {dog_data.shape[1]} columns\\n\")\n",
        "\n",
        "# Unique values for \"mixed_type\" column\n",
        "breed_cat_list_de = dog_data[\"mixed_type\"].unique().tolist()\n",
        "print(\"Breed Categories (German):\")\n",
        "display(breed_cat_list_de)\n",
        "\n",
        "# Create a dictionary for translation\n",
        "# breed_cat_dict = translate_list_to_dict(breed_cat_list_de)\n",
        "# print(\"\\nBreed Category Dictionary (Translation):\")\n",
        "# display(breed_cat_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KbS27PiG75"
      },
      "outputs": [],
      "source": [
        "# Map 'mixed_type' to categories, rename for brevity, and define 'is_pure_breed'\n",
        "mixed_type_dict = {\n",
        "    \"Rassehund\": \"PB\",\n",
        "    \"Mischling, beide Rassen bekannt\": \"BB\",\n",
        "    \"Mischling, sekundäre Rasse unbekannt\": \"BU\",\n",
        "    \"Mischling, beide Rassen unbekannt\": \"UU\",\n",
        "}\n",
        "\n",
        "dog_data[\"mixed_type\"] = dog_data[\"mixed_type\"].map(mixed_type_dict)\n",
        "\n",
        "# dog_data[\"mixed_type\"] = (\n",
        "#     dog_data[\"mixed_type\"]\n",
        "#     .map(breed_cat_dict)\n",
        "#     .map(\n",
        "#         {\n",
        "#             \"pedigree dog\": \"PB\",\n",
        "#             \"Mixed breed, both breeds known\": \"BB\",\n",
        "#             \"Mixed breed, secondary breed unknown\": \"BU\",\n",
        "#             \"Mixed breed, both breeds unknown\": \"UU\",\n",
        "#         }\n",
        "#     )\n",
        "# )\n",
        "dog_data[\"is_pure_breed\"] = dog_data[\"mixed_type\"].eq(\"PB\")\n",
        "dog_data['is_designer_breed'] = dog_data['mixed_type'].eq('BB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNN4Rd6NiG75"
      },
      "outputs": [],
      "source": [
        "# Define owner and dog gender\n",
        "\n",
        "# Drop the columns we just used to create the new columns\n",
        "if \"owner_gender\" in dog_data.columns:\n",
        "    dog_data[\"is_male_owner\"] = dog_data[\"owner_gender\"] == 1\n",
        "    dog_data = dog_data.drop(columns=[\"owner_gender\"])\n",
        "\n",
        "if \"dog_gender\" in dog_data.columns:\n",
        "    dog_data[\"is_male_dog\"] = dog_data[\"dog_gender\"] == 1\n",
        "    dog_data = dog_data.drop(columns=[\"dog_gender\"])\n",
        "\n",
        "dog_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download translations from the github repo\n",
        "breeds_dict_translations_path = Path('./breeds_dict_translations.json')\n",
        "breeds_dict_translations_url = \"https://raw.githubusercontent.com/jonnross88/WhoLetThePuppiesIn/refs/heads/main/notebooks/breeds_dict_translations.json\"\n",
        "\n",
        "color_dict_translations_path = Path('./dog_color_translations.json')\n",
        "color_dict_translations_url = \"https://raw.githubusercontent.com/jonnross88/WhoLetThePuppiesIn/refs/heads/main/notebooks/dog_color_translations.json\"\n",
        "\n",
        "download_file(breeds_dict_translations_path, breeds_dict_translations_url)\n",
        "download_file(color_dict_translations_path, color_dict_translations_url)"
      ],
      "metadata": {
        "id": "pK9RyRII2Nv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RQJnlQWiG75"
      },
      "outputs": [],
      "source": [
        "# Unique values for dog colors\n",
        "dog_colors = dog_data[\"dog_color\"].str.lower().unique().tolist()\n",
        "dog_colors.sort()\n",
        "# print(dog_colors)\n",
        "# Translate dog colors\n",
        "# dog_color_translations = translate_list_to_dict(dog_colors)\n",
        "with open(color_dict_translations_path, 'r') as fp:\n",
        "    dog_color_translations = json.load(fp)\n",
        "\n",
        "# dog_data[\"dog_color_en\"] = dog_data[\"dog_color\"].str.lower().map(\n",
        "#     dog_color_translations)\n",
        "dog_color_df = pd.DataFrame.from_dict(dog_color_translations, orient='index').reset_index().rename(columns={'index': 'dog_color', 0: 'dog_color_en'})\n",
        "print(\"\\nColors dataframe sample\")\n",
        "display(dog_color_df.sample(3))\n",
        "\n",
        "\n",
        "# Unique values for breed_1\n",
        "breeds_1 = dog_data[\"breed_1\"].str.lower().unique().tolist()\n",
        "\n",
        "# Unique values for breed_2\n",
        "breeds_2 = dog_data[\"breed_2\"].str.lower().unique().tolist()\n",
        "\n",
        "breeds_list = list(set(breeds_1 + breeds_2))\n",
        "breeds_list = [remove_accents(breed) for breed in breeds_list]\n",
        "breeds_list.sort()\n",
        "\n",
        "# breeds_dict = translate_list_to_dict(breeds_list)\n",
        "with open(breeds_dict_translations_path, 'r') as fp:\n",
        "    breeds_dict = json.load(fp)\n",
        "\n",
        "breeds_df = pd.DataFrame.from_dict(breeds_dict, orient=\"index\").reset_index().rename(columns={'index': 'breed_de', 0: 'breed_en'})\n",
        "print(\"\\nBreeds dataframe sample\")\n",
        "display(breeds_df.sample(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-TmzCqRiG75"
      },
      "source": [
        "\n",
        "##### Breed Standardization\n",
        "To ensure consistency in the analysis, the breeds in the dataset are standardized. Since the \"breed\" column is free text, allowing dog owners to input their breed information during registration, variations can exist even for the same breeds. To address this, we will use the dataframe we collected in the last notebook which contains the breeds recognized by the FCI (Fédération Cynologique Internationale). Within this dataframe, each recognized FCI breed has a column listing its name in different languages and alternative, unofficial names.\n",
        "\n",
        "This approach helps capture variations in breed names and facilitates grouping similar breeds together.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/jonnross88/WhoLetThePuppiesIn/main/notebooks/fci_breeds.json\n",
        "# Saved the fci data in a bucket for easier editing vs the github method\n",
        "\n",
        "fci_url = 'https://storage.googleapis.com/mrprime_dataset/dogs/fci_breeds.json'\n",
        "\n",
        "fci_breeds = pd.read_json(fci_url)\n",
        "fci_breeds[[\"alt_names\", \"breed_en\"]].sample()\n"
      ],
      "metadata": {
        "id": "VOjQtnJMUzVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# nan_mask = breeds_df[\"standard\"].isna()\n",
        "\n",
        "# matched_value = hf.apply_fuzzy_matching_to_breed_column(\n",
        "#     breeds_df, \"breed_de\", fci_breeds, [fuzz.WRatio]\n",
        "# )\n",
        "\n",
        "# breeds_df.loc[nan_mask, \"standard\"] = matched_value[nan_mask]\n",
        "# nan_mask = breeds_df[\"standard\"].isna()\n",
        "# print(nan_mask.sum())\n",
        "breeds_df"
      ],
      "metadata": {
        "id": "ktpFIJz1Okye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AUjqPpjiG75"
      },
      "outputs": [],
      "source": [
        "# Get the FCI dataframe with the recognized breeds\n",
        "# fci_breeds = pd.read_json(\"../data/fci_breeds.json\")\n",
        "# fci_breeds = pd.read_json(fci_url)\n",
        "# fci_breeds[[\"alt_names\", \"breed_en\"]]\n",
        "\n",
        "# Create a DataFrame with translated breed names\n",
        "# breeds_df.columns = [\"breed_de\", \"breed_en\"]\n",
        "\n",
        "# Initialize a \"standard\" column for breed standardization\n",
        "breeds_df[\"standard\"] = None\n",
        "nan_mask = breeds_df[\"standard\"].isna()\n",
        "\n",
        "# Match each column for breed standardization\n",
        "for col in breeds_df.columns:\n",
        "    matched_value = hf.apply_fuzzy_matching_to_breed_column(\n",
        "        breeds_df.loc[nan_mask], col, fci_breeds, [fuzz.WRatio]\n",
        "    )\n",
        "    breeds_df.loc[nan_mask, \"standard\"] = matched_value[nan_mask]\n",
        "    nan_mask = breeds_df[\"standard\"].isna()\n",
        "\n",
        "# Update the standard column for specific cases\n",
        "breeds_df.loc[nan_mask, \"standard\"] = breeds_df.loc[nan_mask, \"breed_en\"]\n",
        "breeds_df.loc[breeds_df[\"breed_de\"] == \"elo\", \"standard\"] = \"elo\"\n",
        "breeds_df.loc[breeds_df[\"breed_de\"] == \"keine\", \"standard\"] = \"none\"\n",
        "breeds_df.loc[breeds_df[\"breed_de\"] == \"mischling\", \"standard\"] = \"hybrid\"\n",
        "\n",
        "# Convert breed_1 to lowercase for merging\n",
        "dog_data[\"breed_1\"] = dog_data[\"breed_1\"].str.lower()\n",
        "dog_data['breed_1'] = dog_data['breed_1'].apply(remove_accents)\n",
        "dog_data[\"breed_2\"] = dog_data[\"breed_2\"].str.lower()\n",
        "dog_data['breed_2'] = dog_data['breed_2'].apply(remove_accents)\n",
        "\n",
        "# Merge with the breeds_df for standardized breed names\n",
        "dog_data = dog_data.merge(\n",
        "    breeds_df.drop(columns=[\"breed_en\"]),\n",
        "    how='left',\n",
        "    left_on=\"breed_1\",\n",
        "    right_on=\"breed_de\",\n",
        "    suffixes=(\"\", \"_1\"),\n",
        ")\n",
        "\n",
        "dog_data = dog_data.merge(\n",
        "    breeds_df.drop(columns=[\"breed_en\"]),\n",
        "    how='left',\n",
        "    left_on=\"breed_2\",\n",
        "    right_on=\"breed_de\",\n",
        "    suffixes=(\"\", \"_2\"),  # Add suffix to distinguish columns\n",
        ")\n",
        "\n",
        "dog_data['dog_color'] = dog_data['dog_color'].str.lower()\n",
        "dog_data = dog_data.merge(dog_color_df, how='left', left_on='dog_color', right_on='dog_color')\n",
        "dog_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvBksaUBiG75"
      },
      "source": [
        "\n",
        "##### Filtering Doodle Dogs\n",
        "\n",
        "A specific analysis is conducted to filter out dogs with 'doodle' in their breed names, converting them to mixed breeds and updating breed information accordingly. This is a designer breed which is not yet recognized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-BCvWx6iG76"
      },
      "outputs": [],
      "source": [
        "# Create mask to filter out the doodle dogs\n",
        "doodle_mask = dog_data[\"breed_1\"].str.contains(\n",
        "    r\".*doodle\", regex=True, na=False, case=False\n",
        ")\n",
        "print(f\"Number of doodle dogs: {doodle_mask.sum()}\")\n",
        "# convert them to mixed breed if they are pure breeds\n",
        "dog_data.loc[doodle_mask, \"is_pure_breed\"] = False\n",
        "dog_data.loc[doodle_mask, \"standard_2\"] = \"poodle\"\n",
        "dog_data.loc[doodle_mask, \"mixed_type\"] = \"BB\"\n",
        "dog_data.loc[doodle_mask, \"standard\"] = dog_data.loc[doodle_mask, \"breed_1\"].apply(\n",
        "    lambda x: \"golden retriever\" if x.startswith(\"G\") else \"labrador retriever\"\n",
        ")\n",
        "dog_data[doodle_mask].sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhXhK0SkiG76"
      },
      "outputs": [],
      "source": [
        "# Calculate total dogs per owner and reporting_year\n",
        "dog_data[\"pet_count\"] = dog_data.groupby([\"owner_id\", \"reporting_year\"])[\"breed_1\"].transform(\n",
        "    \"count\"\n",
        ")\n",
        "print(f\"Dataset now has {dog_data.shape[0]} rows and {dog_data.shape[1]} columns\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTLgYM-7iG76"
      },
      "source": [
        "##### Missing Values\n",
        "Although initially it looked as if we have no missing values, on close investigation we can see that there are placeholder values for where the missing values are. We replaced these with `Nan` values so that they are not mistaken for real values. As these are only few for the columns `subdistrict`, `dog_age`, and `district` we simply drop those rows. Remaining column with missing values `age_group_10`, we:\n",
        "\n",
        "- fill missing `age_group_10` (dog owners' age groups) with `-1`, tracking these in `age_group_missing`.\n",
        "- use later years' reporting_years to fill age group where possible, and make these edits in `age_group_10`.\n",
        "\n",
        "\n",
        "Finally, we create `age_group_20`, grouping ages into 20-year increments, approximating a generation's length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph6K8Ud5iG76"
      },
      "outputs": [],
      "source": [
        "display(\n",
        "    dog_data.describe(include=\"all\")\n",
        "    .T.sort_values(by=\"unique\")\n",
        "    .infer_objects(copy=False)\n",
        "    .fillna(\"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-srXXZViG77"
      },
      "outputs": [],
      "source": [
        "# Create a list of subdistricts to be used for validation\n",
        "subdistricts_list = subdistrict_gdf[\"subdistrict\"].unique().tolist()\n",
        "\n",
        "# Define a dictionary of conditions and corresponding columns to be updated\n",
        "conditions = {\n",
        "    \"dog_size\": dog_data[\"dog_size\"] == \"UN\",\n",
        "    \"age_group_10\": dog_data[\"age_group_10\"] > 100,\n",
        "    \"district\": dog_data[\"district\"] > 12,\n",
        "    \"dog_age\": dog_data[\"dog_age\"] > 30,\n",
        "    \"subdistrict\": ~dog_data[\"subdistrict\"].isin(subdistricts_list),\n",
        "}\n",
        "\n",
        "# Identify and print unique breeds with 'UN' dog size\n",
        "un_breeds = dog_data.loc[conditions[\"dog_size\"], \"breed_1\"].unique()\n",
        "print(f\"Dogs breeds of those missing dog_size data:\\n{un_breeds}\")\n",
        "\n",
        "# Replace 'UN' dog size with 'K' and other invalid values with NaN\n",
        "for column, condition in conditions.items():\n",
        "    dog_data.loc[condition, column] = \"K\" if column == \"dog_size\" else np.nan\n",
        "\n",
        "# Display the number of NaN values in each column\n",
        "print(\"\\nNumber of NaN values in each column:\")\n",
        "print(dog_data.isna().sum().sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z80-mJl2iG77"
      },
      "outputs": [],
      "source": [
        "dog_data = dog_data.dropna(subset=[\"dog_age\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEgEBU9UiG77"
      },
      "outputs": [],
      "source": [
        "dog_data.columns\n",
        "dog_data.info()\n",
        "dog_data.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTkQBiv8iG77"
      },
      "outputs": [],
      "source": [
        "# convert the numerical columns which had NaN values to int\n",
        "dog_data[\"dog_age\"] = dog_data[\"dog_age\"].astype(int)\n",
        "dog_data[\"district\"] = dog_data[\"district\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvcTkLJeiG77"
      },
      "outputs": [],
      "source": [
        "# Create an indicator variable for missing 'age_group_10' values\n",
        "dog_data[\"age_group_missing\"] = dog_data[\"age_group_10\"].isna().astype(int)\n",
        "\n",
        "# Fill in the missing 'age_group_10' values\n",
        "dog_data[\"age_group_10\"] = dog_data[\"age_group_10\"].fillna(\n",
        "    dog_data.groupby(\"owner_id\")[\"age_group_10\"].transform(\n",
        "        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
        "    )\n",
        ")\n",
        "\n",
        "dog_data[\"age_group_10\"] = dog_data[\"age_group_10\"].fillna(-1).astype(int)\n",
        "dog_data[\"age_group_20\"] = dog_data[\"age_group_10\"].apply(\n",
        "    lambda x: -1 if x == -1 else (x // 20) * 20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voZShaXeiG77"
      },
      "source": [
        "##### Consolidated Dog Data preprocessing\n",
        "Combined all that we did with the dog data set into the `preprocess_dog_data` function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "breeds_df"
      ],
      "metadata": {
        "id": "DQsCycSNDMSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d59ilUXNiG77"
      },
      "outputs": [],
      "source": [
        "# Data obtained form the first notebook\n",
        "# fci_breeds = pd.read_json(\"../data/fci_breeds.json\")\n",
        "# fci_breeds = pd.read_json(\"/content/fci_breeds.json\")\n",
        "\n",
        "\n",
        "\n",
        "# def get_translation_dict(data, column):\n",
        "#     \"\"\"Returns a dataframe with the unique values in the column and their translations\"\"\"\n",
        "#     df = data.copy()\n",
        "#     data_to_translate = df[column].str.lower().unique()\n",
        "\n",
        "#     return translate_list_to_dict(data_to_translate)\n",
        "\n",
        "\n",
        "@memory.cache\n",
        "def get_breed_standard(dict_of_breeds_translations, agency_breeds_df=fci_breeds):\n",
        "    \"\"\"Find the breed standard for each breed in the column\"\"\"\n",
        "    breeds_data = pd.DataFrame.from_dict(dict_of_breeds_translations, orient=\"index\").reset_index().rename(columns={'index': 'breed_de', 0: 'breed_en'})\n",
        "\n",
        "    # apply fuzzy matching to the breed column to get the standardized breed name\n",
        "    # create the 'standard' column and fill it with None\n",
        "    breeds_data[\"standard\"] = None\n",
        "    nan_mask = breeds_data[\"standard\"].isna()\n",
        "    # Match each column for breed standardization\n",
        "\n",
        "    for col in ['breed_de', 'breed_en']:\n",
        "        matched_value = hf.apply_fuzzy_matching_to_breed_column(\n",
        "            breeds_data.loc[nan_mask], col, agency_breeds_df, [fuzz.WRatio])\n",
        "\n",
        "        breeds_data.loc[nan_mask, \"standard\"] = matched_value[nan_mask]\n",
        "        nan_mask = breeds_data[\"standard\"].isna()\n",
        "\n",
        "    # Update the standard column for specific cases\n",
        "    breeds_data.loc[nan_mask, \"standard\"] = breeds_data.loc[nan_mask, \"breed_en\"]\n",
        "    # Special cases\n",
        "    breeds_data.loc[breeds_data[\"breed_de\"] == \"elo\", \"standard\"] = \"elo\"\n",
        "    breeds_data.loc[breeds_data[\"breed_de\"] == \"keine\", \"standard\"] = \"none\"\n",
        "    breeds_data.loc[breeds_data[\"breed_de\"] == \"mischling\", \"standard\"] = \"hybrid\"\n",
        "    return breeds_data\n",
        "\n",
        "\n",
        "def get_doodle_fix(data):\n",
        "    \"\"\"Correct doodle dogs to standard entries\"\"\"\n",
        "    df = data.copy()\n",
        "    # Create mask to filter out the doodle dogs\n",
        "    doodle_mask = df[\"breed_1_de\"].str.contains(r\".*doodle\",\n",
        "                                                regex=True,\n",
        "                                                na=False,\n",
        "                                                case=False)\n",
        "\n",
        "    # convert them to mixed breed if they are pure breeds\n",
        "    df.loc[doodle_mask, \"is_pure_breed\"] = False\n",
        "    df.loc[doodle_mask, \"standard_2\"] = \"poodle\"\n",
        "\n",
        "    df.loc[doodle_mask, \"mixed_type\"] = \"BB\"\n",
        "    df.loc[doodle_mask, \"standard\"] = df.loc[doodle_mask, \"breed_1_de\"].apply(\n",
        "        lambda x: \"golden retriever\"\n",
        "        if x.startswith(\"G\") else \"labrador retriever\")\n",
        "    return df\n",
        "\n",
        "\n",
        "@memory.cache\n",
        "def drop_concealed_nans(data):\n",
        "    df = data.copy()\n",
        "\n",
        "    subdistricts_list = df.subdistrict.value_counts().index.tolist()[:34]\n",
        "    nan_conditions = {\n",
        "        \"dog_size\": df[\"dog_size\"] == \"UN\",\n",
        "        \"age_group_10\": df[\"age_group_10\"] > 100,\n",
        "        \"district\": df[\"district\"] > 12,\n",
        "        \"dog_age\": df[\"dog_age\"] > 30,\n",
        "    }\n",
        "    for column, condition in nan_conditions.items():\n",
        "        df.loc[condition, column] = \"K\" if column == \"dog_size\" else np.nan\n",
        "    return df\n",
        "\n",
        "\n",
        "# define a function which does all of the preprocessing steps\n",
        "@memory.cache\n",
        "def preprocess_dog_data(data, **kwargs):\n",
        "    \"\"\"Preprocess the Zurich dog data\"\"\"\n",
        "\n",
        "    df = data.copy()\n",
        "    df = df.rename(columns=zurich_dog_data_column_name_translations)\n",
        "    df[\"owner_id\"] = df[\"owner_id\"].astype(\"string\").str.zfill(6)\n",
        "\n",
        "    df[\"dog_age\"] = df[\"dog_age\"].astype(int)\n",
        "    df[\"district\"] = df[\"district\"].astype(int)\n",
        "    df[\"subdistrict\"] = df[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        "\n",
        "    # expand to 1 dog on each row\n",
        "    df = df.loc[df.index.repeat(df[\"number_of_dogs\"])]\n",
        "    df = df.drop(\"number_of_dogs\", axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    # sub in the translated values\n",
        "    df[\"mixed_type\"] = df[\"mixed_type\"].map(mixed_type_dict)\n",
        "    df['subdistrict'] = df['subdistrict'].apply(lambda x: None if x not in subdistricts_list else x)\n",
        "    # Create the binary columns\n",
        "    df[\"is_pure_breed\"] = df[\"mixed_type\"].eq(\"PB\")\n",
        "    df['is_designer_breed'] = df['mixed_type'].eq('BB')\n",
        "\n",
        "    df[\"is_male_owner\"] = df[\"owner_gender\"] == 1\n",
        "    df[\"is_male_dog\"] = df[\"dog_gender\"] == 1\n",
        "    df = df.drop(columns=[\"owner_gender\", \"dog_gender\"])\n",
        "\n",
        "    df['dog_color'] = df['dog_color'].str.lower()\n",
        "    df = df.merge(dog_color_df,\n",
        "                  left_on=\"dog_color\",\n",
        "                  right_on=\"dog_color\",\n",
        "                  how=\"left\")\n",
        "\n",
        "    df[\"breed_1_de\"] = df[\"breed_1\"].str.lower().apply(remove_accents)\n",
        "    # df[\"breed_1_de\"] = df[\"breed_1_de\"].apply(remove_accents)\n",
        "    df[\"breed_2_de\"] = df[\"breed_2\"].str.lower().apply(remove_accents)\n",
        "    # df[\"breed_2_de\"] = df[\"breed_2_de\"].apply(remove_accents)\n",
        "\n",
        "    breeds_df = pd.DataFrame()\n",
        "    breeds_df = get_breed_standard(breeds_dict)\n",
        "\n",
        "    df = df.merge(\n",
        "    breeds_df.drop(columns=[\"breed_en\"]),\n",
        "    how='left',\n",
        "    left_on=\"breed_1_de\",\n",
        "    right_on=\"breed_de\",\n",
        "    suffixes=(\"\", \"_1\"),\n",
        "    )\n",
        "    df = df.merge(\n",
        "        breeds_df.drop(columns=[\"breed_en\"]),\n",
        "        how='left',\n",
        "        left_on=\"breed_2_de\",\n",
        "        right_on=\"breed_de\",\n",
        "        suffixes=(\"\", \"_2\"),\n",
        "    )\n",
        "\n",
        "    df = get_doodle_fix(df)\n",
        "\n",
        "    df = drop_concealed_nans(df)\n",
        "    df['is_small_dog'] = df['dog_size'].eq('K')\n",
        "\n",
        "    df = df.dropna(subset=[\"dog_age\", \"district\", \"subdistrict\"])\n",
        "\n",
        "    df[\"dog_age\"] = df[\"dog_age\"].astype(int)\n",
        "    df[\"district\"] = df[\"district\"].astype(int)\n",
        "    # get the pet count\n",
        "    df[\"pet_count\"] = df.groupby(\n",
        "        [\"owner_id\",\"reporting_year\"])[\"breed_1\"].transform(\"count\")\n",
        "    # Create an indicator variable for missing 'age_group_10' values\n",
        "    df[\"age_group_missing\"] = df[\"age_group_10\"].isna().astype(int)\n",
        "    df[\"age_group_10\"] = df[\"age_group_10\"].fillna(\n",
        "        df.groupby(\"owner_id\")[\"age_group_10\"].transform(\n",
        "            lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan))\n",
        "    # fill in the missing 'age_group_10' values and create the age_group_20 column\n",
        "    df[\"age_group_10\"] = df[\"age_group_10\"].fillna(-1).astype(int)\n",
        "    df[\"age_group_20\"] = df[\"age_group_10\"].apply(lambda x: -1\n",
        "                                                  if x == -1 else (x // 20) * 20)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_breed_standard(breeds_dict)\n"
      ],
      "metadata": {
        "id": "znOr8iyzatxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRIFcAuDiG78"
      },
      "outputs": [],
      "source": [
        "dog_data_columns_to_keep = [\n",
        "    \"reporting_year\",\n",
        "    \"owner_id\",\n",
        "    \"dog_size\",\n",
        "    \"dog_age\",\n",
        "    \"age_group_10\",\n",
        "    \"age_group_20\",\n",
        "    \"mixed_type\",\n",
        "    \"is_pure_breed\",\n",
        "    \"is_designer_breed\",\n",
        "    \"is_male_owner\",\n",
        "    \"is_male_dog\",\n",
        "    \"is_small_dog\",\n",
        "    \"dog_color_en\",\n",
        "    \"standard\",\n",
        "    \"standard_2\",\n",
        "    \"pet_count\",\n",
        "    \"district\",\n",
        "    \"subdistrict\",\n",
        "    \"age_group_missing\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxMKt6vbiG78"
      },
      "outputs": [],
      "source": [
        "# query each year separately then combine them\n",
        "# dog_reporting_year_dict = {\n",
        "#     reporting_year:\n",
        "#     preprocess_dog_data(\n",
        "#         hf.query_for_time_period(\n",
        "#             hf.sanitize_df_column_names(pd.read_csv(zurich_dog_data_link)),\n",
        "#             start_year=reporting_year,\n",
        "#             end_year=reporting_year + 1,\n",
        "#             year_col=\"date_date_year\",\n",
        "#         ), ) for reporting_year in range(2015, 2024)\n",
        "# }\n",
        "dog_data = pd.DataFrame()\n",
        "dog_data = preprocess_dog_data(pd.read_csv(zurich_dog_data_link))\n",
        "dog_reporting_year_dict = {\n",
        "    reporting_year:\n",
        "    dog_data.query('reporting_year == @reporting_year') for reporting_year in range(2015, 2024)\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dtViELsiG78"
      },
      "outputs": [],
      "source": [
        "dog_data = pd.concat(dog_reporting_year_dict.values())[dog_data_columns_to_keep]\n",
        "\n",
        "dog_data_to_2020 = pd.concat({\n",
        "    reporting_year: df\n",
        "    for reporting_year, df in dog_reporting_year_dict.items() if reporting_year <= 2020\n",
        "}.values())[dog_data_columns_to_keep]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_to_2020.is_small_dog.value_counts()"
      ],
      "metadata": {
        "id": "2jvSa254wky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt9EAf-XiG78"
      },
      "outputs": [],
      "source": [
        "# dog_data_train.to_csv(\"../data/processed_dog_data_train.csv\", index=False)\n",
        "hf.save_to_data(dog_data_to_2020, \"processed_dog_data_to_2020.csv\")\n",
        "hf.save_to_data(dog_data, \"processed_dog_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkUpEhMRiG78"
      },
      "outputs": [],
      "source": [
        "# get the dog density dataframe\n",
        "dog_density_pivot = create_dog_density_df(dog_data, area_df)\n",
        "hf.save_to_data(dog_density_pivot, \"dog_density_pivot.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTAxnqo3iG78"
      },
      "outputs": [],
      "source": [
        "\n",
        "# get the dog density pivot for all the years\n",
        "all_dog_densities = create_dog_density_df(dog_data, area_df)\n",
        "\n",
        "# convert to long format\n",
        "dog_densities_long = all_dog_densities.reset_index().melt(\n",
        "    id_vars='subdistrict', var_name='year', value_name='dog_density')\n",
        "\n",
        "# define the player widget\n",
        "player = pnw.Player(name='Year',\n",
        "                    start=2015,\n",
        "                    end=2023,\n",
        "                    value=2015,\n",
        "                    step=1,\n",
        "                    width=600,\n",
        "                    interval=5000)\n",
        "\n",
        "\n",
        "# @pn.cache(max_items=10)\n",
        "@pn.depends(player.param.value)\n",
        "def dog_density_chloropeth(query_year):\n",
        "    \"\"\"Returns a chloropleth map of the dog densities\"\"\"\n",
        "    poly_opts = dict(height=400,\n",
        "                    width=400,\n",
        "                    cmap='greens',\n",
        "                    color_levels=[0, 30, 60, 90, 120, 150],\n",
        "                    line_width=2,\n",
        "                    line_color='gray',\n",
        "                    color='dog_density',\n",
        "                    colorbar=True,\n",
        "                    tools=['hover'],\n",
        "                    xaxis='bare',\n",
        "                    yaxis='bare',\n",
        "                    active_tools=['box_zoom'],\n",
        "                    colorbar_position='bottom',\n",
        "                    backend_opts={'toolbar.autohide': True})\n",
        "    # filter the dog densities for the year\n",
        "    dog_density_year = dog_densities_long.query('year == @query_year')\n",
        "    # merge in the area dataframe to get the geometry\n",
        "    dog_density_year = area_gdf[['geometry','subdistrict']].merge(dog_density_year)\n",
        "    # plot the chloropleth map\n",
        "    return gv.Polygons(dog_density_year).opts(\n",
        "        **poly_opts, title=f\"Dog Density per Sub-District for {query_year}\")\n",
        "\n",
        "\n",
        "# combine the player widget and the chloropleth map\n",
        "# map_panel = pn.pane.HoloViews(dog_density_chloropeth)\n",
        "dog_map_panel = pn.panel(dog_density_chloropeth)\n",
        "# pn.Column(player, dog_map_panel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMcRq87biG78"
      },
      "source": [
        "#### Zurich Population Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zurich_pop_data_column_name_translations = {\n",
        "    'StichtagDatJahr': 'reporting_year',\n",
        "    'AlterVSort': 'age_from_sorting',\n",
        "    'AlterVCd': 'age',\n",
        "    'AlterVKurz': 'age_from_short_form',\n",
        "    'AlterV05Sort': 'age_from_05_sorting',\n",
        "    'AlterV05Cd': 'age_from_05_code',\n",
        "    'AlterV05Kurz': 'age_from_05_short_form',\n",
        "    'AlterV10Cd': 'age_group_10',\n",
        "    'AlterV10Kurz': 'age_from_10_short_form',\n",
        "    'AlterV20Cd': 'age_group_20',\n",
        "    'AlterV20Kurz': 'age_from_20_short_form',\n",
        "    'SexCd': 'gender_code',\n",
        "    'SexLang': 'gender_long',\n",
        "    'SexKurz': 'gender',\n",
        "    'KreisCd': 'district',\n",
        "    'KreisLang': 'district_long',\n",
        "    'QuarSort': 'subdistrict_sort',\n",
        "    'QuarCd': 'subdistrict',\n",
        "    'QuarLang': 'neighborhood',\n",
        "    'HerkunftSort': 'origin_sort',\n",
        "    'HerkunftCd': 'origin',\n",
        "    'HerkunftLang': 'origin_long',\n",
        "    'AnzBestWir': 'pop_count'\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OLWLo9iSI8QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_xaxis_age_group_10(plot, element):\n",
        "    \"\"\"Hook to update the x-axis ticker on the plot.\"\"\"\n",
        "    plot.state.xaxis.ticker = FixedTicker(ticks=list(range(0, 100,10)))\n",
        "\n",
        "\n",
        "zurich_pop_data = pd.read_csv(zurich_pop_link)\n",
        "zurich_pop_data = zurich_pop_data.rename(columns=zurich_pop_data_column_name_translations)\n",
        "zurich_pop_data['is_male'] = zurich_pop_data['gender_code'] == 1\n",
        "zurich_pop_data[\"is_swiss\"] = zurich_pop_data['origin'] == 1\n",
        "zurich_pop_data['district'] = zurich_pop_data['district'].astype(str).str.zfill(2)\n",
        "zurich_pop_data['subdistrict'] = zurich_pop_data['subdistrict'].astype(str).str.zfill(3)\n",
        "\n",
        "pop_columns = [\n",
        "    'reporting_year',\n",
        "    'age',\n",
        "    'age_group_10',\n",
        "    'age_group_20',\n",
        "    'is_male',\n",
        "    'is_swiss',\n",
        "    'pop_count',\n",
        "    'district',\n",
        "    'subdistrict',\n",
        "    'neighborhood',\n",
        "]\n",
        "\n",
        "\n",
        "zurich_pop_data_pivot = zurich_pop_data[pop_columns].groupby(['reporting_year','age_group_10'])['pop_count'].sum().reset_index().pivot(\n",
        "    index='age_group_10',\n",
        "    columns='reporting_year',\n",
        "    values='pop_count'\n",
        ").fillna(0).astype(int)\n",
        "zurich_pop_data_pivot.T.hvplot.heatmap(height=600, width=1000).opts(active_tools=['box_zoom'], title=\"Population count by Age Group and Reporting Year\")"
      ],
      "metadata": {
        "id": "WzcQ_er928Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zurich_pop_data['u20'] = zurich_pop_data['age_group_10'].apply(lambda x: True if x < 20 else False)\n",
        "zurich_pop_data['u10'] = zurich_pop_data['age_group_10'].apply(lambda x: True if x < 10 else False)\n",
        "\n",
        "total_pop = zurich_pop_data.groupby(['reporting_year','subdistrict'])['pop_count'].sum().reset_index(name='pop_count')\n",
        "child_pop = zurich_pop_data[zurich_pop_data['u20']].groupby(['reporting_year','subdistrict'])['pop_count'].sum().reset_index(name='u20_pop_count')\n",
        "infant_pop = zurich_pop_data[zurich_pop_data['u10']].groupby(['reporting_year','subdistrict'])['pop_count'].sum().reset_index(name='u10_pop_count')\n",
        "child_pop = child_pop.merge(infant_pop, on=['reporting_year', 'subdistrict'], how='left')\n",
        "\n",
        "child_pop['u20_pop_pct'] = (child_pop['u20_pop_count'] / total_pop['pop_count'])\n",
        "child_pop['u10_pop_pct'] = (child_pop['u10_pop_count'] / total_pop['pop_count'])\n",
        "\n"
      ],
      "metadata": {
        "id": "Sw68-tpB_nfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "child_pop_change = child_pop.set_index(['reporting_year', 'subdistrict'])[['u20_pop_count', 'u10_pop_count']].groupby(['subdistrict']).diff()\n",
        "child_pop_pct_change = child_pop.set_index(['reporting_year', 'subdistrict'])[['u20_pop_count', 'u10_pop_count']].groupby(['subdistrict']).pct_change()\n",
        "child_pop = child_pop.merge(child_pop_change, on=['reporting_year', 'subdistrict'], how='left', suffixes=('', '_change'))\n",
        "child_pop = child_pop.merge(child_pop_pct_change, on=['reporting_year', 'subdistrict'], how='left', suffixes=('', '_pct_change'))\n",
        "\n",
        "# round the pct and pct_change columns to 4dp\n",
        "child_pop = child_pop.round({'u20_pop_count_pct_change': 4, 'u10_pop_count_pct_change': 4, 'u20_pop_pct': 4, 'u10_pop_pct': 4})\n",
        "\n",
        "child_pop = child_pop.query('reporting_year >= 2015')\n",
        "child_pop"
      ],
      "metadata": {
        "id": "daK4Qfp26GnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pop_layout = hv.Layout()\n",
        "\n",
        "for year in range(2015, 2023):\n",
        "    pop_layout += zurich_pop_data_pivot[year].hvplot.bar(bar_width=25).opts(active_tools=['box_zoom'], hooks=[update_xaxis_age_group_10])\n",
        "\n",
        "pop_layout.cols(3)"
      ],
      "metadata": {
        "id": "qjKFYJwCIKhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zurich_pop_data_pivot_delta =  zurich_pop_data_pivot.T.diff().T.drop(columns=[1993])\n",
        "\n",
        "pop_delta_layout = hv.Layout()\n",
        "\n",
        "for year in range(2015, 2023):\n",
        "    pop_delta_layout += zurich_pop_data_pivot_delta[year].hvplot.bar(\n",
        "        bar_width=25, grid=True\n",
        "    ).opts(active_tools=['box_zoom'], hooks=[update_xaxis_age_group_10])\n",
        "\n",
        "pop_delta_layout.cols(3)"
      ],
      "metadata": {
        "id": "g4fSt_tNJdmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co8LYzqeiG79"
      },
      "outputs": [],
      "source": [
        "# combine all of the processing of the population data into a single function call\n",
        "@memory.cache\n",
        "def preprocess_pop_data(data):\n",
        "    \"\"\"Returns a preprocessed population dataframe\"\"\"\n",
        "    # combine the above cell into a single function call\n",
        "    df = data.copy()\n",
        "    df = df.rename(columns=zurich_pop_data_column_name_translations)\n",
        "    df['is_male'] = df['gender_code'] == 1\n",
        "    df[\"is_swiss\"] = df['origin'] == 1\n",
        "    df['district'] = df['district'].astype(str).str.zfill(2)\n",
        "    df['subdistrict'] = df['subdistrict'].astype(str).str.zfill(3)\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwvAtjmniG79"
      },
      "outputs": [],
      "source": [
        "def update_xaxis_year(plot, element):\n",
        "    \"\"\"Hook to update the x-axis ticker on the plot.\"\"\"\n",
        "    plot.state.xaxis.ticker = FixedTicker(ticks=list(range(2015, 2023)))\n",
        "\n",
        "# pop_data = preprocess_pop_data(pd.read_csv(zurich_pop_link)).query('reporting_year >= 2015')\n",
        "\n",
        "# get a pivot of the pop data with the subdistrict as the index and the reporting_year as the columns\n",
        "pop_data_pivot = zurich_pop_data.query('reporting_year >= 2015').groupby([\n",
        "    'reporting_year', 'subdistrict'\n",
        "])['pop_count'].sum().reset_index().pivot(index='subdistrict',\n",
        "                                          columns='reporting_year',\n",
        "                                          values='pop_count')\n",
        "\n",
        "# get the pop density pivot using the area dataframe\n",
        "pop_density_pivot = pop_data_pivot.div(\n",
        "    area_df.set_index('subdistrict')['subd_area_km2'], axis=0).round(2)\n",
        "# pop_density_pivot\n",
        "\n",
        "# normalize the data to plot an area chart\n",
        "pop_density_pivot_norm = pop_density_pivot.div(pop_density_pivot.sum(axis=0),\n",
        "                                               axis=1)\n",
        "\n",
        "pop_data_pivot_norm = pop_data_pivot.div(pop_data_pivot.sum(axis=0), axis=1)\n",
        "pop_overlay = pop_data_pivot_norm.T.hvplot(\n",
        "    line_width=1, ) * pop_data_pivot_norm.T.round(4).hvplot.scatter(\n",
        "        alpha=0.6,\n",
        "        size=10,\n",
        "        title='Normalized Population Data per Sub-District',\n",
        "        legend=False)\n",
        "\n",
        "pop_overlay.opts(\n",
        "    show_legend=False,\n",
        "    height=600,\n",
        "    width=800,\n",
        "    show_grid=True,\n",
        "    xlabel = '',\n",
        "    hooks=[update_xaxis_year],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pn.state.clear_caches()\n",
        "pn.state.kill_all_servers()\n",
        "zurich_pop_data"
      ],
      "metadata": {
        "id": "dSBAL3n6S5jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pop_data = zurich_pop_data.query('reporting_year >= 2015')\n",
        "pop_data_year_subdistrict =  pop_data.groupby(['reporting_year', 'subdistrict'])['pop_count'].sum().reset_index()\n",
        "pop_data_year_subdistrict.sort_values(by=['subdistrict', 'reporting_year'])\n"
      ],
      "metadata": {
        "id": "AgfHHyacde1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @pn.cache(max_items=10)\n",
        "@pn.depends(player.param.value)\n",
        "def people_density_chloropeth(query_year):\n",
        "    \"\"\"Returns a chloropleth map of the population densities\"\"\"\n",
        "    poly_opts = dict(height=400,\n",
        "                    width=400,\n",
        "                    cmap='greens',\n",
        "                    color_levels=[0, 1500, 3000, 4500, 6000, 7500],\n",
        "                    line_width=2,\n",
        "                    line_color='gray',\n",
        "                    color='pop_density',\n",
        "                    colorbar=True,\n",
        "                    tools=['hover'],\n",
        "                    xaxis='bare',\n",
        "                    yaxis='bare',\n",
        "                    active_tools=['box_zoom'],\n",
        "                    colorbar_position='bottom',\n",
        "                    backend_opts={'toolbar.autohide': True})\n",
        "    # query for that year\n",
        "    if query_year >2022:\n",
        "        query_year = 2022\n",
        "        pop_data_subdistrict = pop_data_year_subdistrict.query('reporting_year == @query_year')\n",
        "    else:\n",
        "        pop_data_subdistrict = pop_data_year_subdistrict.query('reporting_year == @query_year')\n",
        "    # merge the area dataframe and get the pop density\n",
        "    pop_area_df = area_gdf.merge(pop_data_subdistrict, on='subdistrict')\n",
        "    pop_area_df['pop_density'] = pop_area_df['pop_count'] / pop_area_df['subd_area_km2']\n",
        "    pop_polygon = gv.Polygons(pop_area_df)\n",
        "\n",
        "    pop_polygon.opts(\n",
        "        **poly_opts, title=f\"Pop Density per Sub-District for {query_year}\")\n",
        "    return pop_polygon\n",
        "\n",
        "\n",
        "# combine the player widget and the chloropleth map\n",
        "pop_map_panel = pn.pane.HoloViews(people_density_chloropeth)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "map_panel = pn.panel(gts.EsriImagery * gv.Polygons(zurich_map_gdf).opts(height=800, width=800, fill_alpha=0, xaxis='bare', yaxis='bare', line_width=2, line_color='white')  )\n",
        "pn.Column(player,\n",
        "          pn.Row(\n",
        "          pn.Column(dog_map_panel, pop_map_panel),\n",
        "          map_panel)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "NPfe4n6lLUy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4X1JmSGIYtYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI0Ekh5TiG79"
      },
      "outputs": [],
      "source": [
        "# save the processed population data to data folder\n",
        "# pop_data.to_csv(\"../data/processed_pop_data.csv\", index=False)\n",
        "hf.save_to_data(pop_data, \"processed_pop_data.csv\")"
      ]
    },
    {
      "source": [
        "def calculate_avg_dogs_owned(dataframe):\n",
        "  \"\"\"Calculates the average dogs owned per owner.\"\"\"\n",
        "  dataframe = dataframe.drop_duplicates(subset=['owner_id', 'reporting_year'])\n",
        "  dataframe['cumulative_pet_count'] = dataframe.groupby('owner_id')['pet_count'].cumsum()\n",
        "  dataframe['avg_dogs_owned'] = (dataframe['cumulative_pet_count'] / dataframe['years_a_dogowner']).round(2)\n",
        "  return dataframe['avg_dogs_owned']\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TzdXU_0qFSeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dog owner percentage ratio for each of the subdistricts for each of the years\n",
        "dogowner_data_year_subdistrict = dog_data.groupby(['reporting_year', 'subdistrict'])['owner_id'].nunique().rename('dogowner_count').reset_index()\n",
        "dog_data_year_subdistrict = dog_data.groupby(['reporting_year', 'subdistrict'])['owner_id'].size().rename('dog_count').reset_index()\n",
        "small_dog_year_subdistrict = dog_data.groupby(['reporting_year', 'subdistrict'])['is_small_dog'].sum().rename('small_dog_count').reset_index()\n",
        "\n",
        "reporting_year_subdistrict_counts = (\n",
        "    pop_data_year_subdistrict\n",
        "    .merge(dogowner_data_year_subdistrict)\n",
        "    .merge(dog_data_year_subdistrict)\n",
        "    .merge(small_dog_year_subdistrict)\n",
        ")\n",
        "\n",
        "reporting_year_subdistrict_counts['dogowner_ratio'] = (reporting_year_subdistrict_counts['dogowner_count'] / reporting_year_subdistrict_counts['pop_count']).round(4)\n",
        "reporting_year_subdistrict_counts['people_per_dog'] = (reporting_year_subdistrict_counts['pop_count'] / reporting_year_subdistrict_counts['dog_count']).astype(int)\n",
        "reporting_year_subdistrict_counts['small_dog_ratio'] = (reporting_year_subdistrict_counts['small_dog_count'] / reporting_year_subdistrict_counts['dog_count']).round(4)\n",
        "reporting_year_subdistrict_counts\n",
        "\n"
      ],
      "metadata": {
        "id": "EcWtO8DFr7ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dog_data['first_year_registered'] = dog_data.groupby(['owner_id'])['reporting_year'].transform('min')\n",
        "dog_data['years_a_dogowner'] = dog_data.groupby('owner_id')['reporting_year'].transform(lambda x: x.rank(method='dense')).astype(int)\n",
        "dog_data['is_new_owner'] = dog_data['years_a_dogowner'] == 1\n",
        "dog_data['is_returning_owner'] = dog_data['years_a_dogowner'] > 1\n",
        "\n",
        "dog_data\n"
      ],
      "metadata": {
        "id": "3JYY__DAuu8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "dog_data_unique = dog_data.drop_duplicates(subset=['owner_id', 'reporting_year'])\n",
        "dog_data_unique['cumulative_pet_count'] = dog_data_unique.groupby('owner_id')['pet_count'].cumsum()\n",
        "dog_data_unique['avg_dogs_owned'] = (dog_data_unique['cumulative_pet_count'] / dog_data_unique['years_a_dogowner']).round(2)\n",
        "dog_data_unique\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NtS72zNtKqkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if 'avg_dogs_owned' in dog_data.columns:\n",
        "    dog_data = dog_data.drop(columns=['avg_dogs_owned'])\n",
        "dog_data = dog_data.merge(dog_data_unique[['owner_id', 'reporting_year', 'avg_dogs_owned']], on=['owner_id', 'reporting_year'], how='left')\n",
        "dog_data\n"
      ],
      "metadata": {
        "id": "Bp02wwcBD070"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_owners_df = dog_data_unique[['owner_id', 'reporting_year', 'years_a_dogowner', 'is_new_owner', 'is_returning_owner', 'avg_dogs_owned', 'subdistrict']]\n",
        "dog_owners_df = dog_owners_df.groupby(['reporting_year', 'subdistrict'], as_index=False).agg({'avg_dogs_owned': 'mean', 'is_new_owner': 'sum', 'is_returning_owner': 'sum', 'years_a_dogowner': 'mean'})\n",
        "dog_owners_df = dog_owners_df.rename(\n",
        "    columns={\n",
        "        'is_new_owner': 'new_owner_count',\n",
        "        'is_returning_owner': 'returning_owner_count',\n",
        "        'years_a_dogowner': 'avg_owner_experience',\n",
        "        'avg_dogs_owned': 'avg_dogs_per_owner',\n",
        "        }\n",
        ")\n",
        "dog_owners_df = dog_owners_df.round({'avg_dogs_per_owner': 2, 'avg_owner_experience': 2})\n",
        "dog_owners_df\n"
      ],
      "metadata": {
        "id": "hogC6VuFP8Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reporting_year_subdistrict_counts = reporting_year_subdistrict_counts.merge(dog_owners_df, on=['reporting_year', 'subdistrict'])\n",
        "# reporting_year_subdistrict_counts = reporting_year_subdistrict_counts.merge(child_pop, on=['reporting_year', 'subdistrict'])\n",
        "reporting_year_subdistrict_counts"
      ],
      "metadata": {
        "id": "lloN9uHH81mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c63WskhqiG79"
      },
      "source": [
        "#### Zurich Income Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqiE6Bg6iG79"
      },
      "outputs": [],
      "source": [
        "zurich_income_data.sample()\n",
        "\n",
        "zurich_income_data_column_names_translations={\n",
        "    'StichtagDatJahr': 'reporting_year',\n",
        "    'QuarCd': 'subdistrict',\n",
        "    'QuarLang': 'neighborhood',\n",
        "    'QuarSort': 'subdistrict_sort',\n",
        "    'SteuerTarifSort': 'tax_rate_sort',\n",
        "    'SteuerTarifCd': 'tax_rate_code',\n",
        "    'SteuerTarifLang': 'tax_status',\n",
        "    'SteuerEinkommen_p50': 'median_income',\n",
        "    'SteuerEinkommen_p75': 'upper_q_income',\n",
        "    'SteuerEinkommen_p25': 'lower_q_income',\n",
        "\n",
        "}\n",
        "zurich_tax_status_translations = {\n",
        "    'Grundtarif': 'Basic tariff',\n",
        "    'Verheiratetentarif': 'Married tariff',\n",
        "    'Einelternfamilientarif': 'Single-parent family tariff',\n",
        "    }\n",
        "\n",
        "zurich_income_data = zurich_income_data.rename(columns=zurich_income_data_column_names_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmSCJRQ-iG79"
      },
      "outputs": [],
      "source": [
        "# Extract unique values from 'tax_tariff_long' column and convert to list\n",
        "# tax_tariff_long_de = zurich_income_data.tax_tariff_long.unique().tolist()\n",
        "\n",
        "# Translate the list to a dictionary using a helper function\n",
        "# tax_tariff_long_translated = translate_list_to_dict(tax_tariff_long_de)\n",
        "\n",
        "# Display the translated dictionary for verification\n",
        "# display(tax_tariff_long_translated)\n",
        "\n",
        "# Map the translated dictionary to 'tax_tariff_long' column, creating a new 'tax_status' column\n",
        "zurich_income_data[\"tax_status\"] = zurich_income_data['tax_status'].map(zurich_tax_status_translations)\n",
        "\n",
        "\n",
        "\n",
        "# Create a dictionary mapping old column names to new ones\n",
        "# income_data_column_mapping = {\n",
        "#     \"quar_lang\": \"neighborhood\",\n",
        "#     \"date_date_year\": \"reporting_year\",\n",
        "#     \"tax_income_p_50\": \"median_income\",\n",
        "#     \"tax_income_p_25\": \"lower_q_income\",\n",
        "#     \"tax_income_p_75\": \"upper_q_income\",\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "zurich_income_data[\"subdistrict\"] = (\n",
        "    zurich_income_data[\"subdistrict\"].astype(int).astype(\"string\").str.zfill(3)\n",
        ")\n",
        "zurich_income_data[\"district\"] = zurich_income_data[\"subdistrict\"].str[:2].astype(int)\n",
        "\n",
        "\n",
        "# Define a list of columns of interest for the final dataframe\n",
        "columns_of_interest_income = [\n",
        "    \"neighborhood\",\n",
        "    \"reporting_year\",\n",
        "    \"district\",\n",
        "    \"subdistrict\",\n",
        "    \"tax_status\",\n",
        "    \"median_income\",\n",
        "    \"lower_q_income\",\n",
        "    \"upper_q_income\",\n",
        "]\n",
        "\n",
        "\n",
        "display(\n",
        "    zurich_income_data.describe(include=\"all\")\n",
        "    .T.sort_values(by=\"unique\")\n",
        "    .infer_objects(copy=False)\n",
        "    .fillna(\"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrikO3gwiG7-"
      },
      "source": [
        "##### Handling Missing Income Data\n",
        "\n",
        "The income datasets only extend up to 2020 due to the tax data evaluation process. To fill in the missing data for 2021 and 2022, we explored 2 strategies:\n",
        "\n",
        "- Using all available data from 1999 onwards\n",
        "- Applying a log transformation to the data from 1999 onwards\n",
        "\n",
        "We assessed these strategies using various metrics, including mean absolute error, mean absolute percentage error, median absolute error, mean squared error, mean squared logarithmic error, and R2 score.\n",
        "\n",
        "We employed an auto ARIMA model to predict the values for 2021 and 2022. We tested the model's accuracy by predicting the values for 2019 and 2020 using the income data from 1999 to 2018 and the log-transformed income data from the same period.\n",
        "\n",
        "The log-transformed data provided more accurate predictions, as indicated by lower mean absolute error, mean absolute percentage error, and median absolute percentage error. Therefore, we chose this approach to predict the missing income data for 2021 and 2022 for the 34 neighborhoods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxi9mnFCiG7-"
      },
      "outputs": [],
      "source": [
        "income_from_1999 = (\n",
        "    zurich_income_data[columns_of_interest_income]\n",
        "    .groupby([\"reporting_year\", \"subdistrict\"])[\n",
        "        [\"median_income\", \"lower_q_income\", \"upper_q_income\"]\n",
        "    ]\n",
        "    .median()\n",
        "    .round(3)\n",
        "    .reset_index()\n",
        ")\n",
        "income_from_1999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4DnjrxviG7-"
      },
      "outputs": [],
      "source": [
        "def create_pivot(df, column):\n",
        "    \"\"\"Create a pivot table and a pivot table of the natural logarithm of the specified column.\"\"\"\n",
        "    df[f\"lg_{column}\"] = np.log(df[column])\n",
        "    pivot = (\n",
        "        df[[\"subdistrict\", \"reporting_year\", column]]\n",
        "        .pivot(index=\"reporting_year\", columns=\"subdistrict\", values=column)\n",
        "        .asfreq(\"YS\")\n",
        "    )\n",
        "    lg_pivot = (\n",
        "        df[[\"subdistrict\", \"reporting_year\", f\"lg_{column}\"]]\n",
        "        .pivot(index=\"reporting_year\", columns=\"subdistrict\", values=f\"lg_{column}\")\n",
        "        .asfreq(\"YS\")\n",
        "    )\n",
        "    return pivot, lg_pivot\n",
        "\n",
        "\n",
        "# Convert the 'reporting_year' column to datetime format\n",
        "income_from_1999[\"reporting_year\"] = pd.to_datetime(income_from_1999[\"reporting_year\"], format=\"%Y\")\n",
        "\n",
        "# Create pivot tables\n",
        "median_income_pivot_from_1999, lg_median_income_pivot_from_1999 = create_pivot(\n",
        "    income_from_1999, \"median_income\"\n",
        ")\n",
        "lower_q_income_pivot_from_1999, lg_lower_q_income_pivot_from_1999 = create_pivot(\n",
        "    income_from_1999, \"lower_q_income\"\n",
        ")\n",
        "upper_q_income_pivot_from_1999, lg_upper_q_income_pivot_from_1999 = create_pivot(\n",
        "    income_from_1999, \"upper_q_income\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jLvyxy5iG7-"
      },
      "outputs": [],
      "source": [
        "print(\"Pivot table of the natural logarithm of the median income:\")\n",
        "display(lg_median_income_pivot_from_1999.tail())\n",
        "\n",
        "print(\"\\nPivot table of the median income:\")\n",
        "median_income_pivot_from_1999.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FkYVS9uiG7-"
      },
      "outputs": [],
      "source": [
        "my_metrics = [\n",
        "    metrics.mean_absolute_error,\n",
        "    metrics.mean_absolute_percentage_error,\n",
        "    metrics.median_absolute_error,\n",
        "    metrics.mean_squared_error,\n",
        "    metrics.mean_squared_log_error,\n",
        "    metrics.r2_score,\n",
        "]\n",
        "\n",
        "\n",
        "def calculate_metrics(actual, predicted):\n",
        "    actual_values = actual.loc[predicted.index].values.ravel()\n",
        "    predicted_values = predicted.values.ravel()\n",
        "    return {\n",
        "        metric.__name__: metric(actual_values, predicted_values)\n",
        "        for metric in my_metrics\n",
        "    }\n",
        "\n",
        "\n",
        "calculate_metrics_partial = partial(calculate_metrics, median_income_pivot_from_1999)\n",
        "\n",
        "\n",
        "def convert_to_long_format(df, value_name=\"value\"):\n",
        "    \"\"\"Converts a DataFrame from wide to long format.\"\"\"\n",
        "    return (\n",
        "        df.unstack()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"level_0\": \"subdistrict\", \"level_1\": \"reporting_year\", 0: value_name})\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_arima_forecast(long_df, value_name, vline=2020, **kwargs):\n",
        "    \"\"\"Plots the value_name column of a Dataframe in long format.\"\"\"\n",
        "    forecast_color = kwargs.get(\"forecast_color\", \"gray\")\n",
        "    non_forecast_color = kwargs.get(\"non_forecast_color\", \"gray\")\n",
        "\n",
        "    v_line = hv.VLine(x=pd.to_datetime(f\"{vline}\")).opts(\n",
        "        color=\"red\", line_dash=\"dotted\"\n",
        "    )\n",
        "    forecast_df = long_df.loc[long_df[\"reporting_year\"] >= f\"{vline}\"]\n",
        "    forecast_line = forecast_df.hvplot(\n",
        "        x=\"reporting_year\",\n",
        "        y=value_name,\n",
        "        by=\"subdistrict\",\n",
        "        color=forecast_color,\n",
        "        line_dash=\"dashed\",\n",
        "        legend=False,\n",
        "    )\n",
        "    non_forecast_df = long_df.loc[long_df[\"reporting_year\"] <= f\"{vline}\"]\n",
        "    non_forecast_line = non_forecast_df.hvplot(\n",
        "        x=\"reporting_year\", y=value_name, by=\"subdistrict\", color=non_forecast_color\n",
        "    )\n",
        "    return non_forecast_line * forecast_line * v_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEDeIZdniG7-"
      },
      "outputs": [],
      "source": [
        "# Arima models to assess the forecast of the median income using log values\n",
        "lg_from_1999_pred_last_2 = hf.forecast_arima(\n",
        "    lg_median_income_pivot_from_1999, 2019, n_periods=2, model_desc=\"Log Model 1999\"\n",
        ")\n",
        "# Arima models to assess the forecast of the median income\n",
        "from_1999_pred_last_2 = hf.forecast_arima(\n",
        "    median_income_pivot_from_1999, 2019, n_periods=2, model_desc=\"From 1999\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlL_jRVNiG7-"
      },
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(\n",
        "    {\n",
        "        \"From 1999\": calculate_metrics_partial(from_1999_pred_last_2),\n",
        "        \"lg From 1999\": calculate_metrics_partial(lg_from_1999_pred_last_2.map(np.exp)),\n",
        "    }\n",
        ")\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze1o2ku3iG7-"
      },
      "source": [
        "The log-transformed data from 1999-2018 yielded the lower error predictions for 2019 and 2020 median income across 34 sub-districts, with the lower mean absolute error (MAE) and mean absolute percentage errors (MAPE). The mean absolute percentage error is especially relevant as it reflects the relative accuracy of predictions.\n",
        "\n",
        "We will now apply the same strategy and use the `auto_arima` algorithm again to estimate the unknown median values, lower quartile values and upper quartile values for the years 2021 and 2022. In doing this all our datasets would have those reporting_year years in common for easy alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVj06SYeiG7_"
      },
      "outputs": [],
      "source": [
        "def forecast_and_convert(df, start_year, periods, model_desc, column_name):\n",
        "    \"\"\"Forecast using autoarima and convert to long format.\"\"\"\n",
        "    lg_pred = hf.forecast_arima(\n",
        "        df, start_year, n_periods=periods, model_desc=model_desc\n",
        "    )\n",
        "    long_format = convert_to_long_format(lg_pred.map(np.exp), column_name)\n",
        "    return long_format\n",
        "\n",
        "\n",
        "# Forecast and convert to long format\n",
        "long_format_median_income_21_22 = forecast_and_convert(\n",
        "    lg_median_income_pivot_from_1999, 2021, 2, \"Log Model 1999\", \"median_income\"\n",
        ")\n",
        "long_format_lower_q_income_21_22 = forecast_and_convert(\n",
        "    lg_lower_q_income_pivot_from_1999,\n",
        "    2021,\n",
        "    2,\n",
        "    \"Log lower_q Model 1999\",\n",
        "    \"lower_q_income\",\n",
        ")\n",
        "long_format_upper_q_income_21_22 = forecast_and_convert(\n",
        "    lg_upper_q_income_pivot_from_1999,\n",
        "    2021,\n",
        "    2,\n",
        "    \"Log upper_q Model 1999\",\n",
        "    \"upper_q_income\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX3_hoyXiG7_"
      },
      "outputs": [],
      "source": [
        "# merge the three forecasted dataframes\n",
        "income_forecast_df = long_format_median_income_21_22.merge(\n",
        "    long_format_lower_q_income_21_22, on=[\"subdistrict\", \"reporting_year\"]\n",
        ").merge(long_format_upper_q_income_21_22, on=[\"subdistrict\", \"reporting_year\"])\n",
        "\n",
        "income_forecast_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B-8y9FFiG7_"
      },
      "outputs": [],
      "source": [
        "# Concatenate the forecasted dataframes with the original dataframe\n",
        "income_from_1999_with_forcasted = pd.concat(\n",
        "    [income_from_1999, income_forecast_df], axis=0\n",
        ")[[\"reporting_year\", \"subdistrict\", \"median_income\", \"lower_q_income\", \"upper_q_income\"]]\n",
        "\n",
        "# Align the reporting_year years with the dog_data\n",
        "income_from_1999_with_forcasted = hf.query_for_time_period(\n",
        "    income_from_1999_with_forcasted, year_col='reporting_year'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JopMs829iG7_"
      },
      "outputs": [],
      "source": [
        "# save the processed income data to data folder\n",
        "# income_from_1999_with_forcasted.to_csv(\"../data/processed_income_data.csv\", index=False)\n",
        "hf.save_to_data(income_from_1999_with_forcasted, \"processed_income_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NPv5illiG7_"
      },
      "outputs": [],
      "source": [
        "income_whole_numbers = income_from_1999_with_forcasted.set_index(\n",
        "    ['reporting_year', 'subdistrict']).astype(int).reset_index()\n",
        "income_whole_numbers['reporting_year'] = income_whole_numbers['reporting_year'].dt.year\n",
        "# add jitter to the reporting_year column\n",
        "income_whole_numbers['reporting_year_jittered'] = income_whole_numbers[\n",
        "    'reporting_year'] + np.random.normal(0, 0.1, size=len(income_whole_numbers))\n",
        "income_whole_numbers.hvplot.scatter(\n",
        "    x='reporting_year_jittered',\n",
        "    y=['lower_q_income', 'median_income', 'upper_q_income'],\n",
        "    size=10,\n",
        "    by='subdistrict',\n",
        "    xlabel='',\n",
        "    title='Income Data per Sub-District',\n",
        "    xticks=[2014, 2016, 2018, 2020, 2022],\n",
        "    height=400,\n",
        "    width=600,\n",
        "    hover_cols=['subdistrict'],\n",
        "    legend='top',\n",
        "    alpha=0.8).opts(backend_opts={'toolbar.autohide': True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4sNt3USiG7_"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    plot_arima_forecast(\n",
        "        income_from_1999_with_forcasted,\n",
        "        \"median_income\",\n",
        "        vline=2020,\n",
        "        non_forecast_color=\"blue\",\n",
        "    ).opts(height=800, active_tools=[\"box_zoom\"])\n",
        "    * plot_arima_forecast(\n",
        "        income_from_1999_with_forcasted,\n",
        "        \"upper_q_income\",\n",
        "        vline=2020,\n",
        "        non_forecast_color=\"green\",\n",
        "    )\n",
        ").opts(height=800, active_tools=[\"box_zoom\"], title=\"Median Income and Upper Quartile Income\", show_legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhlLXh_CiG7_"
      },
      "source": [
        "#### Zurich Household Dataset\n",
        "\n",
        "For the household datasets we first rename some of the columns so that they are more readable and consistent with the other datasets. We then process it to obtain an average household size per neighborhood, weighted by the number of households."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zurich_household_data.sample()\n",
        "\n",
        "# Define a dictionary to map old column names to new ones\n",
        "zurich_household_data_columns_translations = {\n",
        "    'StichTagDatJahr': 'reporting_year',\n",
        "    'QuarSort': 'subdistrict',\n",
        "    'QuarLang': 'neighborhood',\n",
        "    'KreisSort': 'district',\n",
        "    'KreisLang': 'district_long',\n",
        "    'hh_groesseSort': 'household_size',\n",
        "    'hh_groesseLang': 'household_size_long',\n",
        "    'AnzHH': 'household_count',\n",
        "    'AnzBestWir': 'resident_count',\n",
        "}\n",
        "\n",
        "# Rename the columns\n",
        "zurich_household_data = zurich_household_data.rename(columns=zurich_household_data_columns_translations)"
      ],
      "metadata": {
        "id": "1MJ0OkXe55Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iyLQ812iG7_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create new columns\n",
        "zurich_household_data[\"subdistrict\"] = (\n",
        "    zurich_household_data[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")\n",
        "zurich_household_data[\"district\"] = (\n",
        "    zurich_household_data[\"subdistrict\"].str[:2].astype(int)\n",
        ")\n",
        "zurich_household_data[\"household_size\"] = (\n",
        "    zurich_household_data[\"household_size\"].astype(\"string\").str.zfill(2)\n",
        ")\n",
        "\n",
        "# Create a dataframe with only the columns of interest\n",
        "columns_of_interest_household = [\n",
        "    \"neighborhood\",\n",
        "    \"reporting_year\",\n",
        "    \"district\",\n",
        "    \"subdistrict\",\n",
        "    \"household_size\",\n",
        "    \"household_count\",\n",
        "    \"resident_count\",\n",
        "]\n",
        "hh_data = zurich_household_data[columns_of_interest_household]\n",
        "\n",
        "# Align the reporting_year years with the dog_data\n",
        "# hh_data = hf.query_for_time_period(hh_data)\n",
        "\n",
        "# Display dataframe info and first 10 rows\n",
        "hh_data.info()\n",
        "hh_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hh_size_data = (\n",
        "    hh_data[['reporting_year', 'subdistrict', 'household_size', 'household_count']]\n",
        "    .pivot(index=['reporting_year', 'subdistrict'], columns='household_size', values='household_count').add_prefix('hh_')\n",
        "    # .reset_index()\n",
        "    .sort_values(by=['subdistrict', 'reporting_year'])\n",
        ")\n",
        "hh_size_data['hh_24'] = hh_size_data['hh_02'] + hh_size_data['hh_03'] + hh_size_data['hh_04']\n",
        "hh_size_data['hh_56'] = hh_size_data['hh_05'] + hh_size_data['hh_06']\n"
      ],
      "metadata": {
        "id": "dP3ZaY_or2kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hh_size_data_changes = hh_size_data.groupby(['subdistrict']).diff()\n",
        "hh_reporting_year_subdistrict =  hh_size_data.merge(hh_size_data_changes, on=['reporting_year', 'subdistrict'], suffixes=('', '_change')).reset_index()\n",
        "hh_reporting_year_subdistrict.head()\n"
      ],
      "metadata": {
        "id": "wjEx2U2U_TL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb_df = pd.DataFrame()\n",
        "comb_df = (\n",
        "    reporting_year_subdistrict_counts\n",
        "    .merge(dog_owners_df, on=['reporting_year', 'subdistrict'])\n",
        "    .merge(child_pop, on=['reporting_year', 'subdistrict'])\n",
        "    .merge(hh_reporting_year_subdistrict, on=['reporting_year', 'subdistrict'])\n",
        "    .query('reporting_year > 2015')\n",
        "    # .set_index(['reporting_year', 'subdistrict'])\n",
        ")\n",
        "comb_df.describe().T\n"
      ],
      "metadata": {
        "id": "_1Fcm_7B5evb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_for_pairplot = ['hh_01_change', 'hh_24_change', 'hh_56_change',\n",
        "                     'u10_pop_count_change', 'u20_pop_count_change', 'u10_pop_count_pct_change', 'u20_pop_count_pct_change',\n",
        "                     'returning_owner_count', 'new_owner_count', 'avg_dogs_per_owner', 'people_per_dog',\n",
        "                     'small_dog_ratio',\n",
        "                     'reporting_year']\n",
        "\n",
        "sns.pairplot(comb_df[cols_for_pairplot], kind='reg', corner=True, plot_kws={'scatter_kws': {'s': 1}} )"
      ],
      "metadata": {
        "id": "KtNC64pcf5Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "comb_df[cols_for_pairplot].corr().round(2).hvplot.heatmap(width=800, height=800, cmap='coolwarm_r', ).opts(xrotation=90, active_tools=['box_zoom'], symmetric=True )"
      ],
      "metadata": {
        "id": "rBBXBgfRj_vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb_df.sample()"
      ],
      "metadata": {
        "id": "K58dBiqbKnSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def highly_correlated_cols(df, threshold=0.95):\n",
        "    \"\"\"\n",
        "    Identifies highly correlated columns in a DataFrame.\n",
        "    \"\"\"\n",
        "    corr_matrix = df.corr().abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "    return to_drop\n",
        "\n",
        "highly_correlated_cols(comb_df)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YfDiPqy37Pvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIjOTeSNiG8A"
      },
      "outputs": [],
      "source": [
        "# Calculate total residents per subdistrict\n",
        "hh_data[\"total_residents\"] = hh_data.groupby([\"reporting_year\", \"subdistrict\"])[\n",
        "    \"resident_count\"\n",
        "].transform(\"sum\")\n",
        "\n",
        "# Calculate the total households per subdistrict\n",
        "hh_data[\"total_households\"] = hh_data.groupby([\"reporting_year\", \"subdistrict\"])[\n",
        "    \"household_count\"\n",
        "].transform(\"sum\")\n",
        "\n",
        "# Average household size\n",
        "hh_data[\"avg_household_size\"] = hh_data[\"total_residents\"] / hh_data[\"total_households\"]\n",
        "\n",
        "# Calculate weighted average household size\n",
        "hh_data[\"resident_portion\"] = hh_data[\"resident_count\"] / hh_data[\"total_residents\"]\n",
        "# household_data\n",
        "\n",
        "hh_grouped_data = (\n",
        "    hh_data.groupby([\"reporting_year\", \"subdistrict\", \"neighborhood\"])[\n",
        "        [\"avg_household_size\", \"total_households\"]\n",
        "    ]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "hh_grouped_data.info()\n",
        "hh_grouped_data.describe(include=\"all\").T.sort_values(by=\"unique\").infer_objects(\n",
        "    copy=False\n",
        ").fillna(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hh_data = hh_data.sort_values(by=['subdistrict', 'household_size', 'reporting_year'])\n",
        "hh_data['hh_count_delta'] = hh_data.groupby(['subdistrict', 'household_size'])['household_count'].transform(lambda x: x.diff()).fillna(0).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "x4CCv7RVv5Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyv8nzvJiG8A"
      },
      "outputs": [],
      "source": [
        "# plot a stacked bar chart for the reporting_year  2022 for each subdistrict\n",
        "# the size of the bar is proportional to the resident_portion, color by household_size\n",
        "hh_data_2022 = hh_data.query('reporting_year == 2022').reset_index()\n",
        "hh_data_2022"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pn.state.clear_caches()\n",
        "pn.state.kill_all_servers()"
      ],
      "metadata": {
        "id": "-zyigyrpz3eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# barh_chart_player = pnw.Player(name='Year',value=2016,start=2015,end=2022,step=1,width=500,interval=2000)\n",
        "bar_chart_player = pnw.Player(value=2016,start=2015,end=2022,step=1,width=400,interval=2000)\n",
        "\n",
        "hh_size = pnw.ToggleGroup(\n",
        "    behavior='check',\n",
        "    options = ['01', '02', '03', '04', '05', '06'],\n",
        "    value=['01'],\n",
        "    button_style = 'outline',\n",
        "    width=400)\n",
        "\n",
        "unique_household_sizes = hh_data['household_size'].unique()\n",
        "\n",
        "hh_colors = list(cc.glasbey)\n",
        "hh_color_mapping = {size: color for size, color in zip(unique_household_sizes, hh_colors)}\n",
        "\n"
      ],
      "metadata": {
        "id": "OvDt18ruvoWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hh_color_mapping"
      ],
      "metadata": {
        "id": "iEbiIZEgHzsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-f67q2RiG8A"
      },
      "outputs": [],
      "source": [
        "\n",
        "hh_bar_opts = dict(\n",
        "    cmap=hh_color_mapping,\n",
        "    stacked=True,\n",
        "    xlabel='',\n",
        "    height=400,\n",
        "    width=1000,\n",
        "    alpha=0.6,\n",
        "    legend='right',\n",
        "    hover_cols=['subdistrict'],\n",
        "    line_color='white',\n",
        "    grid=True\n",
        ")\n",
        "\n",
        "\n",
        "@pn.depends(bar_chart_player.param.value, hh_size.param.value)\n",
        "def household_count_bar(query_year, hh_size):\n",
        "    \"\"\"Returns a bar chart of the household count per sub-district for the specified year.\"\"\"\n",
        "    hh_data_year = hh_data.copy()\n",
        "    hh_data_year = hh_data_year[hh_data_year['reporting_year'] == query_year].copy()\n",
        "    hh_data_year = hh_data_year[hh_data_year['household_size'].isin(hh_size)]\n",
        "    return hh_data_year.hvplot.bar(\n",
        "        x='subdistrict',\n",
        "        y='household_count',\n",
        "        by='household_size',\n",
        "        title=f'Household Count per Sub-District | {query_year}',\n",
        "        **hh_bar_opts\n",
        "    ).opts(backend_opts={'toolbar.autohide': True}, active_tools=['box_zoom'], )\n",
        "\n",
        "\n",
        "# put the bar chart on a panel\n",
        "household_count_panel = pn.panel(household_count_bar)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@pn.depends(bar_chart_player.param.value, hh_size.param.value)\n",
        "def hh_count_delta_bar(query_year, hh_size):\n",
        "    \"\"\"Returns a bar chart of the household count per sub-district for the specified year.\"\"\"\n",
        "    hh_data_copy = hh_data.copy()\n",
        "    hh_data_year_1 = hh_data_copy[hh_data_copy['reporting_year'] == query_year]\n",
        "    hh_data_year_1 = hh_data_year_1[hh_data_year_1['household_size'].isin(hh_size)]\n",
        "    hh_data_delta_bars =  hh_data_year_1.hvplot.bar(\n",
        "        x='subdistrict',\n",
        "        y='hh_count_delta',\n",
        "        by='household_size',\n",
        "        title=f'Change in Household Count per Sub-District | {query_year}',\n",
        "        **hh_bar_opts\n",
        "    ).redim(reporting_year='year', hh_count_delta='Change from Prior Year', subdistrict='tract', hh_size='household_size')\n",
        "    hh_data_delta_bars.opts(backend_opts={'toolbar.autohide': True}, active_tools=['box_zoom'], )\n",
        "    # add a hline for the xaxis\n",
        "    zero_line = hv.HLine(y=0).opts(color='black', line_width=0.5)\n",
        "\n",
        "    return hh_data_delta_bars * zero_line\n",
        "\n",
        "\n",
        "# put the bar chart on a panel\n",
        "household_delta_count_panel = pn.panel(hh_count_delta_bar)\n"
      ],
      "metadata": {
        "id": "nl4CBDHbk0XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use representative_point() so that the label is in the polygon that it is identifying\n",
        "subdistrict_gdf['label_pos_x'] = subdistrict_gdf['geometry'].apply(lambda x: x.representative_point().coords[:][0][0])\n",
        "subdistrict_gdf['label_pos_y'] = subdistrict_gdf['geometry'].apply(lambda x: x.representative_point().coords[:][0][1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CtXF6WusJZAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the Geodataframe for the subdistrivct labels\n",
        "\n",
        "subdistrict_labels_gdf = z_gdf['z_gdf_0']\n",
        "subdistrict_labels_gdf = subdistrict_labels_gdf.rename(columns={'kuerzel': 'subdistrict'})\n",
        "subdistrict_labels_gdf['subdistrict'] = subdistrict_labels_gdf['subdistrict'].str.zfill(3)\n",
        "subdistrict_labels_gdf.head()"
      ],
      "metadata": {
        "id": "Dy7s4gyPGOF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subdistrict_labels_gdf['x'] = subdistrict_labels_gdf.geometry.x\n",
        "subdistrict_labels_gdf['y'] = subdistrict_labels_gdf.geometry.y\n",
        "\n",
        "\n",
        "subdistrict_labels = hv.Labels(\n",
        "    {\n",
        "        ('x', 'y'): subdistrict_labels_gdf[['x', 'y']].values.tolist(),\n",
        "        'text': subdistrict_labels_gdf['subdistrict'].values.tolist()\n",
        "    },\n",
        "    ['x', 'y'], 'text'\n",
        ")\n",
        "subdistrict_labels.opts(\n",
        "        opts.Labels(text_font_size='10pt')\n",
        "    )\n"
      ],
      "metadata": {
        "id": "bYgGbooEDArG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "neighborhood_map = hv.Polygons(subdistrict_gdf, vdims=['subdistrict']).opts(\n",
        "    xlabel='',\n",
        "    ylabel='',\n",
        "    xaxis='bare',\n",
        "    yaxis='bare',\n",
        "    width=800,\n",
        "    height=800,\n",
        "    fill_alpha=0.5,\n",
        "    cmap=['white'],\n",
        "    tools=['hover'],\n",
        "    active_tools=['box_zoom'],\n",
        "    title=\"Polygon showing the Subdistricts location\"\n",
        ")\n",
        "subdistrict_panel = pn.panel(neighborhood_map * subdistrict_labels)\n",
        "\n",
        "\n",
        "# hv.help(gv.Labels)\n",
        "# subdistrict_labels\n",
        "\n",
        "pn.Row(\n",
        "    pn.Column(\n",
        "    pn.Row(bar_chart_player, hh_size),\n",
        "   household_count_panel, household_delta_count_panel),\n",
        "    subdistrict_panel\n",
        ")\n"
      ],
      "metadata": {
        "id": "xmo7NmaQBt3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKfWy292iG8A"
      },
      "outputs": [],
      "source": [
        "# save the processed household data to data folder\n",
        "# hh_data.to_csv(\"../data/processed_household_data.csv\", index=False)\n",
        "hf.save_to_data(hh_grouped_data, \"processed_household_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZelGeg59iG8A"
      },
      "source": [
        "#### Merged Datasets\n",
        "Now that we have all datasets with some common columns we can attempt to merge them just to see if anything stands out. Since all of our processed files will save to the data folder we can simply just load them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "UPfh2p5RiG8A"
      },
      "outputs": [],
      "source": [
        "# Load processed data from CSV files\n",
        "subdistrict_gdf = gpd.read_file(\"../data/zurich_neighborhoods.geojson\")\n",
        "districts_gdf = gpd.read_file(\"../data/zurich_districts.geojson\")\n",
        "processed_dog_data = pd.read_csv(\"../data/processed_dog_data.csv\")\n",
        "processed_pop_data = pd.read_csv(\"../data/processed_pop_data.csv\")\n",
        "processed_income_data = pd.read_csv(\"../data/processed_income_data.csv\")\n",
        "processed_household_data = pd.read_csv(\"../data/processed_household_data.csv\")\n",
        "\n",
        "# Display the last 5 rows of the processed dog data\n",
        "processed_dog_data.tail()\n",
        "\n",
        "# Pad 'subdistrict' column with leading zeros to make it 3 digits\n",
        "processed_dog_data[\"subdistrict\"] = (\n",
        "    processed_dog_data[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")\n",
        "\n",
        "# Group dog data by 'reporting_year' and 'subdistrict' and calculate the size of each group\n",
        "grouped_dog_data = processed_dog_data.groupby([\"reporting_year\", \"subdistrict\"])\n",
        "dog_count_to_merge = grouped_dog_data.size().rename(\"dog_count\").reset_index()\n",
        "\n",
        "# Count unique 'owner_id' in each group\n",
        "owner_count_to_merge = (\n",
        "    grouped_dog_data[\"owner_id\"].nunique().rename(\"owner_count\").reset_index()\n",
        ")\n",
        "\n",
        "# Count the number of small dogs (dog_size='K') in each group\n",
        "small_dog_count_to_merge = (\n",
        "    processed_dog_data.loc[processed_dog_data[\"dog_size\"] == \"K\"]\n",
        "    .groupby([\"reporting_year\", \"subdistrict\"])\n",
        "    .size()\n",
        "    .rename(\"small_dog_count\")\n",
        "    .reset_index()\n",
        ")\n",
        "# Count the number of pure breed dogs in each group\n",
        "pure_breed_count_to_merge = (\n",
        "    processed_dog_data.loc[processed_dog_data[\"is_pure_breed\"]]\n",
        "    .groupby([\"reporting_year\", \"subdistrict\"])\n",
        "    .size()\n",
        "    .rename(\"pure_breed_count\")\n",
        "    .reset_index()\n",
        ")\n",
        "# Count the number of male owners in each group\n",
        "male_owner_count_to_merge = (\n",
        "    processed_dog_data.loc[processed_dog_data[\"is_male_owner\"] == True]\n",
        "    .groupby([\"reporting_year\", \"subdistrict\"])[\"owner_id\"]\n",
        "    .nunique()\n",
        "    .reset_index(name=\"male_owner_count\")\n",
        ")\n",
        "# Pad 'subdistrict' column in population data and group by 'reporting_year' and 'subdistrict'\n",
        "processed_pop_data[\"subdistrict\"] = (\n",
        "    processed_pop_data[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")\n",
        "pop_to_merge = (\n",
        "    processed_pop_data.groupby([\"reporting_year\", \"subdistrict\"])\n",
        "    .agg({\"pop_count\": \"sum\"})\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Pad 'subdistrict' column in income data, truncate 'reporting_year' to 4 digits, and group by 'reporting_year' and 'subdistrict'\n",
        "processed_income_data[\"subdistrict\"] = (\n",
        "    processed_income_data[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")\n",
        "processed_income_data[\"reporting_year\"] = processed_income_data[\"reporting_year\"].str[:4].astype(int)\n",
        "income_to_merge = (\n",
        "    processed_income_data.groupby([\"reporting_year\", \"subdistrict\"])\n",
        "    .agg({\"median_income\": \"mean\", \"lower_q_income\": \"mean\", \"upper_q_income\": \"mean\"})\n",
        "    .round(3)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Pad 'subdistrict' column in household data and group by 'reporting_year' and 'subdistrict'\n",
        "processed_household_data[\"subdistrict\"] = (\n",
        "    processed_household_data[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")\n",
        "hh_to_merge = (\n",
        "    processed_household_data.groupby([\"reporting_year\", \"subdistrict\"])\n",
        "    .agg({\"avg_household_size\": \"mean\", \"total_households\": \"mean\"})\n",
        "    .round(3)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Merge all the grouped data into a single DataFrame\n",
        "z_subd_merged = (\n",
        "    dog_count_to_merge.merge(owner_count_to_merge)\n",
        "    .merge(male_owner_count_to_merge)\n",
        "    .merge(small_dog_count_to_merge)\n",
        "    .merge(pure_breed_count_to_merge)\n",
        "    .merge(pop_to_merge)\n",
        "    .merge(income_to_merge)\n",
        "    .merge(hh_to_merge)\n",
        "    .merge(subdistrict_gdf[[\"subdistrict\", \"district\", \"subd_area_km2\"]])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "gUCgvRSpiG8A"
      },
      "outputs": [],
      "source": [
        "z_subd_merged[\"small_dog_frac\"] = round(\n",
        "    z_subd_merged[\"small_dog_count\"] / z_subd_merged[\"dog_count\"], 3\n",
        ")\n",
        "# Add in the owner to population ratio\n",
        "z_subd_merged[\"owner_pop_ratio\"] = (\n",
        "    z_subd_merged[\"owner_count\"] / z_subd_merged[\"pop_count\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Add in the geometry data and subd_area_km2 for density calculations\n",
        "z_subd_merged[\"dog_subd_density\"] = (\n",
        "    z_subd_merged[\"dog_count\"] / z_subd_merged[\"subd_area_km2\"]\n",
        ")\n",
        "z_subd_merged[\"hh_subd_density\"] = (\n",
        "    z_subd_merged[\"total_households\"] / z_subd_merged[\"subd_area_km2\"]\n",
        ")\n",
        "z_subd_merged[\"pop_subd_density\"] = (\n",
        "    z_subd_merged[\"pop_count\"] / z_subd_merged[\"subd_area_km2\"]\n",
        ")\n",
        "z_subd_merged[\"owner_subd_density\"] = (\n",
        "    z_subd_merged[\"owner_count\"] / z_subd_merged[\"subd_area_km2\"]\n",
        ")\n",
        "\n",
        "\n",
        "z_subd_merged_2015 = hf.query_for_time_period(z_subd_merged, 2015, 2016, year_col=\"reporting_year\")\n",
        "z_subd_merged.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4LDxfTiG8A"
      },
      "source": [
        "##### Dimensionality Reduction: UMAP vs PCA\n",
        "\n",
        "When dealing with high-dimensional data, dimensionality reduction techniques like UMAP (Uniform Manifold Approximation and Projection) and PCA (Principal Component Analysis) are often used. These techniques transform the data into a lower-dimensional space, making it easier to visualize and analyze.\n",
        "##### PCA (Principal Component Analysis)\n",
        "**PCA** is a linear technique that focuses on preserving the global structure of the data, which refers to the overall variance in the data. It's computationally efficient but may not always capture the relationships in the data when reducing to very low dimensions.\n",
        "\n",
        "On the other hand, **UMAP** is a nonlinear technique that aims to preserve both the global and local structure of the data. The local structure refers to the relationships in the data when reducing to very low dimensions, like 2D or 3D. This makes UMAP potentially more effective than PCA for visualization purposes, but it comes at the cost of higher computational intensity.\n",
        "\n",
        "Here's a summary of the key differences:\n",
        "\n",
        "| | UMAP | PCA |\n",
        "|---|---|---|\n",
        "| **Preserves Global Structure** | Yes | Yes |\n",
        "| **Preserves Local Structure** | Yes | No |\n",
        "| **Computational Intensity** | High | Low |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiaJATJliG8B"
      },
      "outputs": [],
      "source": [
        "# declare a panel widget for buttons\n",
        "n_neighbors_slider = pnw.IntSlider(\n",
        "    value=15, start=5, end=100, step=5, width=400, name=\"n_neighbors\"\n",
        ")\n",
        "my_clusters_slider = pnw.IntSlider(\n",
        "    value=5, start=2, end=35, step=1, width=400, name=\"n_clusters\"\n",
        ")\n",
        "min_dist_button = pnw.RadioButtonGroup(options=[0.1, 0.2, 0.4, 0.7], value=0.2)\n",
        "# List of values to try for n_neighbors and min_dist\n",
        "n_neighbors_values = list(range(5, 51, 5))\n",
        "min_dist_values = [0.1, 0.2, 0.4, 0.7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8dlkfg5iG8B"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "# X_set = z_subd_merged.copy()\n",
        "X_set = comb_df.copy()\n",
        "\n",
        "columns_to_ohe = [\"reporting_year\"]\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "for column in columns_to_ohe:\n",
        "    # Convert the district column to string\n",
        "    X_set[column] = X_set[column].astype(str)\n",
        "    encoded_features = ohe.fit_transform(X_set[column].values.reshape(-1, 1))\n",
        "    df_encoded_features = pd.DataFrame(\n",
        "        encoded_features, columns=ohe.categories_[0], index=X_set.index\n",
        "    )\n",
        "df_encoded_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comb_df.isna().sum()\n",
        "# X_set\n",
        "comb_df"
      ],
      "metadata": {
        "id": "srP3nD1gNBL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "kBy5IHaZiG8B"
      },
      "outputs": [],
      "source": [
        "# List the columns to be used on PCA analysis\n",
        "columns_to_use = [\n",
        "    \"dog_subd_density\",\n",
        "    \"hh_subd_density\",\n",
        "    \"pop_subd_density\",\n",
        "    \"owner_subd_density\",\n",
        "    \"dog_count\",\n",
        "    \"pop_count\",\n",
        "    \"owner_count\",\n",
        "    \"male_owner_count\",\n",
        "    \"pure_breed_count\",\n",
        "    \"owner_pop_ratio\",\n",
        "    \"total_households\",\n",
        "    \"median_income\",\n",
        "    \"avg_household_size\",\n",
        "    \"small_dog_frac\",\n",
        "    \"small_dog_count\",\n",
        "    \"subd_area_km2\",\n",
        "    # \"lower_q_income\",\n",
        "    # \"upper_q_income\",\n",
        "]\n",
        "\n",
        "# State the range for the number of clusters to be used for the KMeans algorithm\n",
        "cluster_range = range(2, 20)\n",
        "\n",
        "# scale the data\n",
        "scaler = StandardScaler()\n",
        "# This is for just one year, so 2015 only\n",
        "# scaled_data = scaler.fit_transform(z_subd_merged_2015[columns_to_use])\n",
        "\n",
        "# scaled_data = scaler.fit_transform(z_subd_merged[columns_to_use])\n",
        "cols_for_pca = comb_df.columns.tolist()\n",
        "cols_for_pca.remove(\"subdistrict\")\n",
        "cols_for_pca.remove(\"reporting_year\")\n",
        "scaled_data = scaler.fit_transform(comb_df[cols_for_pairplot])\n",
        "\n",
        "scaled_data_df = pd.DataFrame(scaled_data, columns=cols_for_pairplot, index=comb_df.index)\n",
        "X_set = pd.concat([scaled_data_df, df_encoded_features], axis=1)\n",
        "pca = PCA()\n",
        "x_pca = pca.fit_transform(X_set)\n",
        "\n",
        "\n",
        "# Create the DataFrame for the PCA data\n",
        "pca_df = pd.DataFrame(\n",
        "    x_pca,\n",
        "    columns=[f\"PC{i+1}\" for i in range(x_pca.shape[1])],\n",
        "    index=X_set.index,\n",
        ")\n",
        "# Create the plot of 1st and 2nd principal components\n",
        "pca_plot = pca_df.hvplot.scatter(\n",
        "    x=\"PC1\",\n",
        "    y=\"PC2\",\n",
        "    title=\"PCA of Zurich Sub-Districts\",\n",
        "    hover_cols=[\"subdistrict\"],\n",
        "    width=500,\n",
        "    height=500,\n",
        "    colorbar=True,\n",
        "    grid=True\n",
        ").opts(active_tools=['box_zoom'])\n",
        "\n",
        "# Create the explained variance plot\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.cumsum(explained_variance)\n",
        "explained_variance_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Principal Component\": range(1, len(explained_variance) + 1),\n",
        "        \"Explained Variance\": explained_variance,\n",
        "        \"Cumulative Explained Variance\": cumulative_explained_variance,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Explained variance plot\n",
        "ev_plot = explained_variance_df.hvplot(\n",
        "    x=\"Principal Component\",\n",
        "    y=[\"Explained Variance\", \"Cumulative Explained Variance\"],\n",
        "    # kind=\"bar\",\n",
        "    title=\"Explained Variance by Principal Component\",\n",
        "    width=500,\n",
        "    height=500,\n",
        "    shared_axes=False,\n",
        ").opts(active_tools=['box_zoom'],legend_position=\"top_right\")\n",
        "\n",
        "# Show layout of the PCA plots\n",
        "hv.Layout([pca_plot, ev_plot]).cols(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC08xKNuiG8B"
      },
      "outputs": [],
      "source": [
        "x_pca.shape\n",
        "X_set.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBNQNfCNiG8B"
      },
      "outputs": [],
      "source": [
        "# Fit the PCA model\n",
        "pca = PCA()\n",
        "pca.fit(scaled_data_df)\n",
        "h_color = \"green\"\n",
        "line_opts = dict(color=h_color, line_dash=\"dashed\",\n",
        "                 line_alpha=0.8, line_width=1)\n",
        "\n",
        "# Plot the cumulative explained variance ratio with number of components\n",
        "n = pca.n_components_\n",
        "cvr = np.r_[0, pca.explained_variance_ratio_.cumsum()]\n",
        "cvr_plot = hv.Curve(cvr)\n",
        "cvr_scatter = hv.Scatter(cvr_plot).opts(size=5, color=\"darkgray\", alpha=0.8)\n",
        "\n",
        "# Add dashed lines at the 5th component and its corresponding cumulative variance ratio\n",
        "v5_line = hv.Path([(5, 0), (5, cvr[5])]).opts(**line_opts)\n",
        "h5_line = hv.Path([(0, cvr[5]), (5, cvr[5])]).opts(**line_opts)\n",
        "v5_label = hv.Text(\n",
        "    x=5,\n",
        "    y=0,\n",
        "    text=\" 5 Components\",\n",
        "    valign=\"bottom\",\n",
        "    halign=\"left\",\n",
        ").opts(color=h_color)\n",
        "h5_label = hv.Text(\n",
        "    x=0,\n",
        "    y=cvr[5],\n",
        "    text=f\" {cvr[5]:.2f}\",\n",
        "    halign=\"left\",\n",
        "    valign=\"top\",\n",
        ").opts(color=h_color)\n",
        "\n",
        "\n",
        "hv.Overlay(cvr_plot * cvr_scatter * v5_line * h5_line * v5_label * h5_label).opts(\n",
        "    title=\"Cumulative Explained Variance Ratio\",\n",
        "    xlabel=\"Components\",\n",
        "    ylabel=\"\",\n",
        "    ylim=(0, 1),\n",
        "    xlim=(0, n + 1),\n",
        "    height=400,\n",
        "    width=400,\n",
        "    tools=[\"hover\"],\n",
        "    show_legend=False,\n",
        "    yticks=list(np.linspace(0, 1, 3)),\n",
        "    xticks=list(range(0, n + 1, 2)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBAbc-NuiG8B"
      },
      "source": [
        "We then selected only the principal components which accounted for 95% of the `cumulative explained variance ratio`. Using these reduced number of  dimensions, we were then able to perform a `Kmeans` cluster analysis to identify clusters of our `subdistricts` with similar features. Although we have 34 neighborhoods within 12 districts, the neighborhoods may not necessarily be clustered along those lines. To assess the quality of our clustering results, we use the `silhouette score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "UvRKZHf-iG8B"
      },
      "outputs": [],
      "source": [
        "# transform the data\n",
        "pca_data = pca.transform(scaled_data_df)\n",
        "\n",
        "pca_data_df = pd.DataFrame(pca_data, columns=[f\"PC{i}\" for i in range(1, n + 1)])\n",
        "\n",
        "\n",
        "# Get the number of components that explain at least 95% of the variance\n",
        "num_components = np.where(cvr >= 0.95)[0][0]\n",
        "\n",
        "# Select the first `num_components` columns\n",
        "pca_data_df_reduced = pca_data_df.iloc[:, :num_components]\n",
        "\n",
        "print(f\"{pca_data_df_reduced.shape[1]} components explain 95% of the variance\")\n",
        "pca_data_df_reduced.head()\n",
        "\n",
        "pca_data_widget = pnw.DataFrame(pca_data_df_reduced, name=\"PCA Data\")\n",
        "\n",
        "\n",
        "@pn.depends(\n",
        "    num_clusters=my_clusters_slider.param.value, pca_dataset=pca_data_widget.param.value\n",
        ")\n",
        "def get_pca_plots(num_clusters, pca_dataset):\n",
        "    \"\"\"Create PCA plots for the given number of clusters.\"\"\"\n",
        "\n",
        "    cluster_labels = hf.compute_kmeans_labels(pca_dataset, num_clusters)\n",
        "\n",
        "    clustered_dataset_df = hf.create_clustered_data_df(pca_dataset, cluster_labels)\n",
        "\n",
        "    clustered_dataset_df = hf.add_columns(\n",
        "        clustered_dataset_df, comb_df, [\"subdistrict\"]\n",
        "    )\n",
        "\n",
        "    plot = hf.create_scatterplot_with_origin_cross(\n",
        "        clustered_dataset_df,\n",
        "        x=\"PC1\",\n",
        "        y=\"PC2\",\n",
        "        title=f\"K-means Clustering with k={num_clusters}\",\n",
        "    )\n",
        "    return plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtAJEoQHiG8C"
      },
      "outputs": [],
      "source": [
        "@memory.cache\n",
        "def calculate_clusters_scores(embeddings_dict, cluster_range=None):\n",
        "    \"\"\"Calculate K-means clustering scores for all embeddings in the dictionary and return a DataFrame.\"\"\"\n",
        "    # Create a list to store the dataframes\n",
        "    score_dataframes = []\n",
        "    if cluster_range is None:\n",
        "        # Define the range of cluster numbers\n",
        "        cluster_range = list(range(2, 20))\n",
        "\n",
        "    for embedding_key, embeddings in tqdm(embeddings_dict.items(), desc=\"Calculating scores\"):\n",
        "        for num_clusters in cluster_range:\n",
        "            kmeans_model = KMeans(n_init=20, n_clusters=num_clusters, random_state=628).fit(\n",
        "                embeddings\n",
        "            )\n",
        "            cluster_labels = kmeans_model.labels_\n",
        "\n",
        "            silhouette_score = round(\n",
        "                metrics.silhouette_score(embeddings, cluster_labels), 3\n",
        "            )\n",
        "            calinski_harabasz_score = round(\n",
        "                metrics.calinski_harabasz_score(embeddings, cluster_labels),\n",
        "            )\n",
        "            davies_bouldin_score = round(\n",
        "                metrics.davies_bouldin_score(embeddings, cluster_labels), 3\n",
        "            )\n",
        "\n",
        "            # Create a DataFrame\n",
        "            scores_df = pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding_key\": [embedding_key],\n",
        "                    \"num_clusters\": [num_clusters],\n",
        "                    \"silhouette_score\": [silhouette_score],\n",
        "                    \"calinski_harabasz_score\": [calinski_harabasz_score],\n",
        "                    \"davies_bouldin_score\": [davies_bouldin_score],\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Append the dataframe to the list\n",
        "            score_dataframes.append(scores_df)\n",
        "\n",
        "    # Concatenate all dataframes in the list into a single dataframe\n",
        "    result_df = pd.concat(score_dataframes, ignore_index=True)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "# get the scores for the PCA data\n",
        "\n",
        "pca_scores = hf.calculate_clusters_scores({\"PCA\": pca_data_df_reduced}, cluster_range)\n",
        "# pca_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "AC9ZUiB_iG8C"
      },
      "outputs": [],
      "source": [
        "# Define the color mapping functions\n",
        "def color_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return [\"color: red\" if v else \"\" for v in is_max]\n",
        "\n",
        "\n",
        "def color_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return [\"color: blue\" if v else \"\" for v in is_min]\n",
        "\n",
        "\n",
        "# Apply the color mapping functions to the DataFrame\n",
        "styled_df = pca_scores.style.apply(\n",
        "    color_max, subset=[\"silhouette_score\", \"calinski_harabasz_score\"]\n",
        ").apply(color_min, subset=[\"davies_bouldin_score\"])\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj4_Jvp-iG8C"
      },
      "source": [
        "We are able to see how the neighborhoods are clustered in the scatterplot below (on an arbitrary plane off the first 2 principle components), which is colored by the `cluster` label below. We also included two other metric which although we did not use, were still useful to look at.\n",
        "- `Calinski-Harabasz`: ratio of the between-clusters and inter-clusters dispersion for all clusters. The higher the value, the better the clustering.\n",
        "- `Davies-Bouldin`: Similarity between clusters Comparing the distance between the clusters and the size of the clusters themselves. A lower score here relates to a model with better separation between the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "_gklPLfJiG8C"
      },
      "outputs": [],
      "source": [
        "# list the cluster_metrics to be considered\n",
        "cluster_metrics = [\n",
        "    \"silhouette_score\",\n",
        "    \"calinski_harabasz_score\",\n",
        "    \"davies_bouldin_score\",\n",
        "]\n",
        "\n",
        "pn.Column(my_clusters_slider, pn.pane.HoloViews(get_pca_plots))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        },
        "id": "fz5MA1XpiG8C"
      },
      "outputs": [],
      "source": [
        "# Get the best number of clusters for silhouette score\n",
        "best_n_clusters = pca_scores.loc[pca_scores[\"silhouette_score\"].idxmax()][\n",
        "    \"num_clusters\"\n",
        "]\n",
        "print(f\"Best number of clusters Silhouette: {best_n_clusters}\")\n",
        "\n",
        "# Apply k-means clustering to the PCA data\n",
        "kmeans = KMeans(n_init=20, n_clusters=best_n_clusters, random_state=628)\n",
        "kmeans.fit(pca_data_df_reduced)\n",
        "pca_data_df_reduced[\"cluster\"] = kmeans.labels_\n",
        "\n",
        "# add columns district, subdistrict, and reporting_year\n",
        "pca_data_df_reduced = hf.add_columns(\n",
        "    pca_data_df_reduced, comb_df, [\"subdistrict\", \"reporting_year\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Group the DataFrame by 'subdistrict' and get the set of 'cluster' for each 'subdistrict'\n",
        "subdistrict_pca_cluster = (\n",
        "    pca_data_df_reduced[[\"subdistrict\", \"reporting_year\", \"cluster\"]]\n",
        "    .groupby([\"subdistrict\"])[\"cluster\"]\n",
        "    .apply(set)\n",
        "    .reset_index(name=\"pca_cluster_set\")\n",
        ")\n",
        "# subdistrict_pca_cluster"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the number of unique clusters for each 'subdistrict' and store it in a new column\n",
        "subdistrict_pca_cluster[\"subdistrict_cluster_count\"] = subdistrict_pca_cluster[\n",
        "    \"pca_cluster_set\"\n",
        "].apply(len)\n",
        "\n",
        "# Check if there are any 'subdistrict' with more than one unique cluster and sum them\n",
        "(~subdistrict_pca_cluster[\"subdistrict_cluster_count\"] == 1).sum()\n",
        "\n",
        "# unravel the set in the pcs column\n",
        "subdistrict_pca_cluster[\"pca_cluster\"] = subdistrict_pca_cluster[\n",
        "    \"pca_cluster_set\"\n",
        "].apply(lambda x: list(x)[0])\n",
        "\n",
        "\n",
        "# Plot the Clusters (colormap) and Districts(white line) and subdistricts (black (default) line)\n",
        "subdistrict_gdf.merge(\n",
        "    subdistrict_pca_cluster[[\"subdistrict\", \"pca_cluster\"]]\n",
        ").hvplot.polygons(\n",
        "    aspect=\"equal\",\n",
        "    geo=True,\n",
        "    tiles=\"EsriImagery\",\n",
        "    color=\"pca_cluster\",\n",
        "    # cmap=\"glasbey_dark\",\n",
        "    colormap=cc.glasbey_dark[: best_n_clusters + 1],\n",
        "    hover_cols=[\"all\"],\n",
        "    title=f\"PCA || {best_n_clusters} Clusters\",\n",
        "    xaxis=\"bare\",\n",
        "    yaxis=\"bare\",\n",
        "    colorbar=False,\n",
        "    alpha=0.3,\n",
        ") * gv.Polygons(\n",
        "    districts_gdf\n",
        ").opts(\n",
        "    line_color=\"white\", line_width=2, fill_alpha=0.02, height=600, width=600\n",
        ")"
      ],
      "metadata": {
        "id": "2IFcQMTt8zeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSsFhYPViG8C"
      },
      "source": [
        "We can also use these cluster labels to see how the neighborhoods are distributed on a map. We can see that the clusters are not necessarily along the district lines (white outline) but still have a strong geographical contiguity. This clustering was based only on the non geometry features, meaning excluding the `area` and geographical coordinates as features for example. Features like the `area` and the geographical corordinates will not make much sense for pca as they have no variation within them from year to year as the other features do. As our features also included `reporting_year` which was the year feature, some `subdistricts` were classified into different clusters for different years. This may be due to some non linearity in our system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACnCTrybiG8C"
      },
      "outputs": [],
      "source": [
        "# Compute correlations between original variables and principal components\n",
        "\n",
        "corr = pd.DataFrame(\n",
        "    pca.components_.T[:, :num_components],\n",
        "    columns=pca_data_df.columns[:num_components],\n",
        ")\n",
        "corr[\"variable\"] = cols_for_pairplot\n",
        "# corr_var_df = pd.DataFrame({'variable': cols_for_pca})\n",
        "# corr = pd.concat([corr_var_df, corr], axis=1)\n",
        "\n",
        "corr = corr.melt(id_vars=\"variable\", var_name=\"PC\", value_name=\"corr\")\n",
        "# plot using hvplot\n",
        "corr_plot = corr.hvplot.bar(\n",
        "    x=\"PC\",\n",
        "    y=\"corr\",\n",
        "    by=\"variable\",\n",
        "    width=2200,\n",
        "    height=500,\n",
        "    title=\"Correlation Between Original Variables and Principal Components\",\n",
        "    ylabel=\"\",\n",
        "    tools=[\"hover\"],\n",
        ").opts(active_tools=[\"box_zoom\"])\n",
        "corr_plot.opts(xrotation=90, xlabel=\"\", gridstyle={\"grid_line_color\": \"lightgray\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFDL5mtCiG8D"
      },
      "source": [
        "The bar plot helps us to understand The principal components in terms of the original variables.\n",
        "- a high positive value means the original variable on the principal component are strongly positively correlated\n",
        "- a high negative value means that the original variable are strongly negatively correlated.\n",
        "\n",
        "We began to see a pattern emerging here with the 'density' features being highly correlated with other 'density' features, likewise for the counts and the 'ratios' features. This is shown more clearly in the circle correlation plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEx6eXL7iG8D"
      },
      "outputs": [],
      "source": [
        "correlations = pd.DataFrame(pca.components_, columns=scaled_data_df.columns).T\n",
        "correlations.columns = [f\"PC{i}\" for i in range(1, n + 1)]\n",
        "labels_df = pd.DataFrame(\n",
        "    {\"x\": correlations[\"PC1\"], \"y\": correlations[\"PC2\"], \"label\": correlations.index}\n",
        ")\n",
        "circle_correlation = correlations.hvplot.scatter(\n",
        "    x=\"PC1\",\n",
        "    y=\"PC2\",\n",
        "    width=800,\n",
        "    height=500,\n",
        "    title=\"Correlation Between Principal Components\",\n",
        "    hover_cols=[\"index\"],\n",
        "    xlim=(-1, 1),\n",
        ")\n",
        "\n",
        "(\n",
        "    circle_correlation\n",
        "    # plot the dog_subd_density point as a red color\n",
        "    * labels_df.loc[labels_df[\"label\"] == \"dog_subd_density\"]\n",
        "    .hvplot.points()\n",
        "    .opts(color=\"red\")\n",
        "    * hv.VLine(0).opts(color=\"gray\", line_dash=\"dotted\")\n",
        "    * hv.HLine(0).opts(color=\"gray\", line_dash=\"dotted\")\n",
        "    # * hv.Labels(labels_df, [\"x\", \"y\"], \"label\").opts(\n",
        "    #     yoffset=-0.05, xoffset=-0.05, text_alpha=0.6\n",
        "    # )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efyNlkENiG8D"
      },
      "outputs": [],
      "source": [
        "print(labels_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrkvbZ0ZiG8D"
      },
      "source": [
        "##### UMAP (Uniform Manifold Approximation and Projection)\n",
        "We now do a similar analysis using UMAP. The `UMAP` class has a few more parameters to play with than `PCA` so it can be slightly more confusing but you gain more control over the process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pn.state.clear_caches()\n",
        "pn.state.kill_all_servers()"
      ],
      "metadata": {
        "id": "aM_i-LE37vn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ff6EvEBiG8D"
      },
      "outputs": [],
      "source": [
        "# Get the embeddings dictionary which\n",
        "embeddings_dict = hf.compute_embeddings(\n",
        "    scaled_data_df, n_neighbors_values, min_dist_values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu3VLv7liG8D"
      },
      "outputs": [],
      "source": [
        "@pn.cache(max_items=20)\n",
        "@pn.depends(n_neighbors_slider.param.value, my_clusters_slider.param.value)\n",
        "def get_umap_plot(neighbor, n_clusters):\n",
        "    \"\"\"Returns a HoloViews scatter plot of the UMAP embeddings.\"\"\"\n",
        "    plots = []\n",
        "    embeddings_keys = [(neighbor, min_distance) for min_distance in min_dist_values]\n",
        "\n",
        "    for embedding_key in embeddings_keys:\n",
        "        # Retrieve the specific embeddings from the dictionary using the key\n",
        "        embeddings = embeddings_dict[embedding_key]\n",
        "\n",
        "        # Compute the cluster labels for the current embeddings\n",
        "        cluster_labels = hf.compute_kmeans_labels(embeddings, n_clusters)\n",
        "\n",
        "        # Create a DataFrame that combines the embeddings and their corresponding cluster labels\n",
        "        embeddings_df = hf.create_clustered_data_df(embeddings, cluster_labels)\n",
        "        embeddings_df = hf.add_columns(\n",
        "            embeddings_df, z_subd_merged, [\"subdistrict\"]\n",
        "        )\n",
        "\n",
        "        # Generate a plot for the current embeddings and append it to the list of plots\n",
        "        plot = hf.create_scatterplot_with_origin_cross(\n",
        "            embeddings_df,\n",
        "            title=f\"UMAP || {n_clusters=} || {neighbor=} || min_dist={embedding_key[1]}\",\n",
        "        )\n",
        "        plots.append(plot)\n",
        "\n",
        "    return hv.Layout(plots).cols(2).opts(shared_axes=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuvVIRsziG8D"
      },
      "outputs": [],
      "source": [
        "clusters_scores_df = calculate_clusters_scores(\n",
        "    embeddings_dict, cluster_range\n",
        ").sort_values(by=\"silhouette_score\", ascending=False)\n",
        "\n",
        "# get the embeddings for the best silhouette score,\n",
        "# the best calinski_harabasz score and\n",
        "best_silhouette_embeddings = clusters_scores_df.loc[\n",
        "    clusters_scores_df[\"silhouette_score\"].idxmax()\n",
        "]\n",
        "best_calinski_harabasz_embeddings = clusters_scores_df.loc[\n",
        "    clusters_scores_df[\"calinski_harabasz_score\"].idxmax()\n",
        "]\n",
        "# the best davies_bouldin score. Here we look for the minimum value\n",
        "best_davies_bouldin_embeddings = clusters_scores_df.loc[\n",
        "    clusters_scores_df[\"davies_bouldin_score\"].idxmin()\n",
        "]\n",
        "print(\n",
        "    f\"\"\"\n",
        "Best Silhouette Score:{best_silhouette_embeddings[\"silhouette_score\"]:.3f}\\n\n",
        "Best Calinski Harabasz Score:{best_calinski_harabasz_embeddings[\"calinski_harabasz_score\"]:.0f}\\n\n",
        "Best Davies Bouldin Score:{best_davies_bouldin_embeddings[\"davies_bouldin_score\"]:.3f}\n",
        "\"\"\"\n",
        ")\n",
        "# display the 3 embeddings\n",
        "display(best_silhouette_embeddings)\n",
        "display(best_calinski_harabasz_embeddings)\n",
        "display(best_davies_bouldin_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRNgY9S7iG8E"
      },
      "source": [
        "We calculate all the embeddings beforehand and store them in a dictionary so that our calls using the widget do not have to recompute them each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fD5RFJZiG8E"
      },
      "outputs": [],
      "source": [
        "# Create panel for UMAP plot\n",
        "umap_panel = pn.pane.HoloViews(get_umap_plot)\n",
        "pn.Column(pn.Row(n_neighbors_slider, my_clusters_slider), umap_panel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqjjSi27iG8E"
      },
      "outputs": [],
      "source": [
        "# get the embedding from embedding dict for the best silhouette score\n",
        "best_silhouette_embeddings_key = best_silhouette_embeddings[\"embedding_key\"]\n",
        "best_silhouette_n_clusters = best_silhouette_embeddings[\"num_clusters\"]\n",
        "\n",
        "# get the actual embeddings\n",
        "embeddings_of_best_silhouette = embeddings_dict[best_silhouette_embeddings_key]\n",
        "print(f\"Best silhouette embedding key: {best_silhouette_embeddings_key}\")\n",
        "print(f\"Best silhouette n_clusters: {best_silhouette_n_clusters}\")\n",
        "\n",
        "# get the cluster labels for the best silhouette score\n",
        "best_silhouette_cluster_labels = hf.compute_kmeans_labels(\n",
        "    embeddings_of_best_silhouette, best_silhouette_embeddings[\"num_clusters\"]\n",
        ")\n",
        "\n",
        "# see which subdistricts are in which cluster\n",
        "best_silhouette_embeddings_df = hf.create_clustered_data_df(\n",
        "    embeddings_of_best_silhouette, best_silhouette_cluster_labels\n",
        ")\n",
        "# Add in the subdistrict, district, and reporting_year columns\n",
        "best_silhouette_embeddings_df = hf.add_columns(\n",
        "    best_silhouette_embeddings_df, z_subd_merged, [\n",
        "        \"subdistrict\", \"district\", \"reporting_year\"]\n",
        ")\n",
        "\n",
        "# group by cluster to see which subdistricts-reporting_year combinations are in which cluster\n",
        "clusters_subdistricts = (\n",
        "    best_silhouette_embeddings_df.groupby(\n",
        "        [\"cluster\", \"reporting_year\"])[\"subdistrict\"]\n",
        "    .apply(list)\n",
        "    .reset_index(name=\"subdistricts_cluster\")\n",
        ")\n",
        "# Ensure that each subdistrict is only in one cluster\n",
        "# clusters_subdistricts.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAhN6zpDiG8E"
      },
      "outputs": [],
      "source": [
        "# Group the DataFrame by 'subdistrict' and get the set of 'cluster' for each 'subdistrict'\n",
        "subdistrict_umap_cluster = (\n",
        "    best_silhouette_embeddings_df.groupby(\"subdistrict\")[\"cluster\"]\n",
        "    .apply(set)\n",
        "    .reset_index(name=\"umap_cluster_set\")\n",
        ")\n",
        "# subdistrict_umap_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjhgJA9kiG8E"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the number of unique clusters for each 'subdistrict' and store it in a new column\n",
        "subdistrict_umap_cluster[\"subdistrict_cluster_count\"] = subdistrict_umap_cluster[\n",
        "    \"umap_cluster_set\"\n",
        "].apply(len)\n",
        "\n",
        "# subdistrict_umap_cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check if there are any 'subdistrict' with more than one unique cluster and sum them\n",
        "(~subdistrict_umap_cluster[\"subdistrict_cluster_count\"] == 1).sum()\n",
        "# unravel the set in the umap_cluster_set column\n",
        "subdistrict_umap_cluster[\"umap_cluster\"] = subdistrict_umap_cluster[\n",
        "    \"umap_cluster_set\"\n",
        "].apply(lambda x: list(x)[0])\n",
        "\n",
        "# Calculate the number of subdistricts in each cluster and store it in a new column\n",
        "subdistrict_umap_cluster[\"cluster_count\"] = subdistrict_umap_cluster.groupby(\n",
        "    [\"umap_cluster\"]\n",
        ")[\"subdistrict\"].transform(\"count\")\n",
        "\n",
        "subdistrict_gdf.merge(\n",
        "    subdistrict_umap_cluster[[\"subdistrict\", \"umap_cluster\"]]\n",
        ").hvplot.polygons(\n",
        "    aspect=\"equal\",\n",
        "    geo=True,\n",
        "    tiles=\"EsriImagery\",\n",
        "    color=\"umap_cluster\",\n",
        "    colormap=cc.glasbey_dark[: best_silhouette_n_clusters + 1],\n",
        "    colorbar=False,\n",
        "    hover_cols=[\"all\"],\n",
        "    xaxis=\"bare\",\n",
        "    yaxis=\"bare\",\n",
        "    alpha=0.6,\n",
        "    title=f\"UMAP Clusters || {best_silhouette_n_clusters} clusters\",\n",
        ") * gv.Polygons(\n",
        "    districts_gdf\n",
        ").opts(\n",
        "    line_color=\"white\", line_width=2, fill_alpha=0.02, height=600, width=600\n",
        ")"
      ],
      "metadata": {
        "id": "2fKBp5a0Aji8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21lg4IXCiG8E"
      },
      "outputs": [],
      "source": [
        "subdistrict_umap_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7wdzPWpiG8F"
      },
      "source": [
        "With **UMAP**, we can see that our `subdistricts` did not migrate to different clusters as the `reporting_year` year changed. This is a good sign as the algorithm was able to pick up on this similarity without us explicitly mentioning it and without explicitly giving it a feature like `area` which would have made it more obvious. as it does not vary from year to year for a single district.\n",
        "\n",
        "This was something that the **PCA** was not able to pick up on with out the `area` feature, but the chloropleth maps for both of these Dimension Reduction algorithms looks very similar. This gives some proof of predictability to our data, and we can look into with more details in the exploratory data analysis phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVOtB_BFiG8F"
      },
      "outputs": [],
      "source": [
        "subdistrict_umap_cluster.sort_values(by=\"cluster_count\", ascending=False)\n",
        "# best_silhouette_embeddings_df.cluster.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_for_pairplot"
      ],
      "metadata": {
        "id": "MzqokYDPTypM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8noj_TXiG8F"
      },
      "outputs": [],
      "source": [
        "# Check which features correlate with the New Owner count\n",
        "corr = pd.DataFrame(scaled_data, columns=cols_for_pairplot).corr()\n",
        "corr[\"new_owner_count\"].sort_values().hvplot.barh(\n",
        "    width=800, height=500, title=\"Correlation with New Owner count\"\n",
        ").opts(xlabel=\"\", active_tools=[\"box_zoom\"])\n",
        "corr_new_owner_count = (\n",
        "    corr[\"new_owner_count\"]\n",
        "    .sort_values()\n",
        "    .hvplot.barh(width=500, height=500, title=\"Correlation with New Owner count\")\n",
        "    .opts(active_tools=[\"box_zoom\"])\n",
        ")\n",
        "\n",
        "\n",
        "# Get a corr bar plot for the dog count for next to the New Owner count plot\n",
        "corr_returning_owner_count = (\n",
        "    corr[\"returning_owner_count\"]\n",
        "    .sort_values()\n",
        "    .hvplot.barh(\n",
        "        width=500, height=500, title=\"Correlation with Returning Owner Count\", xlim=(None, 1)\n",
        "    )\n",
        "    .opts(active_tools=[\"box_zoom\"])\n",
        ")\n",
        "corr_new_owner_count + corr_returning_owner_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKd8LZh9iG8G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brought over\n",
        "## EDA\n",
        "### Load in Data"
      ],
      "metadata": {
        "id": "WvltPiqVp1TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import clear_output\n",
        "# from panel import widgets as pnw  # For widgets and formatting\n",
        "# import numpy as np  # For number computing\n",
        "# import pandas as pd  # For data manipulation\n",
        "# import panel as pn\n",
        "# from bokeh.models import FixedTicker\n",
        "# import holoviews as hv\n",
        "from holoviews import opts\n",
        "import geoviews as gv\n",
        "from geoviews import tile_sources as gts\n",
        "import geopandas as gpd\n",
        "import hvplot.pandas  # noqa\n",
        "import spatialpandas as spd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tqdm import tqdm  # Progress bars\n",
        "from wordcloud import WordCloud  # For generating word cloud visualizations\n",
        "\n",
        "import helper_functions as hf\n",
        "\n",
        "# clear_output()"
      ],
      "metadata": {
        "id": "Rtl54YYPqAIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbhGzqgnnQLm"
      },
      "outputs": [],
      "source": [
        "subdistrict_gdf = gpd.read_file(\"../data/zurich_neighborhoods.geojson\")\n",
        "district_gdf = gpd.read_file(\"../data/zurich_districts.geojson\")\n",
        "# district_desc = pd.read_csv(\"../data/zurich_districts.csv\")\n",
        "dog_data_train = pd.read_csv(\"../data/processed_dog_data.csv\")\n",
        "# dog_data_train = pd.read_csv(\"../data/processed_dog_data_train.csv\")\n",
        "# Fix data types as they were lost when saving to csv\n",
        "dog_data_train[\"owner_id\"] = dog_data_train[\"owner_id\"].astype(\"string\").str.zfill(6)\n",
        "dog_data_train[\"subdistrict\"] = (\n",
        "    dog_data_train[\"subdistrict\"].astype(\"string\").str.zfill(3)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pn.state.kill_all_servers()\n",
        "pn.state.clear_caches()"
      ],
      "metadata": {
        "id": "tO88FKBHhYoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5biTFShbnQLm"
      },
      "outputs": [],
      "source": [
        "poly_opts = dict(\n",
        "    width=600,\n",
        "    height=600,\n",
        "    color_index=None,\n",
        "    xaxis=None,\n",
        "    yaxis=None,\n",
        "    backend_opts={\"toolbar.autohide\": True},\n",
        ")\n",
        "# Neighborhood polygons\n",
        "neighborhood_poly = gv.Polygons(subdistrict_gdf.to_crs(ccrs.GOOGLE_MERCATOR.proj4_init),\n",
        "        crs = ccrs.GOOGLE_MERCATOR).opts(\n",
        "    tools=[\"hover\", \"tap\"],\n",
        "    **poly_opts,\n",
        "    line_color=\"skyblue\",\n",
        "    line_width=2,\n",
        "    fill_color=\"lightgray\",\n",
        "    fill_alpha=0,\n",
        "    line_alpha=0.5,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# district polygons\n",
        "# district_gdf_desc = district_gdf.merge(district_desc)\n",
        "district_poly = gv.Polygons(district_gdf.to_crs(ccrs.GOOGLE_MERCATOR.proj4_init),\n",
        "        crs = ccrs.GOOGLE_MERCATOR).opts(\n",
        "    **poly_opts,\n",
        "    line_color=\"orange\",\n",
        "    fill_alpha=0.02,\n",
        "    tools=[\"tap\"],\n",
        "    active_tools=['tap'],\n",
        "    selection_alpha=0.6,\n",
        "    selection_color='#008080',\n",
        "    selection_line_color='#008080',\n",
        "    line_width=3,\n",
        "    line_alpha=1,\n",
        ")\n",
        "\n",
        "district_poly_pane = pn.pane.HoloViews(district_poly)\n",
        "\n"
      ],
      "metadata": {
        "id": "fZSHNQaOUkny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# district_nd_overlay\n"
      ],
      "metadata": {
        "id": "0szxqhzTb1mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Destrict Description pane and Wordcloud"
      ],
      "metadata": {
        "id": "hFDyPIoacDgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wLHhU8LnQLn"
      },
      "outputs": [],
      "source": [
        "# tap_district = hv.streams.Tap(x=None, y=None, source=district_poly)\n",
        "\n",
        "\n",
        "# def get_selected_district(x, y):\n",
        "#     \"\"\"Returns the selected district based on the x and y coordinates\"\"\"\n",
        "#     return district_gdf_desc[\n",
        "#         district_gdf_desc.geometry.to_crs(ccrs.GOOGLE_MERCATOR.proj4_init).contains(Point(x, y))\n",
        "#     ]\n",
        "\n",
        "\n",
        "# @pn.depends(tap_district.param.x, tap_district.param.y)\n",
        "# def display_info(x, y):\n",
        "#     \"\"\"Displays a brief description of the selected district\"\"\"\n",
        "#     if x is None or y is None:\n",
        "#         return pn.pane.Markdown(\"No district selected\")\n",
        "#     else:\n",
        "#         # Find the selected district based on the x and y coordinates\n",
        "#         selected_district =  district_gdf_desc[\n",
        "#             district_gdf_desc.geometry.to_crs(ccrs.GOOGLE_MERCATOR.proj4_init).contains(Point(x, y))\n",
        "#         ]\n",
        "\n",
        "#         if selected_district.empty:\n",
        "#             return pn.pane.Markdown(\"No district selected\")\n",
        "\n",
        "#         dname = selected_district[\"district_name\"].values[0]\n",
        "#         dnum = selected_district[\"district\"].values[0]\n",
        "#         ddesc = selected_district[\"desc\"].values[0]\n",
        "#         link = selected_district[\"link\"].values[0]\n",
        "#         return pn.pane.Markdown(\n",
        "#             f\"\"\"\n",
        "#             <div style=\"\n",
        "#             border: 2px solid #4a4a4a;\n",
        "#             border-radius: 10px;\n",
        "#             padding: 20px 20px 20px 20px;\n",
        "#             background-color: #f9f9f9;\n",
        "#             box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);\n",
        "#             word-wrap: break-word;\n",
        "#             \">\n",
        "#             <h2 style='color: #008080;'>{dnum}</h2>\n",
        "#             <h1 style='color: #000080;'>{dname}</h1>\n",
        "#             <h3 style='color: #708090;'>{ddesc}</h3>\n",
        "#             <a href=\"{link}\" >Source</a>\n",
        "#             </div>\n",
        "\n",
        "#             \"\"\",\n",
        "#             width=300,\n",
        "#         )\n",
        "\n",
        "# display_info_row = pn.Row(display_info, sizing_mode=\"stretch_width\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pn.Row(gts.EsriWorldBoundariesAndPlacesAlternate *district_poly, display_info)\n",
        "# pn.Row(district_poly_pane, display_info_row)"
      ],
      "metadata": {
        "id": "vI-nUng6iBOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tap_district2 = hv.streams.Tap(x=None, y=None, rename={'x': 'x2', 'y': 'y2'}, source=district_poly)\n",
        "\n",
        "# @pn.depends(tap_district.param.x, tap_district.param.y)\n",
        "# def display_wordcloud(x, y):\n",
        "#     \"\"\"Displays a wordcloud of the selected district based on the description\n",
        "#     of the district in the shape of the district poly\"\"\"\n",
        "#     if x is None or y is None:\n",
        "#         text = \"district select on map\"\n",
        "#         wordcloud = WordCloud(width=800, height=500, background_color=\"white\").generate(\n",
        "#             text\n",
        "#         )\n",
        "#         return hv.RGB(np.array(wordcloud)).opts(\n",
        "#             width=800, height=500, active_tools=[\"box_zoom\"],\n",
        "#             title = f\"x is {x}, y is {y}\"\n",
        "#         )\n",
        "\n",
        "#     point = hv.Points([(x, y)]).redim(x='point_x', y='point_y')\n",
        "#     x2 = point.data.point_x.values[0]\n",
        "#     y2 = point.data.point_y.values[0]\n",
        "\n",
        "#     selected_district2 =  district_gdf_desc[\n",
        "#         district_gdf_desc.geometry.to_crs(ccrs.GOOGLE_MERCATOR.proj4_init).contains(Point(x2, y2))\n",
        "#         ]\n",
        "#     if selected_district2.empty:\n",
        "#         text = f\"selection not found {int(x2)} {int(y2)}\"\n",
        "#         wordcloud = WordCloud(width=800, height=500, background_color=\"white\").generate(text)\n",
        "#         return hv.RGB(np.array(wordcloud)).opts(\n",
        "#             width=800, height=500, active_tools=[\"box_zoom\"]\n",
        "#         )\n",
        "\n",
        "#     dname = selected_district2[\"district_name\"].values[0]\n",
        "#     dnum = selected_district2[\"district\"].values[0]\n",
        "#     ddesc = selected_district2[\"desc\"].values[0]\n",
        "#     text = f\"{dnum} {dname} {ddesc}\"\n",
        "\n",
        "#     poly = selected_district2[\"geometry\"].iloc[0]\n",
        "#     print(poly.bounds)\n",
        "\n",
        "#     # Get the bounding box of the poly\n",
        "#     minx, miny, maxx, maxy = poly.bounds\n",
        "\n",
        "#     # Calculate the width and height of the bounding box\n",
        "#     margin = 0.1\n",
        "#     width = (maxx - minx) * (1 + margin)\n",
        "#     height = (maxy - miny) * (1 + margin)\n",
        "#     # Calculate the new minimum x and y coordinates\n",
        "#     minx -= width * margin / 2\n",
        "#     miny -= height * margin / 2\n",
        "\n",
        "#     # Create a new image with the same aspect ratio as the bounding box\n",
        "#     image_width = 800\n",
        "#     image_height = int(image_width * height / width)\n",
        "#     test = Image.new(\"1\", (image_width, image_height), 0)\n",
        "\n",
        "#     # Convert the coordinates to a numpy array\n",
        "#     coords = np.array(list(poly.exterior.coords))\n",
        "#     coords -= [minx, miny]\n",
        "#     coords *= [image_width / width, image_height / height]\n",
        "#     coords[:, 1] = image_height - coords[:, 1]\n",
        "#     # Convert the coordinates back to a list of tuples\n",
        "#     scaled_coords = list(map(tuple, coords))\n",
        "#     print(scaled_coords)\n",
        "\n",
        "#     # Draw the scaled poly onto the image\n",
        "#     ImageDraw.Draw(test).polygon(scaled_coords, outline=1, fill=1)\n",
        "\n",
        "#     wordcloud = WordCloud(\n",
        "#         mask=~np.array(test) * 255,\n",
        "#         # color_func=lambda *args, **kwargs: breed_color,\n",
        "#         include_numbers=True,\n",
        "#         margin=20,\n",
        "#         # contour_color=breed_color,\n",
        "#         contour_width=5,\n",
        "#         width=800,\n",
        "#         height=500,\n",
        "#         background_color=\"white\",\n",
        "#     ).generate(text)\n",
        "#     return hv.RGB(np.array(wordcloud)).opts(\n",
        "#             width=800,\n",
        "#             height=500,\n",
        "#             active_tools=[\"box_zoom\"],\n",
        "#             backend_opts={\"toolbar.autohide\": True},\n",
        "#             title = f\"{x2} {y2} {dname}\")\n",
        "\n",
        "\n",
        "# # wordcloud_bound = pn.bind(display_wordcloud, x = tap_district_2.param.x, y=tap_district_2.param.y)\n",
        "# # wordcloud_dmap = hv.DynamicMap(wordcloud_bound, streams=[tap_district])\n",
        "# # pn.Row(district_poly , pn.bind(display_wordcloud, x = tap_district.param.x, y=tap_district.param.y))\n",
        "\n",
        "# wordcloud_pane = pn.panel(display_wordcloud)"
      ],
      "metadata": {
        "id": "ujoklw0hhywd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pn.Row(district_poly_pane, display_info_row, wordcloud_pane)\n"
      ],
      "metadata": {
        "id": "7eXmwfMZ1VaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# district_layout = pn.Column(\n",
        "#     pn.pane.HoloViews(display_wordcloud),\n",
        "#     pn.Row(gts.EsriWorldBoundariesAndPlacesAlternate *district_poly, display_info, sizing_mode=\"stretch_width\"),\n",
        "#     )\n",
        "\n",
        "# district_layout_card = pn.Card(\n",
        "#     district_layout,\n",
        "#     title=\"District Descript\",\n",
        "#     sizing_mode=\"stretch_width\",\n",
        "# )\n",
        "# district_layout_card"
      ],
      "metadata": {
        "id": "H8zNwudl1TNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dog Count & Dog Owner count"
      ],
      "metadata": {
        "id": "6PB0tCNecQ4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEsMozFmnQLn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# A single row from the dog data\n",
        "(\n",
        "    dog_data_train.describe(include=\"all\")\n",
        "    .T.infer_objects()\n",
        "    .sort_values(by=\"unique\")\n",
        "    .fillna(\"\")\n",
        ")\n",
        "dog_data_train.sample().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3LmE7MMnQLn"
      },
      "outputs": [],
      "source": [
        "def update_xaxis(plot, element):\n",
        "    \"\"\"Hook to update the x-axis ticker on the plot.\"\"\"\n",
        "    plot.state.xaxis.ticker = FixedTicker(ticks=list(range(2015, 2023)))\n",
        "\n",
        "\n",
        "dogs_total_by_reporting_year = dog_data_train.groupby(\"reporting_year\").size()\n",
        "print(f\"Total number of dogs per year:\\n{dogs_total_by_reporting_year}\")\n",
        "\n",
        "total_dogs_line = dogs_total_by_reporting_year.hvplot.bar().opts(\n",
        "    show_legend=False,\n",
        "    title=\"Total Dogs Registered Each Year\",\n",
        "    active_tools=[\"box_zoom\"],\n",
        "    # height=500,\n",
        "    # width=400,\n",
        ")\n",
        "\n",
        "dog_count_yoy_pct_change = dogs_total_by_reporting_year.pct_change().fillna(0) * 100\n",
        "total_dogs_yoy_bar = dog_count_yoy_pct_change.hvplot(kind=\"line\").opts(\n",
        "    hooks=[update_xaxis],\n",
        "    active_tools=[\"box_zoom\"],\n",
        "    title=\"YOY % Change in Dog Count\",\n",
        "    ylabel=\"%\",\n",
        ")\n",
        "\n",
        "(total_dogs_line + total_dogs_yoy_bar).cols(1).opts(shared_axes=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Butterfly plot here shows the The edge distribution of male and female dogs for each of the years. We can see here that both genders tend to live around the same length of time we also see that there are slightly more male dogs than female dogs."
      ],
      "metadata": {
        "id": "-X_L5kTL3WL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_player = pnw.Player(\n",
        "    name=\"Yearly Player\",\n",
        "    start=2015,\n",
        "    end=2020,\n",
        "    value=2020,\n",
        "    step=1,\n",
        "    loop_policy=\"loop\",\n",
        "    interval=3000,\n",
        ")\n",
        "\n",
        "\n",
        "@pn.depends(yearly_player.param.value)\n",
        "def get_dog_age_butterfly_plot(reporting_year):\n",
        "    \"\"\"\n",
        "    Decorated with @pn.depends, this function generates a butterfly plot of male\n",
        "    and female dog age distributions for a given roster year.\n",
        "\n",
        "    Parameters:\n",
        "    roster (int): The roster year to filter the dog data by.\n",
        "\n",
        "    Returns:\n",
        "    hvplot: A butterfly plot of male and female dog age distributions for the given roster year.\n",
        "    \"\"\"\n",
        "    # Define bar plot options\n",
        "    bar_opts = dict(\n",
        "        invert=True,\n",
        "        height=500,\n",
        "        width=500,\n",
        "        bar_width=2,\n",
        "        rot=90,\n",
        "        xlim=(0, 24),\n",
        "        xlabel=\"\",\n",
        "        yaxis=\"bare\",\n",
        "        ylabel=\"Count\",\n",
        "        grid=True,\n",
        "    )\n",
        "    # Filter the DataFrame for the roster\n",
        "    filtered_dog_data = dog_data_train.copy()\n",
        "    # filtered_dog_data = pd.read_csv(\"../data/processed_dog_data.csv\")\n",
        "    roster_dog_data = filtered_dog_data.query(f\"reporting_year=={reporting_year}\")\n",
        "    # Filter for the is_male_dog\n",
        "    male_roster_dog_data = roster_dog_data.loc[roster_dog_data[\"is_male_dog\"]]\n",
        "    male_roster_dog_data = (male_roster_dog_data.groupby(\n",
        "        [\"dog_age\"]).size().reset_index(name=\"age_frequency\"))\n",
        "    male_roster_dog_data = male_roster_dog_data.set_index(\"dog_age\")\n",
        "    total_male = male_roster_dog_data[\"age_frequency\"].sum()\n",
        "    male_plot = male_roster_dog_data.hvplot.bar(\n",
        "        **bar_opts,\n",
        "        ylim=(0, 620),\n",
        "        title=f\"Male Dog Age Distribution || {reporting_year} || {total_male} Canines\",\n",
        "        color=\"skyblue\",\n",
        "    ).opts(active_tools=[\"box_zoom\"])\n",
        "\n",
        "    female_roster_dog_data = roster_dog_data[~roster_dog_data[\"is_male_dog\"]]\n",
        "    female_roster_dog_data = (female_roster_dog_data.groupby(\n",
        "        [\"dog_age\"]).size().reset_index(name=\"age_frequency\"))\n",
        "    female_roster_dog_data = female_roster_dog_data.set_index(\"dog_age\")\n",
        "    total_female = female_roster_dog_data[\"age_frequency\"].sum()\n",
        "    female_roster_dog_data[\"age_frequency\"] = (\n",
        "        -1 * female_roster_dog_data[\"age_frequency\"])\n",
        "    female_plot = female_roster_dog_data.hvplot.bar(\n",
        "        **bar_opts,\n",
        "        ylim=(-620, 0),\n",
        "        title=\n",
        "        f\"Female Dog Age Distribution || {reporting_year} || {total_female} Canines\",\n",
        "        color=\"pink\",\n",
        "    ).opts(active_tools=[\"box_zoom\"])\n",
        "    return (female_plot + male_plot).opts(shared_axes=False, )"
      ],
      "metadata": {
        "id": "fxICkDhUxItx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_age_distribution_pane = pn.panel(get_dog_age_butterfly_plot)\n",
        "pn.Column(\n",
        "    yearly_player,\n",
        "    dog_age_distribution_pane,\n",
        "    sizing_mode=\"stretch_width\",\n",
        ")"
      ],
      "metadata": {
        "id": "zo_m0A3G3YiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.get_line_plots(\n",
        "    dog_data_train.groupby(\"reporting_year\", as_index=False)[\n",
        "        \"mixed_type\"].value_counts(),\n",
        "    x=\"reporting_year\",\n",
        "    group_by=\"mixed_type\",\n",
        "    highlight_list=[\"PB\", \"BB\"],\n",
        ").opts(hooks=[update_xaxis], title=\"Most Dogs are Pure Breeds\", xlabel=\"\", height=500, show_grid=True)"
      ],
      "metadata": {
        "id": "1HMeWzq1311N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.get_line_plots(\n",
        "    dog_data_train.groupby(\"reporting_year\", as_index=False)[\n",
        "        \"dog_size\"].value_counts(),\n",
        "    x=\"reporting_year\",\n",
        "    group_by=\"dog_size\",\n",
        "    highlight_list=[\"K\"],\n",
        ").opts(hooks=[update_xaxis], title=\"More Dogs are Small Breeds\", xlabel=\"\", height=500, show_grid=True)"
      ],
      "metadata": {
        "id": "je8uU1Mt4mmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that there is a strong upward tick in the number of dog owners for 30 and 40 year-old. Up until 2019 it was the 50 year olds that had the highest number of dog owners This is a trend that we will explore further in this analysis."
      ],
      "metadata": {
        "id": "ycVuDhyy31Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_train.is_male_owner.value_counts()"
      ],
      "metadata": {
        "id": "3qk_MHzQ7vZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main uptick has been in the dog owners in their 30s and 40s"
      ],
      "metadata": {
        "id": "lRxx_vj9I1Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_plot_opts = dict(\n",
        "    height=500,\n",
        "    width=600,\n",
        "    hooks=[update_xaxis],\n",
        "    xlabel=\"\",\n",
        "    ylabel=\"\",\n",
        "    show_grid=True,\n",
        ")\n",
        "highlighted_age_groups = [10, 30, 40]\n",
        "dog_owner_grouped_count = dog_data_train.groupby([\"reporting_year\", \"age_group_10\"], as_index=False)[\n",
        "    \"owner_id\"\n",
        "].nunique()\n",
        "\n",
        "female_dog_owner_grouped_count = dog_data_train[~dog_data_train['is_male_owner']==True].groupby(\n",
        "    [\"reporting_year\", \"age_group_10\"], as_index=False)[\"owner_id\"].nunique()\n",
        "male_dog_owner_grouped_count = dog_data_train[dog_data_train['is_male_owner']==True].groupby(\n",
        "    [\"reporting_year\", \"age_group_10\"], as_index=False)[\"owner_id\"].nunique()\n",
        "\n",
        "male_dog_owner_pop_plot = hf.get_line_plots(\n",
        "    data=male_dog_owner_grouped_count,\n",
        "    x=\"reporting_year\",\n",
        "    group_by=\"age_group_10\",\n",
        "    highlight_list = highlighted_age_groups,\n",
        ").opts(**line_plot_opts, title=\"Male 30 & 40 year old Dog Owners count is trending upwards\")\n",
        "\n",
        "female_dog_owner_pop_plot = hf.get_line_plots(\n",
        "    data=female_dog_owner_grouped_count,\n",
        "    x=\"reporting_year\",\n",
        "    group_by=\"age_group_10\",\n",
        "    highlight_list = highlighted_age_groups,\n",
        ").opts(**line_plot_opts, title=\"Female 30 & 40 year old Dog Owners count is trending upwards\")\n",
        "\n",
        "(female_dog_owner_pop_plot + male_dog_owner_pop_plot)\n",
        "# male_dog_owner_pop_plot.opts(\n",
        "#     title=\"Young Adults & Middle-Age Dog Owners count is trending upwards\", **line_plot_opts\n",
        "# )"
      ],
      "metadata": {
        "id": "6TqWvr7X48gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_owners_df"
      ],
      "metadata": {
        "id": "VIG6QlMZrjHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_owners_df.query('reporting_year > 2015').hvplot(\n",
        "    x='reporting_year', y='new_owner_count', by='subdistrict', height=800, line_alpha=0.5\n",
        ").opts(legend_cols=2)"
      ],
      "metadata": {
        "id": "bvbQBzLO8gz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with people population change\n",
        "\n",
        "pop_data_grouped_count = pop_data.groupby(['reporting_year', 'age_group_10'], as_index=False)['pop_count'].sum()\n",
        "people_pop_plot = hf.get_line_plots(\n",
        "    data = pop_data_grouped_count, x='reporting_year', group_by='age_group_10', highlight_list = highlighted_age_groups\n",
        ")\n",
        "\n",
        "female_pop_grouped_count = pop_data[~pop_data['is_male']].groupby(['reporting_year', 'age_group_10'], as_index=False)['pop_count'].sum()\n",
        "male_pop_grouped_count = pop_data[pop_data['is_male']].groupby(['reporting_year', 'age_group_10'], as_index=False)['pop_count'].sum()\n",
        "female_pop_plot = hf.get_line_plots(\n",
        "    data = female_pop_grouped_count, x='reporting_year', group_by='age_group_10', highlight_list = highlighted_age_groups\n",
        ").opts(**line_plot_opts, title=\"Female Population Change\")\n",
        "male_pop_plot = hf.get_line_plots(\n",
        "    data = male_pop_grouped_count, x='reporting_year', group_by='age_group_10', highlight_list = highlighted_age_groups\n",
        ").opts(**line_plot_opts, title=\"Male Population Change\")\n",
        "\n",
        "(female_pop_plot + male_pop_plot)\n",
        "\n"
      ],
      "metadata": {
        "id": "asjw3SE1J74e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ccqz-CM2xSSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do older dog owners tend to have older dogs? **\n",
        "\n",
        "We will investigate this graphically then in more depth.\n",
        "\n",
        "From the voilinplot below it is clear that younger dog owners tend to have younger dogs and that the older dog owners To a lesser extreme also tend to have older dogs. Despite the slightly upwards positive correlation, each Age Group of the dog owners encompasses a wide range For dog owners in the 50s having the widest range."
      ],
      "metadata": {
        "id": "YKPNjD1exWQc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "087tCS7IxWGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_train['age_range'] = dog_data_train['age_group_10'].astype('str') + 's'\n",
        "dog_data_train[\"age_range\"] = pd.Categorical(\n",
        "    dog_data_train[\"age_range\"],\n",
        "    ordered=True,\n",
        "    categories=[\"10s\", \"20s\", \"30s\", \"40s\", \"50s\", \"60s\", \"70s\", \"80s\", \"90s\"],\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Distribution of dog owners at each age group\n",
        "sns.violinplot(\n",
        "    x=\"age_range\",\n",
        "    y=\"dog_age\",\n",
        "    data=dog_data_train,\n",
        "    order=[\"10s\", \"20s\", \"30s\", \"40s\", \"50s\", \"60s\", \"70s\", \"80s\", \"90s\"],\n",
        "    # palette='dark:#1f77b4',\n",
        "    cut=0\n",
        "\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ApwnB2HKvjWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOkwF5HJ1i3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_toggle = pnw.ToggleGroup(\n",
        "    button_style='outline',\n",
        "    name='reporting_year',\n",
        "    value=2015,\n",
        "    options=list(range(2015, 2023)),\n",
        "    behavior='radio'\n",
        ")\n",
        "\n",
        "@pn.depends(year_toggle.param.value)\n",
        "def create_violins(year):\n",
        "    year_data = dog_data_train[dog_data_train[\"reporting_year\"] == year].sort_values(by=\"age_range\")\n",
        "    return hv.Violin(year_data, [\"age_range\", \"is_male_dog\"], \"dog_age\").opts(\n",
        "        width=800, height=400, cut=0, split='is_male_dog',\n",
        "        # inner='stick',\n",
        "        ylabel='', xlabel='', ylim=(-1, 24), show_grid=True,\n",
        "        active_tools=['box_zoom'],\n",
        "        title=f\"Violins | Dog Age Distribution | Owner Age Group | {year}\"\n",
        "    )\n",
        "\n",
        "pn.Column(\n",
        "    year_toggle,\n",
        "    pn.pane.HoloViews(create_violins)\n",
        ")"
      ],
      "metadata": {
        "id": "Pak6slExw-iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Distribution of male and female dog owners In each district\n",
        "g = sns.FacetGrid(dog_data_train, col=\"is_male_dog\", aspect=5, height=3, col_wrap=1)\n",
        "g.map_dataframe(\n",
        "    sns.violinplot,\n",
        "    x=\"age_range\",\n",
        "    y=\"dog_age\",\n",
        "    hue=\"is_male_owner\",\n",
        "    split=True,\n",
        "    palette='dark:#1f77b4',\n",
        "    inner='stick',\n",
        "    cut=0\n",
        "\n",
        ")\n",
        "g.add_legend()\n",
        "# add title\n",
        "plt.subplots_adjust(top=0.9)\n",
        "g.fig.suptitle(\"Dog Age | Owner Age | Male Owner bool\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fYrHaVtb19Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compare the is_male_owner and the is_male_dog columns\n",
        "# pd.crosstab(dog_data_train[\"is_male_owner\"], dog_data_train[\"is_male_dog\"]).reset_index(drop=True)\n",
        "is_male_df = dog_data_train[['reporting_year', 'is_male_dog', 'is_male_owner']].value_counts().reset_index(name=\"count\").sort_values('reporting_year').set_index('reporting_year')\n",
        "for year in range(2015, 2024):\n",
        "    display(year,\n",
        "        is_male_df.query('reporting_year == @year').pivot(\n",
        "    index='is_male_dog', columns='is_male_owner', values='count'\n",
        ").style.background_gradient(cmap='Blues')\n",
        "    )\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "NgYYwSl-1rAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gender HeatMap"
      ],
      "metadata": {
        "id": "GaNV7dX9nGw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_train.standard.nunique()"
      ],
      "metadata": {
        "id": "-FCuO8JJuWhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colors\n",
        "unique_breeds = dog_data[\"standard\"].unique()\n",
        "\n",
        "num_unique_breeds = len(unique_breeds)\n",
        "\n",
        "# Repeat the colormap to cover all unique breeds\n",
        "repeated_cmap = list(cc.glasbey_dark) * (num_unique_breeds // len(cc.glasbey_dark) + 1)\n",
        "\n",
        "# Explicit mapping for the color to use for each standard breed\n",
        "explicit_mapping = {breed: repeated_cmap[i] for i, breed in enumerate(unique_breeds)}\n",
        "\n",
        "\n",
        "my_colors = hv.Cycle(list(explicit_mapping.values()))\n",
        "# colormaps for the the gender\n",
        "boy_cmap = list(sns.color_palette(\"light:#00008b\", n_colors=6).as_hex())\n",
        "girl_cmap = list(sns.color_palette(\"light:#8b008b\", n_colors=6).as_hex())"
      ],
      "metadata": {
        "id": "af3BANjCpSsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXENMdB5nACg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular Breeds"
      ],
      "metadata": {
        "id": "OhaqKGkhBjkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_train.sample()"
      ],
      "metadata": {
        "id": "uMjwzh3pswU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_all_cols = pd.concat(dog_reporting_year_dict.values())#[dog_data_columns_to_keep]\n",
        "\n",
        "dog_data_to_2020_all_cols = pd.concat({\n",
        "    reporting_year: df\n",
        "    for reporting_year, df in dog_reporting_year_dict.items() if reporting_year <= 2020\n",
        "}.values())\n",
        "\n",
        "# dog_data_all_cols[['reporting_year', 'owner_id',\n",
        "\n",
        "#        'district', 'subdistrict', 'breed_1',\n",
        "#        'breed_2', 'breed_mixed_cd', 'mixed_type',\n",
        "#        'dog_size',\n",
        "#       'dog_age',\n",
        "#  'is_pure_breed',\n",
        "#         'breed_1_de',\n",
        "#        'breed_2_de', 'breed_1_en', 'standard', 'breed_2_en', 'standard_2',\n",
        "#        'pet_count',]]"
      ],
      "metadata": {
        "id": "_VcdXlmGFC4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 5 dog breeds(standards) for each year\n",
        "dog_data_train.groupby(['reporting_year',\"standard\",]).size().reset_index(name=\"count\").groupby(\"reporting_year\").apply(lambda x: x.nlargest(10, \"count\")).reset_index(drop=True)\n",
        "\n",
        "# Get the 10 most common dog breeds per year in the train dataset.\n",
        "\n",
        "# dog_data_train['standard_lst'] = dog_data_train.apply(lambda row: list(set([row['standard'], row['standard_2']])) if row['standard_2'] != 'none' else [row['standard']], axis=1)\n",
        "dog_data_train['standard_lst'] = dog_data_train.apply(lambda row: [row['standard'], row['standard']] if row['standard_2'] == 'none' else [row['standard'], row['standard_2']], axis=1)\n",
        "# get the top 5 breeds per year per district from the exploded standard_lst dataframe\n",
        "exploded_df = dog_data_train.explode(['standard_lst'])\n",
        "(\n",
        "    exploded_df\n",
        "    .groupby(['reporting_year', 'district', 'standard_lst']).size().reset_index(name=\"count_num\")\n",
        "    .groupby([\"reporting_year\", \"district\"]).apply(lambda x: x.nlargest(3, \"count_num\"), include_groups=False)\n",
        "    .reset_index().drop(columns=['level_2'])\n",
        ")\n"
      ],
      "metadata": {
        "id": "J4YPgySF_iEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJxZrq5ExdQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "n=10\n",
        "topn_breeds_df = pd.DataFrame(columns=['standard_lst'])\n",
        "\n",
        "for year in dog_reporting_year_dict.keys():\n",
        "    tmp_year_df = dog_reporting_year_dict[year].copy()\n",
        "    tmp_year_df['standard_lst'] = tmp_year_df.apply(lambda row: [row['standard'], row['standard']] if row['standard_2'] == 'none' else [row['standard'], row['standard_2']], axis=1)\n",
        "\n",
        "    exploded_tmp = tmp_year_df.explode(['standard_lst'])\n",
        "    tmp_to_merge = exploded_tmp.groupby(['standard_lst']).size().reset_index(name=f\"Y{year}\")\n",
        "    topn_breeds_df = topn_breeds_df.merge(tmp_to_merge, on='standard_lst', how='outer')\n",
        "\n",
        "# Divide by 2 as we exploded the standard_lst column\n",
        "topn_breeds_df = topn_breeds_df.set_index('standard_lst').fillna(0).astype(int).sort_values(by='Y2023', ascending=False) // 2\n",
        "majority_breed_dict = {}\n",
        "for column in topn_breeds_df.columns:\n",
        "    # sort the column\n",
        "    year_numbers = topn_breeds_df[column].sort_values(ascending=False)\n",
        "\n",
        "    breeds_num = (year_numbers.cumsum()/ year_numbers.sum() < 0.95).sum()\n",
        "    breeds = year_numbers.index.tolist()[:breeds_num]\n",
        "    majority_breed_dict[column] = breeds\n",
        "\n",
        "topn_breeds_df\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GQx_6xPbmqFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moran's I"
      ],
      "metadata": {
        "id": "MFgCBQeE0Dqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "breed_list = dog_data[dog_data['reporting_year']==2023].standard.value_counts().head(20).index.tolist()\n",
        "# breed_list = [breed for breed in breed_list if  breed != 'unknown']\n",
        "breed_list\n"
      ],
      "metadata": {
        "id": "HKFO6wqyhUK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_grouped_counts(dog_reporting_year_dict[2022], threshold=25).div(subdistrict_gdf.set_index('subdistrict')['subd_area_km2'], axis=0)\n",
        "# subdistrict_gdf\n",
        "# subdistrict_gdf.set_index('subdistrict')['subd_area_km2']"
      ],
      "metadata": {
        "id": "hXlHE0VozHf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# w = Queen.from_dataframe(subdistrict_gdf)\n",
        "# w.transform = 'R'\n",
        "\n",
        "def get_grouped_counts(dataframe):\n",
        "    dataframe_c = dataframe.copy()\n",
        "    dataframe_c['standard_lst'] = dataframe_c.apply(lambda row: [row['standard'], row['standard']] if row['standard_2'] == 'none' else [row['standard'], row['standard_2']], axis=1)\n",
        "    dataframe_c = dataframe_c.explode(['standard_lst'])\n",
        "    grouped_counts = dataframe_c.groupby(['subdistrict', 'standard_lst'])['standard_lst'].count().unstack().fillna(0)\n",
        "    return grouped_counts\n",
        "\n",
        "def calculate_moran_i(dataframe, standards, weights_matrix):\n",
        "    moran_dict = {}\n",
        "    for standard in standards:\n",
        "        moran = Moran(dataframe[standard], weights_matrix)\n",
        "        moran_dict[standard] = moran\n",
        "    return moran_dict\n",
        "\n",
        "def plot_moran_i(morans, year):\n",
        "\n",
        "    for standard, mo in morans[year].items():\n",
        "        plot_moran(mo)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "moran_i = {}\n",
        "morans = {}\n",
        "\n",
        "\n",
        "\n",
        "for year in dog_reporting_year_dict.keys():\n",
        "    year_counts = pd.DataFrame()\n",
        "    year_counts = get_grouped_counts(dog_reporting_year_dict[year])\n",
        "    year_density = year_counts.div(subdistrict_gdf.set_index('subdistrict')['subd_area_km2'], axis=0)\n",
        "    # Create weights matrix based on the current year's data\n",
        "    spatial_df = subdistrict_gdf.merge(year_density, on='subdistrict', how='left')\n",
        "    w = Queen.from_dataframe(spatial_df)\n",
        "    w.transform = 'R'\n",
        "    morans[year] = calculate_moran_i(spatial_df, breed_list, w)\n",
        "    moran_i[year] = {standard: mo.I for standard, mo in morans[year].items()}\n",
        "\n",
        "\n",
        "moran_i_df = pd.DataFrame(moran_i).T\n",
        "moran_i_df.style.highlight_max(axis=1, color='darkgreen')\n",
        "\n",
        "# get_grouped_counts(dog_reporting_year_dict[2023], threshold=25)\n",
        "# plot_moran_i(morans, 2023)\n",
        "\n"
      ],
      "metadata": {
        "id": "QGATdJXJzeMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year in range(2015, 2023):\n",
        "    print(f\"Year {year}\")\n",
        "    plot_moran(morans[year]['dachshund'], year)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "X5lOe_Wi0km1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data[(dog_data['standard'].isin(['poodle', 'dachshund', 'golden retreiver'])) | (dog_data['standard_2'].isin(['poodle', 'dachshund', 'golden retreiver']))]"
      ],
      "metadata": {
        "id": "dkIz9KJhfnZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moran_test = pd.DataFrame()\n",
        "moran_test = dog_reporting_year_dict[2015].copy()\n",
        "moran_test['standard_lst'] = moran_test.apply(lambda row: [row['standard'], row['standard']] if row['standard_2'] == 'none' else [row['standard'], row['standard_2']], axis=1)\n",
        "moran_test = moran_test.explode(['standard_lst'])\n",
        "moran_test = moran_test.groupby(['subdistrict', 'standard_lst'])['standard_lst'].count().unstack().drop(columns=['unknown']).fillna(0)\n",
        "# filter out the low number breeds\n",
        "mask = moran_test.max() > 25\n",
        "\n",
        "moran_test = moran_test.T[mask].T\n",
        "\n",
        "spatial_df = subdistrict_gdf.merge(moran_test, on='subdistrict', how='left')\n",
        "\n",
        "w = Queen.from_dataframe(spatial_df)\n",
        "w.transform = 'R'\n",
        "\n",
        "print(f\"As p-value is less than 0.05, there is statistically significant spatial autocorrelation.\")\n",
        "moran_dict = {}\n",
        "for standard in spatial_df.columns[5:]:\n",
        "    moran = Moran(spatial_df[standard], w)\n",
        "    moran_dict[standard] = {'p_value': moran.p_sim, 'moran_I': moran.I}\n",
        "\n",
        "pd.DataFrame(moran_dict).T.sort_values(by='moran_I', ascending=False)\n",
        "\n",
        "# moran_test.T[mask].hvplot.heatmap(\n",
        "#     height=500, width=1000,\n",
        "#     line_color='gray', line_width=0.5,\n",
        "#     cmap='greens',\n",
        "#     ).opts(\n",
        "#         color_levels=5,\n",
        "#     # color_levels=[0, 60, 120, 180, 240, 300],\n",
        "#         active_tools=['box_zoom'])"
      ],
      "metadata": {
        "id": "7UG0oeow8YCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the top n small breeds in Zurich\n",
        "ktopn = (\n",
        "    dog_data_train.loc[dog_data_train[\"dog_size\"] == \"K\"][\"standard\"]\n",
        "    .value_counts()\n",
        "    .head(15)\n",
        "    .index.tolist()\n",
        ")\n",
        "ktopn_pure = (\n",
        "    dog_data_train.loc[\n",
        "        (dog_data_train[\"dog_size\"] == \"K\") & (\n",
        "            dog_data_train[\"is_pure_breed\"])\n",
        "    ]\n",
        "    .standard.value_counts()\n",
        "    .head(15)\n",
        "    .index.tolist()\n",
        ")\n",
        "\n",
        "# The top n big breeds in Zurich\n",
        "itopn = (\n",
        "    dog_data_train[dog_data_train[\"dog_size\"] == \"I\"][\"standard\"]\n",
        "    .value_counts()\n",
        "    .head(15)\n",
        "    .index.tolist()\n",
        ")\n",
        "itopn_pure = (\n",
        "    dog_data_train.loc[\n",
        "        (dog_data_train[\"dog_size\"] == \"I\") & (\n",
        "            dog_data_train[\"is_pure_breed\"])\n",
        "    ]\n",
        "    .standard.value_counts()\n",
        "    .head(15)\n",
        "    .index.tolist()\n",
        ")\n",
        "\n",
        "topn = ktopn + itopn\n",
        "topn_pure = ktopn_pure + itopn_pure"
      ],
      "metadata": {
        "id": "DjOKynboBO40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_data_train.info()\n",
        "pn.state.kill_all_servers()"
      ],
      "metadata": {
        "id": "9ideQo2j4Dkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pn.state.kill_all_servers()\n",
        "pn.state.clear_caches()"
      ],
      "metadata": {
        "id": "KRxIcLuqnbHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tap stream linked to the HeatMap\n",
        "breed_tap = hv.streams.Tap(source=None)\n",
        "reporting_year_slider = pnw.IntSlider(\n",
        "    value=2015, start=2015, end=2023,\n",
        "    name=\"reporting_year\",\n",
        "    width=200,\n",
        ")\n",
        "pure_breed_checkbox = pnw.Checkbox(name=\"Pure Breed\", value=True, width=200)\n",
        "is_male_owner_checkbox = pnw.Checkbox(name=\"Male Dog Owner\", value=True, width=200)\n",
        "# breed_selector = pnw.Select(name=\"Breed\", options=topn, value=\"french bulldog\")\n",
        "breed_selector = pnw.ToggleGroup(name=\"Breed\", options=breed_list, value=\"french bulldog\", behavior='radio',\n",
        "                                 button_style='outline', orientation='vertical', width=200, button_type='success')\n",
        "\n",
        "\n",
        "top_n_slider = pnw.IntSlider(name=\"Top N\", start=1, end=30, step=1, value=10, width=200)\n"
      ],
      "metadata": {
        "id": "uunsVGbSnAKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @pn.depends(reporting_year_slider.param.value, is_male_owner_checkbox.param.value)\n",
        "def get_gender_reporting_year_df(reporting_year, gender):\n",
        "    return dog_data_train.loc[\n",
        "        (dog_data_train[\"is_male_owner\"] == gender) & (dog_data_train[\"reporting_year\"] == reporting_year)\n",
        "    ].copy()\n",
        "\n",
        "\n",
        "@pn.depends(\n",
        "    reporting_year_slider.param.value,\n",
        "    is_male_owner_checkbox.param.value,\n",
        "    top_n_slider.param.value,\n",
        ")\n",
        "def get_top_n_gender_breeds(reporting_year, gender, top_n):\n",
        "    gender_reporting_year_df = get_gender_reporting_year_df(reporting_year=reporting_year, gender=gender)\n",
        "    return gender_reporting_year_df[\"standard\"].value_counts().head(top_n).index.tolist()\n",
        "\n",
        "\n",
        "@pn.depends(\n",
        "    reporting_year_slider.param.value,\n",
        "    is_male_owner_checkbox.param.value,\n",
        "    top_n_slider.param.value,\n",
        ")\n",
        "def get_gender_heatmap(reporting_year, gender, top_n):\n",
        "    gender_reporting_year_df = get_gender_reporting_year_df(reporting_year=reporting_year, gender=gender)\n",
        "    topn_gender_breeds = get_top_n_gender_breeds(reporting_year=reporting_year, gender=gender, top_n=top_n\n",
        "    )\n",
        "\n",
        "    filtered_df = (\n",
        "        gender_reporting_year_df.loc[gender_reporting_year_df[\"standard\"].isin(\n",
        "            topn_gender_breeds)]\n",
        "        .groupby([\"standard\", \"district\"])\n",
        "        .size()\n",
        "        .fillna(0)\n",
        "        .reset_index(name=\"count\")\n",
        "    )\n",
        "    sex = \"Male\" if gender else \"Female\"\n",
        "    top_gender_breeds_heatmap = hv.HeatMap(\n",
        "        filtered_df, [\"district\", \"standard\"], \"count\"\n",
        "    ).redim(standard=\"gender_standard\")\n",
        "\n",
        "    top_gender_breeds_heatmap.opts(\n",
        "        height=(33 * top_n) + 50,\n",
        "        width=800,\n",
        "        cmap=boy_cmap if gender else girl_cmap,\n",
        "        colorbar=True,\n",
        "        active_tools=[\"box_zoom\"],\n",
        "        tools=['tap', 'hover', 'box_select'],\n",
        "        title=f\"Top {top_n} breeds | {reporting_year} | {sex} Owners\",\n",
        "        clim=(0, 100),\n",
        "    )\n",
        "    breed_tap.source = top_gender_breeds_heatmap\n",
        "\n",
        "    return top_gender_breeds_heatmap\n",
        "\n",
        "\n",
        "dynamic_gender_heatmap_panel = pn.panel(get_gender_heatmap)\n",
        "# pn.Column(\n",
        "#     pn.Row(\n",
        "#     is_male_owner_checkbox,\n",
        "#     reporting_year_slider,\n",
        "#     top_n_slider,\n",
        "#     ),\n",
        "#     dynamic_gender_heatmap_panel,\n",
        "# )"
      ],
      "metadata": {
        "id": "BVIMU9mkr-xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define a Tap stream linked to the owner age group\n",
        "owner_tap = hv.streams.Tap(source=None, x=1, y=30)\n",
        "\n",
        "\n",
        "@pn.depends(reporting_year_slider.param.value, is_male_owner_checkbox.param.value)\n",
        "def get_age_heatmap(reporting_year, gender):\n",
        "    gender_reporting_year_df = get_gender_reporting_year_df(reporting_year=reporting_year, gender=gender)\n",
        "    gender_grouped = (\n",
        "        gender_reporting_year_df.groupby([\"district\", \"age_group_10\"], as_index=False)[\"owner_id\"]\n",
        "        .nunique()\n",
        "        .rename(columns={\"owner_id\": \"count\"})\n",
        "    )\n",
        "    sex = \"Male\" if gender else \"Female\"\n",
        "    district_age_heatmap = hv.HeatMap(\n",
        "        gender_grouped, [\"district\", \"age_group_10\"], \"count\"\n",
        "        ).redim(age_group_10=\"age_group\")\n",
        "\n",
        "    district_age_heatmap.opts(\n",
        "        opts.HeatMap(\n",
        "        cmap=boy_cmap if gender else girl_cmap,\n",
        "        height=500, width=800,\n",
        "        ylim=(0, 100),\n",
        "        xlim=(0, 13),\n",
        "        colorbar=True,\n",
        "        line_width=4,\n",
        "        nonselection_alpha=0.9,\n",
        "        selection_line_color='red',\n",
        "        active_tools=[\"box_zoom\"],\n",
        "        tools=[\"hover\", \"tap\", \"box_select\"],\n",
        "        title=f\"{sex} Dog Owners | {reporting_year} | by Age Group vs District\",\n",
        "    ))\n",
        "    owner_tap.source = district_age_heatmap\n",
        "    return district_age_heatmap\n",
        "\n",
        "\n",
        "age_group_panel = pn.panel(get_age_heatmap)\n"
      ],
      "metadata": {
        "id": "uIDxnvKpsQgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_plots_opts = dict(\n",
        "    height=500,\n",
        "    width=800,\n",
        "    invert_axes=True,\n",
        "    cmap=explicit_mapping,\n",
        "    show_legend=False,\n",
        "    xlabel=\"\",\n",
        "    fontscale=1.2,\n",
        ")\n",
        "\n",
        "\n",
        "@pn.depends(\n",
        "    owner_tap.param.x,\n",
        "    owner_tap.param.y,\n",
        "    reporting_year_slider.param.value,\n",
        "    is_male_owner_checkbox.param.value,\n",
        ")\n",
        "def update_barplot(x, y, reporting_year, gender):\n",
        "    if x is not None and y is not None:\n",
        "        data = get_gender_reporting_year_df(reporting_year=reporting_year, gender=gender)\n",
        "        district_x = math.ceil(x - 0.5)\n",
        "        age_group_y = math.ceil(y / 10) * 10\n",
        "        # print(f\"District: {district_x}, Age Group: {age_group_y}\")\n",
        "        bar_data = (\n",
        "            data.loc[(data[\"district\"] == district_x) & (\n",
        "                data[\"age_group_10\"] == age_group_y)][\"standard\"]\n",
        "            .value_counts()\n",
        "            .head(10)\n",
        "            .reset_index()\n",
        "        )\n",
        "        bar_data.columns = [\"standard\", \"count\"]\n",
        "        if len(bar_data) == 0:\n",
        "            return hv.Bars([], \"standard\", \"count\").opts(\n",
        "                **bar_plots_opts,\n",
        "                title=f\"No Breeds for Age-group:{age_group_y} | Districts:{district_x}\",\n",
        "                active_tools=[\"box_zoom\"],\n",
        "            )\n",
        "\n",
        "        return hv.Bars(bar_data, \"standard\", \"count\").opts(\n",
        "            **bar_plots_opts,\n",
        "            color=\"standard\",\n",
        "            title=f\"Top {min(10,len(bar_data))} Popular Breeds | Age-group:{age_group_y} | Districts:{district_x}\",\n",
        "            active_tools=[\"box_zoom\"],\n",
        "        )\n",
        "    if x is None or y is None:\n",
        "        bar_data = (\n",
        "            data[\"standard\"]\n",
        "            .value_counts()\n",
        "            .head(10)\n",
        "            .rename(\"count\")\n",
        "            .reset_index()\n",
        "            .rename(columns={\"index\": \"standard\"})\n",
        "        )\n",
        "        return hv.Bars(bar_data, kdims=[\"standard\"], vdims=\"count\").opts(\n",
        "            **bar_plots_opts,\n",
        "            color=\"standard\",\n",
        "            title=f\"Top 10 Breeds\",\n",
        "            tools=[\"hover\"],\n",
        "            active_tools=[\"box_zoom\"],\n",
        "        )\n",
        "\n",
        "\n",
        "update_barplot_panel = pn.panel(update_barplot)\n",
        "pn.Column(\n",
        "    pn.Row(\n",
        "    is_male_owner_checkbox, reporting_year_slider,\n",
        "    ), pn.Row(\n",
        "    age_group_panel, update_barplot_panel,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "vdRqBck8sdPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "districts_gdf.info()"
      ],
      "metadata": {
        "id": "hDOEYdUHEGo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_opts = dict(\n",
        "    width=800, height=500,\n",
        "    line_width=2,\n",
        "    xaxis=None,\n",
        "    yaxis=None,\n",
        "    aspect=\"equal\",\n",
        "    tools=['hover', 'tap', 'box_select']\n",
        ")\n",
        "\n",
        "@pn.depends(\n",
        "    breed_selector.param.value,\n",
        "    reporting_year_slider.param.value,\n",
        "    is_male_owner_checkbox.param.value,\n",
        ")\n",
        "def get_breed_chloropleth(breed, reporting_year, gender):\n",
        "\n",
        "    gender_reporting_year_df = get_gender_reporting_year_df(reporting_year=reporting_year, gender=gender)\n",
        "    gender_reporting_year_df['standard_lst'] = gender_reporting_year_df.apply(lambda row: [row['standard'], row['standard']] if row['standard_2'] == 'none' else [row['standard'], row['standard_2']], axis=1)\n",
        "    gender_reporting_year_df = gender_reporting_year_df.explode(['standard_lst'])\n",
        "    # print(breed)\n",
        "    standard_data = gender_reporting_year_df.loc[gender_reporting_year_df[\"standard_lst\"] == breed]\n",
        "    standard_data = standard_data.groupby(\"subdistrict\").size().div(2).astype(int).reset_index(name=\"count\")\n",
        "\n",
        "\n",
        "    breed_gdf = subdistrict_gdf.merge(\n",
        "        standard_data, on='subdistrict', how=\"left\"\n",
        "    )\n",
        "    # breed_gdf = breed_gdf.drop(columns=[\"desc\", \"km2\"])\n",
        "    breed_gdf = breed_gdf.fillna(0)\n",
        "    breed_color = explicit_mapping[breed]\n",
        "    breed_cmap = list(\n",
        "        sns.color_palette(\"light:\" + breed_color, n_colors=6).as_hex()\n",
        "    )\n",
        "    sex = \"Male\" if gender else \"Female\"\n",
        "    breed_poly = gv.Polygons(breed_gdf)\n",
        "\n",
        "    return breed_poly.opts(\n",
        "            color=\"count\",\n",
        "            cmap=breed_cmap,\n",
        "            # clim=(0, 60),\n",
        "            colorbar=True,\n",
        "            line_color=\"darkgray\",\n",
        "            width=800, height=500,\n",
        "            line_width=2,\n",
        "            xaxis=None,\n",
        "            yaxis=None,\n",
        "            aspect=\"equal\",\n",
        "            tools=['hover', 'tap', 'box_select'],\n",
        "            title=f\"{breed.title()} | {reporting_year} | {sex} Owners\",\n",
        "        )\n",
        "\n",
        "\n",
        "breed_chloropleth = pn.panel(get_breed_chloropleth,)\n",
        "\n",
        "\n",
        "# pn.Row(\n",
        "# pn.Column(\n",
        "#     is_male_owner_checkbox,\n",
        "#     reporting_year_slider,\n",
        "#     breed_selector,\n",
        "#     ),\n",
        "#     breed_chloropleth\n",
        "# )\n"
      ],
      "metadata": {
        "id": "7ViHSacDp82U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Combine the heatmap and the text display into a layout\n",
        "# pn.Column(\n",
        "#     pn.Row(\n",
        "#     is_male_owner_checkbox, reporting_year_slider, top_n_slider,\n",
        "#     ),\n",
        "#     pn.Row(\n",
        "#     dynamic_gender_heatmap_panel,\n",
        "#     breed_chloropleth,\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "YpyJU__6Cg0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/jonnross88/Springboard/main/images/dalle_dog_wordcloud.png -O dalle_dog_wordcloud.png\n",
        "dalle_dog_path = Path(\"/content/dalle_dog_wordcloud.png\")\n",
        "dalle_dog_url = \"https://raw.githubusercontent.com/jonnross88/Springboard/main/images/dalle_dog_wordcloud.png\"\n",
        "\n",
        "download_file(dalle_dog_path, dalle_dog_url)"
      ],
      "metadata": {
        "id": "mCmRtNruB6c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_path = \"/content/dalle_dog_wordcloud.png\"\n",
        "dog_image = hv.RGB.load_image(mask_path)\n",
        "threshold = 0.7\n",
        "dog_image_mask = dog_image.data\n",
        "dog_image_mask[dog_image_mask > threshold] = 1\n",
        "\n",
        "# Create a count of the various pure breeds\n",
        "pure_breed_count = dog_data_train.loc[\n",
        "    (dog_data_train[\"is_pure_breed\"])\n",
        "].standard.value_counts()\n",
        "\n",
        "# Create a word cloud for the pure breeds\n",
        "wc = WordCloud(\n",
        "    max_font_size=66,\n",
        "    contour_width=10,\n",
        "    contour_color=\"steelblue\",\n",
        "    background_color=\"white\",\n",
        "    colormap=\"cet_glasbey_dark\",\n",
        "    mask=(dog_image_mask * 255).astype(np.uint8),\n",
        ").generate_from_frequencies(pure_breed_count)\n",
        "\n",
        "\n",
        "breed_wordcoud = hv.RGB(wc.to_array()).opts(\n",
        "    width=wc.width,\n",
        "    height=wc.height,\n",
        "    title=\"Breed Popularity\",\n",
        "    show_frame=False,\n",
        "    xaxis=None,\n",
        "    yaxis=None,\n",
        "    # bgcolor=\"gray\",\n",
        "    padding=0.05,\n",
        "    active_tools=[\"box_zoom\"],\n",
        ")\n",
        "pn.pane.HoloViews(breed_wordcoud)"
      ],
      "metadata": {
        "id": "_azEAns9BmSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5IuWNgkFF2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21o7XTMvnQLn"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "# get the count of dogs by sub-district and reporting_year\n",
        "dog_count_by_sub_d_reporting_year = (dog_data_train.groupby(\n",
        "    [\"reporting_year\", \"subdistrict\"],\n",
        "    as_index=False).size().pivot(index=\"subdistrict\",\n",
        "                                 columns=\"reporting_year\",\n",
        "                                 values=\"size\"))\n",
        "# put the sub-district back into the columns\n",
        "dog_count_df_std = pd.DataFrame(\n",
        "    scaler.fit_transform(dog_count_by_sub_d_reporting_year),\n",
        "    columns=dog_count_by_sub_d_reporting_year.columns,\n",
        "    index=dog_count_by_sub_d_reporting_year.index,\n",
        ")\n",
        "# get the percent change of the dog count by sub-district and reporting_year\n",
        "dog_count_pct_change_std = pd.DataFrame(\n",
        "    scaler.fit_transform(\n",
        "        dog_count_by_sub_d_reporting_year.pct_change(axis=1).fillna(0) * 100),\n",
        "    columns=dog_count_by_sub_d_reporting_year.columns,\n",
        "    index=dog_count_by_sub_d_reporting_year.index,\n",
        ")\n",
        "# plot the standardized dog count and percent change\n",
        "dog_count_df_std.unstack().reset_index(name=\"count_std\").merge(\n",
        "    dog_count_pct_change_std.unstack().reset_index(\n",
        "        name=\"pct_change_std\")).hvplot.scatter(\n",
        "            by=\"reporting_year\",\n",
        "            y=\"count_std\",\n",
        "            x=\"pct_change_std\",\n",
        "            height=600,\n",
        "            width=600,\n",
        "            xlim=(-3, 3),\n",
        "            ylim=(-3, 3),\n",
        "        ) * hv.VLine(0).opts(color=\"lightgray\",\n",
        "                             line_dash=\"dashed\") * hv.HLine(0).opts(\n",
        "                                 color=\"lightgray\", line_dash=\"dashed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKoozt7xnQLo"
      },
      "outputs": [],
      "source": [
        "# same plot but without the fillna(0) in the pct_change_std\n",
        "dog_count_pct_change_long = (\n",
        "    dog_count_by_sub_d_reporting_year.pct_change(axis=1).unstack()\n",
        "    # .dropna()\n",
        "    .reset_index(name=\"pct_change\"))\n",
        "dog_count_long = dog_count_by_sub_d_reporting_year.unstack().reset_index(\n",
        "    name=\"count\").dropna()\n",
        "\n",
        "dog_count_std = scaler.fit_transform(\n",
        "    dog_count_by_sub_d_reporting_year.unstack().values.reshape(-1, 1))\n",
        "\n",
        "dog_count_long[\"count_std\"] = dog_count_std\n",
        "\n",
        "(dog_count_long.merge(dog_count_pct_change_long).hvplot.scatter(\n",
        "    by=\"reporting_year\",\n",
        "    y=\"count_std\",\n",
        "    x=\"pct_change\",\n",
        "    height=600,\n",
        "    width=600,\n",
        ") * hv.VLine(0).opts(color=\"lightgray\", line_dash=\"dashed\") *\n",
        " hv.HLine(0).opts(color=\"lightgray\", line_dash=\"dashed\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvNE46G9nQLo"
      },
      "outputs": [],
      "source": [
        "# display a sample of the dog data train\n",
        "dog_data_train.sample(3)\n",
        "dog_data_train.info()\n",
        "\n",
        "dog_data_train.describe(include=\"all\").T.round(2).infer_objects().sort_values(by=\"unique\").fillna(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDgkbDDlnQLo"
      },
      "outputs": [],
      "source": [
        "# Calculate yearly counts\n",
        "yearly_counts = dog_data_train.groupby(['reporting_year', 'subdistrict', 'standard'\n",
        "                                        ]).size().reset_index(name='size')\n",
        "\n",
        "# Calculate yearly change\n",
        "yearly_counts['yearly_change'] = yearly_counts.groupby(\n",
        "    ['subdistrict', 'standard'])['size'].diff().fillna(0)\n",
        "\n",
        "# Calculate change of the change (second derivative)\n",
        "yearly_counts['change_of_change'] = yearly_counts.groupby(\n",
        "    ['subdistrict', 'standard'])['yearly_change'].diff().fillna(0)\n",
        "\n",
        "# Normalize the changes\n",
        "yearly_counts['change_of_change_normalized'] = yearly_counts.groupby(\n",
        "    ['subdistrict', 'standard'])['change_of_change'].transform(\n",
        "        lambda x: (x - x.mean()) / x.std()).fillna(0).round(2)\n",
        "\n",
        "# Rank the breeds\n",
        "yearly_counts['rank'] = yearly_counts.groupby(\n",
        "    ['reporting_year',\n",
        "     'subdistrict'])['change_of_change_normalized'].rank(ascending=False,\n",
        "                                                          na_option='bottom')\n",
        "\n",
        "# Create target variable\n",
        "yearly_counts['is_emerging'] = (yearly_counts['rank'] == 1).astype(int)\n",
        "\n",
        "# set all 2015 records to  be non-emerging\n",
        "yearly_counts.loc[yearly_counts['reporting_year'].isin([2015, 2016]),\n",
        "                  'is_emerging'] = 0\n",
        "\n",
        "# yearly_counts.sort_values(by=['subdistrict', 'standard', 'reporting_year']).head(50)\n",
        "yearly_counts.sort_values(by=['is_emerging', 'reporting_year'],\n",
        "                          ascending=False).head(50)\n",
        "yearly_counts.sort_values(by=['size'], ascending=False).head(50)\n",
        "yearly_counts.query('reporting_year > 2016').sort_values(by=['is_emerging', 'reporting_year'],\n",
        "                                                 ascending=False).head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btrjAQcHnQLo"
      },
      "outputs": [],
      "source": [
        "# groupby subdistrict and reporting_year and count the number of each breed\n",
        "breed_count_by_reporting_year_sub_d = dog_data_train.groupby(\n",
        "    [\"reporting_year\", \"subdistrict\", \"standard\"]).size().unstack(level=2).fillna(0)\n",
        "\n",
        "# breed_count_by_reporting_year_sub_d.divide(breed_count_by_reporting_year_sub_d.sum(axis=1), axis=0)\n",
        "breed_count_by_reporting_year_sub_d.sort_index(level='reporting_year').groupby(level='subdistrict').diff().fillna(0)\n",
        "grouped_breed_count = breed_count_by_reporting_year_sub_d.groupby(level='subdistrict')\n",
        "grouped_breed_count.pct_change().fillna(0)\n",
        "total_breed_count = grouped_breed_count.sum()\n",
        "total_breed_count\n",
        "\n",
        "\n",
        "# filter the breed count by sub-district and reporting_year to only include breeds in the breeds_more_than_threshold list\n",
        "# this is a multi-index dataframe with sub-district and breeds\n",
        "breed_count_by_reporting_year_sub_d"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Greenspace"
      ],
      "metadata": {
        "id": "-ySj8UyXz3S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greenspace_zip_url = 'https://storage.googleapis.com/mrprime_dataset/zurich/greenspace.zip'\n",
        "parks_zip_url = \"https://storage.googleapis.com/mrprime_dataset/zurich/parks.zip\"\n"
      ],
      "metadata": {
        "id": "DLunS_T55IjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greenspace_gdf_dict = hf.get_gdf_from_zip_url(greenspace_zip_url)\n",
        "parks_gdf_dict = hf.get_gdf_from_zip_url(parks_zip_url)"
      ],
      "metadata": {
        "id": "_wteU8al5KMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greenspace_gdf = list(greenspace_gdf_dict.values())[0]\n",
        "greenspace_gdf.sample()\n"
      ],
      "metadata": {
        "id": "Br1hpncj5_yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parks_gdf = list(parks_gdf_dict.values())[0]\n",
        "parks_gdf.sample()"
      ],
      "metadata": {
        "id": "_BP4g-xC6i7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "greenspace_columns = {\n",
        "    'objektidentifikator': 'object_identifier',\n",
        "    'pflegeareal': 'maintenance_area',\n",
        "    'produkt': 'product',\n",
        "    'erfassungseinheit': 'recording_unit',\n",
        "    'pflegeeinheit': 'maintenance_unit',\n",
        "    'pflegestufe': 'maintenance_level'\n",
        "}\n",
        "greenspace_gdf = greenspace_gdf.rename(columns=greenspace_columns)\n",
        "greenspace_gdf = greenspace_gdf.drop(columns='objectid')\n",
        "greenspace_gdf.sample()\n"
      ],
      "metadata": {
        "id": "nIUvFMVi0IEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recording_unit_mapping = {\n",
        "    '611 Parkanlagen': 'park_facility',\n",
        "    '643 Schulgrün': 'school_greenspace',\n",
        "    '632 Badeanlagen': 'bathing_facility',\n",
        "    '641 Strassenbäume': 'street_tree',\n",
        "    '662 Bachunterhalt ERZ': 'stream_maintenance',\n",
        "    '661 Wohnliegenschaften LVZ': 'residential',\n",
        "    '624 Friedhofanlagen': 'cemetery_facility',\n",
        "    '663 Grünflächenpflege VBZ': 'greenspace_maintenance',\n",
        "    '623 Grabdienstleistungen': 'grave_service',\n",
        "    '631 Sportanlagen': 'sports_facility',\n",
        "    '642 Strassenbegleitgrün': 'roadside_greenspace',\n",
        "    '651 Sozialbauten IMMO': 'social_housing',\n",
        "    '650 Schulgrau IMMO': 'school_grey_area',\n",
        "    '660 Wohnsiedlungen LVZ': 'housing_estate',\n",
        "    '652 Verwaltungsbauten IMMO': 'administrative_building',\n",
        "    '654 Werkbauten IMMO': 'industrial_building',\n",
        "    '653 Kulturbauten IMMO': 'cultural_building',\n",
        "    '690 Verrechenbare Dienstleistungen': 'billable_service',\n",
        "    '664 Grünflächenpflege EWZ': 'greenspace_maintenance',\n",
        "}\n",
        "greenspace_gdf['recording_unit'] = greenspace_gdf['recording_unit'].map(recording_unit_mapping, na_action='ignore')\n"
      ],
      "metadata": {
        "id": "-ynjZauN9QK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "# Identify invalid geometries\n",
        "invalid_geometries = greenspace_gdf[~greenspace_gdf.geometry.is_valid]\n",
        "\n",
        "# If invalid geometries are found, attempt to fix them using buffer(0)\n",
        "if not invalid_geometries.empty:\n",
        "    print(f\"Found {len(invalid_geometries)} invalid geometries. Attempting to fix...\")\n",
        "    greenspace_gdf.geometry = greenspace_gdf.geometry.buffer(0)\n",
        "else:\n",
        "    print(\"No invalid geometries found.\")\n",
        "\n",
        "# Perform the dissolve operation\n",
        "greenspace_gdf_dissolved = greenspace_gdf.dissolve(by='maintenance_area')\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rE8l49AUqSt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "park_facility_gdf = greenspace_gdf_dissolved[greenspace_gdf_dissolved['recording_unit'] == 'park_facility'].drop(columns=['recording_unit', 'product', 'object_identifier', 'maintenance_unit']).reset_index()\n",
        "park_facility_gdf.nunique()"
      ],
      "metadata": {
        "id": "A0RzK7QTILP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parks_plot = gv.Points(parks_gdf, vdims=['name']).opts(color='red', tools=['hover'], size=5, marker='square')"
      ],
      "metadata": {
        "id": "BP3_iHeSNFcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "park_facility_gdf['area_m2'] = park_facility_gdf.to_crs('EPSG:2056').area.astype(int)\n"
      ],
      "metadata": {
        "id": "vqGrz2VEpvGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ks30R5IcPW9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def assign_subdistrict(park, subdistricts):\n",
        "    \"\"\"Assigns a park to the subdistrict with the largest intersection area.\"\"\"\n",
        "    max_area = 0\n",
        "    assigned_subdistrict = None\n",
        "\n",
        "    for index, subdistrict in subdistricts.iterrows():\n",
        "        intersection = park.geometry.intersection(subdistrict.geometry)\n",
        "        area = intersection.area\n",
        "\n",
        "        if area > max_area:\n",
        "            max_area = area\n",
        "            assigned_subdistrict = subdistrict[\"subdistrict\"]  # Assuming \"subdistrict\" column in subdistrict_gdf\n",
        "\n",
        "    return assigned_subdistrict\n",
        "\n",
        "# Ensure both geodataframes have the same CRS\n",
        "park_facility_gdf = park_facility_gdf.to_crs(subdistrict_gdf.crs)\n",
        "\n",
        "# Apply the assign_subdistrict function to each park\n",
        "park_facility_gdf['subdistrict'] = park_facility_gdf.apply(\n",
        "    lambda park: assign_subdistrict(park, subdistrict_gdf), axis=1\n",
        ")\n",
        "park_facility_gdf\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BX2bCYRhQTIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gts.ESRI * gv.Polygons(park_facility_gdf, vdims=['maintenance_level']).opts(height=800, width=1000, color='maintenance_level', alpha=0.5, tools=['hover', 'tap'], active_tools=['box_zoom'], show_legend=True)\n",
        "\n",
        "maintenance_poly_dict = {\n",
        "    level: gv.Polygons(gdf.to_crs(ccrs.GOOGLE_MERCATOR.proj4_init), vdims=['maintenance_level', 'maintenance_area'],\n",
        "                          crs=ccrs.GOOGLE_MERCATOR).opts(\n",
        "                              line_width=0, alpha=0.5, height=800, width=1200,\n",
        "                              show_legend=True, legend_position='right', muted_alpha=0.001,\n",
        "                              tools=['hover'],\n",
        "                              xaxis='bare', yaxis='bare', xlabel='', ylabel='')\n",
        "                          for level, gdf in park_facility_gdf.groupby('maintenance_level')\n",
        "}\n",
        "\n",
        "maintenance_plot = gv.NdOverlay(maintenance_poly_dict, kdims=['maintenance_level']).opts(title='Park Facilities by Maintenance Level')\n",
        "background_map = gts.ESRI.opts(alpha=0.5)\n",
        "\n",
        "pn.panel(background_map * maintenance_plot * parks_plot)\n"
      ],
      "metadata": {
        "id": "wmJKMxusFxvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parks_gdf"
      ],
      "metadata": {
        "id": "jV1ope2Mz-KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "column_meanings = {\n",
        "    'adr_inter': 'Internal address',\n",
        "    'adresse': 'Address',\n",
        "    'adrzus_int': 'Internal address addition',\n",
        "    'behindertenparkplatz': 'Disabled parking',\n",
        "    'bemerkung': 'Remarks, comments',\n",
        "    'ccmail': 'CC email address',\n",
        "    'da': 'Data acquisition (date?)',\n",
        "    'datum': 'Date',\n",
        "    'datum_cms': 'Date (CMS related)',\n",
        "    'dep': 'Department or area',\n",
        "    'editor': 'Editor (name?)',\n",
        "    'erforderlichedokumente': 'Required documents',\n",
        "    'fax': 'Fax number',\n",
        "    'hausnummer': 'House number',\n",
        "    'hindernisfreiheit': 'Accessibility',\n",
        "    'infrastruktur': 'Infrastructure',\n",
        "    'isbetriebsferien_gebaeude': 'Building closed due to company holidays',\n",
        "    'isbetriebsferien_schalter': 'Counter closed due to company holidays',\n",
        "    'kategorie': 'Category',\n",
        "    'mail': 'Email address',\n",
        "    'name': 'Name',\n",
        "    'namenzus': 'Name addition',\n",
        "    'objectid': 'Object ID',\n",
        "    'oeffnungszeiten_gebaeude_di': 'Building opening hours (Tuesday)',\n",
        "    'oeffnungszeiten_schalter_mo': 'Counter opening hours (Monday)',\n",
        "    'oeffnungszeiten_schalter_sa': 'Counter opening hours (Saturday)',\n",
        "    'oeffnungszeiten_schalter_so': 'Counter opening hours (Sunday)',\n",
        "    'ort': 'City or location',\n",
        "    'plz': 'Postal code',\n",
        "    'poi_id': 'Point of Interest ID',\n",
        "    'postadresse': 'Postal address',\n",
        "    'publish_internet': 'Publish on internet',\n",
        "    'strasse': 'Street',\n",
        "    'suchen': 'Search terms',\n",
        "    'tel': 'Telephone number',\n",
        "    'tel2': 'Alternative telephone number',\n",
        "    'www': 'Website',\n",
        "    'zahlungsmittel_internet': 'Payment methods (internet)',\n",
        "    'zahlungsmittel_schalter': 'Payment methods (counter)',\n",
        "    'zahlungsmittel_telefon': 'Payment methods (telephone)',\n",
        "    'zvv_label': 'ZVV (public transport) label',\n",
        "    'zvv_link': 'ZVV (public transport) link',\n",
        "\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ai2f0age2kuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "( gts.OSM * gv.Points(parks_gdf).opts(size=5)).opts(height=800, width=1000)"
      ],
      "metadata": {
        "id": "LW46Jz6h0T6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}